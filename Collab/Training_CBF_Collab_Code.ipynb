{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Training_CBF_Collab_Code",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kBBJ43yhCtb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "8551dc3d-91ba-4ebf-d800-314b84183ea8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_BSda2fo6Ay",
        "colab_type": "text"
      },
      "source": [
        "**CBF Dataset** \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euuRQewIoRD4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3e94ddfa-d54e-449f-8ae7-789c6b9a3f25"
      },
      "source": [
        "!python gdrive/My\\ Drive/Colab\\ Notebooks/LSTM-FCN/all_datasets_training.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "\n",
            "Epoch 00769: loss did not improve from 0.00009\n",
            "Epoch 770/2000\n",
            " - 2s - loss: 2.1794e-04 - accuracy: 1.0000 - val_loss: 0.0240 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00770: loss did not improve from 0.00009\n",
            "Epoch 771/2000\n",
            " - 2s - loss: 9.9825e-05 - accuracy: 1.0000 - val_loss: 0.0239 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00771: loss did not improve from 0.00009\n",
            "Epoch 772/2000\n",
            " - 2s - loss: 1.1370e-04 - accuracy: 1.0000 - val_loss: 0.0239 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00772: loss did not improve from 0.00009\n",
            "Epoch 773/2000\n",
            " - 2s - loss: 2.0888e-04 - accuracy: 1.0000 - val_loss: 0.0239 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00773: loss did not improve from 0.00009\n",
            "Epoch 774/2000\n",
            " - 2s - loss: 2.1368e-04 - accuracy: 1.0000 - val_loss: 0.0239 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00774: loss did not improve from 0.00009\n",
            "Epoch 775/2000\n",
            " - 2s - loss: 1.0789e-04 - accuracy: 1.0000 - val_loss: 0.0238 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00775: loss did not improve from 0.00009\n",
            "Epoch 776/2000\n",
            " - 2s - loss: 1.4423e-04 - accuracy: 1.0000 - val_loss: 0.0238 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00776: loss did not improve from 0.00009\n",
            "Epoch 777/2000\n",
            " - 2s - loss: 1.3632e-04 - accuracy: 1.0000 - val_loss: 0.0238 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00777: loss did not improve from 0.00009\n",
            "Epoch 778/2000\n",
            " - 2s - loss: 1.2780e-04 - accuracy: 1.0000 - val_loss: 0.0237 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00778: loss did not improve from 0.00009\n",
            "Epoch 779/2000\n",
            " - 2s - loss: 2.1556e-04 - accuracy: 1.0000 - val_loss: 0.0238 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00779: loss did not improve from 0.00009\n",
            "Epoch 780/2000\n",
            " - 2s - loss: 1.2461e-04 - accuracy: 1.0000 - val_loss: 0.0238 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00780: loss did not improve from 0.00009\n",
            "Epoch 781/2000\n",
            " - 2s - loss: 1.3878e-04 - accuracy: 1.0000 - val_loss: 0.0238 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00781: loss did not improve from 0.00009\n",
            "Epoch 782/2000\n",
            " - 2s - loss: 1.4405e-04 - accuracy: 1.0000 - val_loss: 0.0238 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00782: loss did not improve from 0.00009\n",
            "Epoch 783/2000\n",
            " - 2s - loss: 1.1226e-04 - accuracy: 1.0000 - val_loss: 0.0238 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00783: loss did not improve from 0.00009\n",
            "Epoch 784/2000\n",
            " - 2s - loss: 1.8018e-04 - accuracy: 1.0000 - val_loss: 0.0239 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00784: loss did not improve from 0.00009\n",
            "Epoch 785/2000\n",
            " - 2s - loss: 2.0757e-04 - accuracy: 1.0000 - val_loss: 0.0239 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00785: loss did not improve from 0.00009\n",
            "Epoch 786/2000\n",
            " - 2s - loss: 1.7410e-04 - accuracy: 1.0000 - val_loss: 0.0238 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00786: loss did not improve from 0.00009\n",
            "Epoch 787/2000\n",
            " - 2s - loss: 1.1590e-04 - accuracy: 1.0000 - val_loss: 0.0238 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00787: loss did not improve from 0.00009\n",
            "Epoch 788/2000\n",
            " - 2s - loss: 1.4482e-04 - accuracy: 1.0000 - val_loss: 0.0238 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00788: loss did not improve from 0.00009\n",
            "Epoch 789/2000\n",
            " - 2s - loss: 3.1147e-04 - accuracy: 1.0000 - val_loss: 0.0238 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00789: loss did not improve from 0.00009\n",
            "Epoch 790/2000\n",
            " - 2s - loss: 1.3861e-04 - accuracy: 1.0000 - val_loss: 0.0238 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00790: loss did not improve from 0.00009\n",
            "Epoch 791/2000\n",
            " - 2s - loss: 1.4733e-04 - accuracy: 1.0000 - val_loss: 0.0238 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00791: loss did not improve from 0.00009\n",
            "Epoch 792/2000\n",
            " - 2s - loss: 1.4586e-04 - accuracy: 1.0000 - val_loss: 0.0238 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00792: loss did not improve from 0.00009\n",
            "Epoch 793/2000\n",
            " - 2s - loss: 1.7352e-04 - accuracy: 1.0000 - val_loss: 0.0237 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00793: loss did not improve from 0.00009\n",
            "Epoch 794/2000\n",
            " - 2s - loss: 1.6203e-04 - accuracy: 1.0000 - val_loss: 0.0237 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00794: loss did not improve from 0.00009\n",
            "Epoch 795/2000\n",
            " - 2s - loss: 1.6043e-04 - accuracy: 1.0000 - val_loss: 0.0237 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00795: loss did not improve from 0.00009\n",
            "Epoch 796/2000\n",
            " - 2s - loss: 1.3770e-04 - accuracy: 1.0000 - val_loss: 0.0237 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00796: loss did not improve from 0.00009\n",
            "Epoch 797/2000\n",
            " - 2s - loss: 1.5173e-04 - accuracy: 1.0000 - val_loss: 0.0237 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00797: loss did not improve from 0.00009\n",
            "Epoch 798/2000\n",
            " - 2s - loss: 1.2720e-04 - accuracy: 1.0000 - val_loss: 0.0237 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00798: loss did not improve from 0.00009\n",
            "Epoch 799/2000\n",
            " - 2s - loss: 1.8402e-04 - accuracy: 1.0000 - val_loss: 0.0238 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00799: loss did not improve from 0.00009\n",
            "Epoch 800/2000\n",
            " - 2s - loss: 1.6330e-04 - accuracy: 1.0000 - val_loss: 0.0238 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00800: loss did not improve from 0.00009\n",
            "Epoch 801/2000\n",
            " - 2s - loss: 1.6820e-04 - accuracy: 1.0000 - val_loss: 0.0239 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00801: loss did not improve from 0.00009\n",
            "Epoch 802/2000\n",
            " - 2s - loss: 2.3488e-04 - accuracy: 1.0000 - val_loss: 0.0239 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00802: loss did not improve from 0.00009\n",
            "Epoch 803/2000\n",
            " - 2s - loss: 1.7870e-04 - accuracy: 1.0000 - val_loss: 0.0239 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00803: loss did not improve from 0.00009\n",
            "Epoch 804/2000\n",
            " - 2s - loss: 1.2446e-04 - accuracy: 1.0000 - val_loss: 0.0239 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00804: loss did not improve from 0.00009\n",
            "Epoch 805/2000\n",
            " - 2s - loss: 1.4721e-04 - accuracy: 1.0000 - val_loss: 0.0240 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00805: loss did not improve from 0.00009\n",
            "Epoch 806/2000\n",
            " - 2s - loss: 1.5320e-04 - accuracy: 1.0000 - val_loss: 0.0240 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00806: loss did not improve from 0.00009\n",
            "Epoch 807/2000\n",
            " - 2s - loss: 1.8747e-04 - accuracy: 1.0000 - val_loss: 0.0240 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00807: loss did not improve from 0.00009\n",
            "Epoch 808/2000\n",
            " - 2s - loss: 1.5784e-04 - accuracy: 1.0000 - val_loss: 0.0240 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00808: loss did not improve from 0.00009\n",
            "Epoch 809/2000\n",
            " - 2s - loss: 1.6542e-04 - accuracy: 1.0000 - val_loss: 0.0239 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00809: loss did not improve from 0.00009\n",
            "Epoch 810/2000\n",
            " - 2s - loss: 1.8878e-04 - accuracy: 1.0000 - val_loss: 0.0237 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00810: loss did not improve from 0.00009\n",
            "Epoch 811/2000\n",
            " - 2s - loss: 1.2806e-04 - accuracy: 1.0000 - val_loss: 0.0236 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00811: loss did not improve from 0.00009\n",
            "Epoch 812/2000\n",
            " - 2s - loss: 1.3550e-04 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00812: loss did not improve from 0.00009\n",
            "Epoch 813/2000\n",
            " - 2s - loss: 1.5564e-04 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00813: loss did not improve from 0.00009\n",
            "Epoch 814/2000\n",
            " - 2s - loss: 1.4756e-04 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00814: loss did not improve from 0.00009\n",
            "Epoch 815/2000\n",
            " - 2s - loss: 1.5553e-04 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00815: loss did not improve from 0.00009\n",
            "Epoch 816/2000\n",
            " - 2s - loss: 1.2483e-04 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00816: loss did not improve from 0.00009\n",
            "Epoch 817/2000\n",
            " - 2s - loss: 1.4021e-04 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00817: loss did not improve from 0.00009\n",
            "Epoch 818/2000\n",
            " - 2s - loss: 1.6928e-04 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00818: loss did not improve from 0.00009\n",
            "Epoch 819/2000\n",
            " - 2s - loss: 1.4607e-04 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00819: loss did not improve from 0.00009\n",
            "Epoch 820/2000\n",
            " - 2s - loss: 1.3636e-04 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00820: loss did not improve from 0.00009\n",
            "Epoch 821/2000\n",
            " - 2s - loss: 1.4956e-04 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00821: loss did not improve from 0.00009\n",
            "Epoch 822/2000\n",
            " - 2s - loss: 1.8397e-04 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00822: loss did not improve from 0.00009\n",
            "Epoch 823/2000\n",
            " - 2s - loss: 1.5825e-04 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00823: loss did not improve from 0.00009\n",
            "Epoch 824/2000\n",
            " - 2s - loss: 1.1216e-04 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00824: loss did not improve from 0.00009\n",
            "Epoch 825/2000\n",
            " - 2s - loss: 1.3873e-04 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00825: loss did not improve from 0.00009\n",
            "Epoch 826/2000\n",
            " - 2s - loss: 1.0844e-04 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00826: loss did not improve from 0.00009\n",
            "Epoch 827/2000\n",
            " - 2s - loss: 1.5644e-04 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00827: loss did not improve from 0.00009\n",
            "Epoch 828/2000\n",
            " - 2s - loss: 1.4697e-04 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00828: loss did not improve from 0.00009\n",
            "Epoch 829/2000\n",
            " - 2s - loss: 1.6845e-04 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00829: loss did not improve from 0.00009\n",
            "Epoch 830/2000\n",
            " - 2s - loss: 1.6340e-04 - accuracy: 1.0000 - val_loss: 0.0236 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00830: loss did not improve from 0.00009\n",
            "Epoch 831/2000\n",
            " - 2s - loss: 1.2930e-04 - accuracy: 1.0000 - val_loss: 0.0236 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00831: loss did not improve from 0.00009\n",
            "Epoch 832/2000\n",
            " - 2s - loss: 1.9108e-04 - accuracy: 1.0000 - val_loss: 0.0237 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00832: loss did not improve from 0.00009\n",
            "Epoch 833/2000\n",
            " - 2s - loss: 2.4561e-04 - accuracy: 1.0000 - val_loss: 0.0236 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00833: loss did not improve from 0.00009\n",
            "Epoch 834/2000\n",
            " - 2s - loss: 9.6321e-05 - accuracy: 1.0000 - val_loss: 0.0236 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00834: loss did not improve from 0.00009\n",
            "Epoch 835/2000\n",
            " - 2s - loss: 1.1897e-04 - accuracy: 1.0000 - val_loss: 0.0236 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00835: loss did not improve from 0.00009\n",
            "Epoch 836/2000\n",
            " - 2s - loss: 1.7610e-04 - accuracy: 1.0000 - val_loss: 0.0236 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00836: loss did not improve from 0.00009\n",
            "Epoch 837/2000\n",
            " - 2s - loss: 1.4931e-04 - accuracy: 1.0000 - val_loss: 0.0236 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00837: loss did not improve from 0.00009\n",
            "Epoch 838/2000\n",
            " - 2s - loss: 1.2789e-04 - accuracy: 1.0000 - val_loss: 0.0236 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00838: loss did not improve from 0.00009\n",
            "Epoch 839/2000\n",
            " - 2s - loss: 1.0554e-04 - accuracy: 1.0000 - val_loss: 0.0236 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00839: loss did not improve from 0.00009\n",
            "Epoch 840/2000\n",
            " - 2s - loss: 1.5824e-04 - accuracy: 1.0000 - val_loss: 0.0237 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00840: loss did not improve from 0.00009\n",
            "Epoch 841/2000\n",
            " - 2s - loss: 1.4942e-04 - accuracy: 1.0000 - val_loss: 0.0237 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00841: loss did not improve from 0.00009\n",
            "Epoch 842/2000\n",
            " - 2s - loss: 1.4034e-04 - accuracy: 1.0000 - val_loss: 0.0238 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00842: loss did not improve from 0.00009\n",
            "Epoch 843/2000\n",
            " - 2s - loss: 1.8553e-04 - accuracy: 1.0000 - val_loss: 0.0239 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00843: loss did not improve from 0.00009\n",
            "\n",
            "Epoch 00843: ReduceLROnPlateau reducing learning rate to 0.0003149802807761199.\n",
            "Epoch 844/2000\n",
            " - 2s - loss: 1.7507e-04 - accuracy: 1.0000 - val_loss: 0.0240 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00844: loss did not improve from 0.00009\n",
            "Epoch 845/2000\n",
            " - 2s - loss: 1.4026e-04 - accuracy: 1.0000 - val_loss: 0.0241 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00845: loss did not improve from 0.00009\n",
            "Epoch 846/2000\n",
            " - 2s - loss: 1.2510e-04 - accuracy: 1.0000 - val_loss: 0.0241 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00846: loss did not improve from 0.00009\n",
            "Epoch 847/2000\n",
            " - 2s - loss: 1.2663e-04 - accuracy: 1.0000 - val_loss: 0.0241 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00847: loss did not improve from 0.00009\n",
            "Epoch 848/2000\n",
            " - 2s - loss: 1.3226e-04 - accuracy: 1.0000 - val_loss: 0.0241 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00848: loss did not improve from 0.00009\n",
            "Epoch 849/2000\n",
            " - 2s - loss: 1.4252e-04 - accuracy: 1.0000 - val_loss: 0.0241 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00849: loss did not improve from 0.00009\n",
            "Epoch 850/2000\n",
            " - 2s - loss: 1.0473e-04 - accuracy: 1.0000 - val_loss: 0.0241 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00850: loss did not improve from 0.00009\n",
            "Epoch 851/2000\n",
            " - 2s - loss: 1.1675e-04 - accuracy: 1.0000 - val_loss: 0.0241 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00851: loss did not improve from 0.00009\n",
            "Epoch 852/2000\n",
            " - 2s - loss: 1.2226e-04 - accuracy: 1.0000 - val_loss: 0.0241 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00852: loss did not improve from 0.00009\n",
            "Epoch 853/2000\n",
            " - 2s - loss: 1.0469e-04 - accuracy: 1.0000 - val_loss: 0.0241 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00853: loss did not improve from 0.00009\n",
            "Epoch 854/2000\n",
            " - 2s - loss: 1.4845e-04 - accuracy: 1.0000 - val_loss: 0.0241 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00854: loss did not improve from 0.00009\n",
            "Epoch 855/2000\n",
            " - 2s - loss: 1.1739e-04 - accuracy: 1.0000 - val_loss: 0.0241 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00855: loss did not improve from 0.00009\n",
            "Epoch 856/2000\n",
            " - 2s - loss: 1.2603e-04 - accuracy: 1.0000 - val_loss: 0.0241 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00856: loss did not improve from 0.00009\n",
            "Epoch 857/2000\n",
            " - 2s - loss: 9.6075e-05 - accuracy: 1.0000 - val_loss: 0.0241 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00857: loss did not improve from 0.00009\n",
            "Epoch 858/2000\n",
            " - 2s - loss: 1.6784e-04 - accuracy: 1.0000 - val_loss: 0.0241 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00858: loss did not improve from 0.00009\n",
            "Epoch 859/2000\n",
            " - 2s - loss: 1.2748e-04 - accuracy: 1.0000 - val_loss: 0.0241 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00859: loss did not improve from 0.00009\n",
            "Epoch 860/2000\n",
            " - 2s - loss: 1.2791e-04 - accuracy: 1.0000 - val_loss: 0.0241 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00860: loss did not improve from 0.00009\n",
            "Epoch 861/2000\n",
            " - 2s - loss: 1.8509e-04 - accuracy: 1.0000 - val_loss: 0.0241 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00861: loss did not improve from 0.00009\n",
            "Epoch 862/2000\n",
            " - 2s - loss: 1.4986e-04 - accuracy: 1.0000 - val_loss: 0.0241 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00862: loss did not improve from 0.00009\n",
            "Epoch 863/2000\n",
            " - 2s - loss: 1.3042e-04 - accuracy: 1.0000 - val_loss: 0.0241 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00863: loss did not improve from 0.00009\n",
            "Epoch 864/2000\n",
            " - 2s - loss: 1.0760e-04 - accuracy: 1.0000 - val_loss: 0.0241 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00864: loss did not improve from 0.00009\n",
            "Epoch 865/2000\n",
            " - 2s - loss: 1.3449e-04 - accuracy: 1.0000 - val_loss: 0.0241 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00865: loss did not improve from 0.00009\n",
            "Epoch 866/2000\n",
            " - 2s - loss: 1.0730e-04 - accuracy: 1.0000 - val_loss: 0.0241 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00866: loss did not improve from 0.00009\n",
            "Epoch 867/2000\n",
            " - 2s - loss: 1.3034e-04 - accuracy: 1.0000 - val_loss: 0.0240 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00867: loss did not improve from 0.00009\n",
            "Epoch 868/2000\n",
            " - 2s - loss: 1.8656e-04 - accuracy: 1.0000 - val_loss: 0.0240 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00868: loss did not improve from 0.00009\n",
            "Epoch 869/2000\n",
            " - 2s - loss: 1.4766e-04 - accuracy: 1.0000 - val_loss: 0.0240 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00869: loss did not improve from 0.00009\n",
            "Epoch 870/2000\n",
            " - 2s - loss: 8.8885e-05 - accuracy: 1.0000 - val_loss: 0.0240 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00870: loss improved from 0.00009 to 0.00009, saving model to ./weights/lstmfcn_128_cells_weights/CBF_weights.h5\n",
            "Epoch 871/2000\n",
            " - 2s - loss: 1.2334e-04 - accuracy: 1.0000 - val_loss: 0.0239 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00871: loss did not improve from 0.00009\n",
            "Epoch 872/2000\n",
            " - 2s - loss: 1.9818e-04 - accuracy: 1.0000 - val_loss: 0.0238 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00872: loss did not improve from 0.00009\n",
            "Epoch 873/2000\n",
            " - 2s - loss: 1.1883e-04 - accuracy: 1.0000 - val_loss: 0.0238 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00873: loss did not improve from 0.00009\n",
            "Epoch 874/2000\n",
            " - 2s - loss: 1.5063e-04 - accuracy: 1.0000 - val_loss: 0.0237 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00874: loss did not improve from 0.00009\n",
            "Epoch 875/2000\n",
            " - 2s - loss: 1.2753e-04 - accuracy: 1.0000 - val_loss: 0.0236 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00875: loss did not improve from 0.00009\n",
            "Epoch 876/2000\n",
            " - 2s - loss: 1.7143e-04 - accuracy: 1.0000 - val_loss: 0.0236 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00876: loss did not improve from 0.00009\n",
            "Epoch 877/2000\n",
            " - 2s - loss: 1.2613e-04 - accuracy: 1.0000 - val_loss: 0.0236 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00877: loss did not improve from 0.00009\n",
            "Epoch 878/2000\n",
            " - 2s - loss: 1.4700e-04 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00878: loss did not improve from 0.00009\n",
            "Epoch 879/2000\n",
            " - 2s - loss: 1.6526e-04 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00879: loss did not improve from 0.00009\n",
            "Epoch 880/2000\n",
            " - 2s - loss: 1.1757e-04 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00880: loss did not improve from 0.00009\n",
            "Epoch 881/2000\n",
            " - 2s - loss: 1.6832e-04 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00881: loss did not improve from 0.00009\n",
            "Epoch 882/2000\n",
            " - 2s - loss: 1.4656e-04 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00882: loss did not improve from 0.00009\n",
            "Epoch 883/2000\n",
            " - 2s - loss: 1.2236e-04 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00883: loss did not improve from 0.00009\n",
            "Epoch 884/2000\n",
            " - 2s - loss: 1.3098e-04 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00884: loss did not improve from 0.00009\n",
            "Epoch 885/2000\n",
            " - 2s - loss: 1.5940e-04 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00885: loss did not improve from 0.00009\n",
            "Epoch 886/2000\n",
            " - 2s - loss: 1.0568e-04 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00886: loss did not improve from 0.00009\n",
            "Epoch 887/2000\n",
            " - 2s - loss: 1.1684e-04 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00887: loss did not improve from 0.00009\n",
            "Epoch 888/2000\n",
            " - 2s - loss: 1.1314e-04 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00888: loss did not improve from 0.00009\n",
            "Epoch 889/2000\n",
            " - 2s - loss: 9.8724e-05 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00889: loss did not improve from 0.00009\n",
            "Epoch 890/2000\n",
            " - 2s - loss: 1.7897e-04 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00890: loss did not improve from 0.00009\n",
            "Epoch 891/2000\n",
            " - 2s - loss: 1.3964e-04 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00891: loss did not improve from 0.00009\n",
            "Epoch 892/2000\n",
            " - 2s - loss: 1.2463e-04 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00892: loss did not improve from 0.00009\n",
            "Epoch 893/2000\n",
            " - 2s - loss: 1.2086e-04 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00893: loss did not improve from 0.00009\n",
            "Epoch 894/2000\n",
            " - 2s - loss: 1.5283e-04 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00894: loss did not improve from 0.00009\n",
            "Epoch 895/2000\n",
            " - 2s - loss: 1.6932e-04 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00895: loss did not improve from 0.00009\n",
            "Epoch 896/2000\n",
            " - 2s - loss: 1.5179e-04 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00896: loss did not improve from 0.00009\n",
            "Epoch 897/2000\n",
            " - 2s - loss: 1.5638e-04 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00897: loss did not improve from 0.00009\n",
            "Epoch 898/2000\n",
            " - 2s - loss: 1.7491e-04 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00898: loss did not improve from 0.00009\n",
            "Epoch 899/2000\n",
            " - 2s - loss: 1.5111e-04 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00899: loss did not improve from 0.00009\n",
            "Epoch 900/2000\n",
            " - 2s - loss: 1.0062e-04 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9967\n",
            "\n",
            "Epoch 00900: loss did not improve from 0.00009\n",
            "Epoch 901/2000\n",
            " - 2s - loss: 1.2096e-04 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9967\n",
            "\n",
            "Epoch 00901: loss did not improve from 0.00009\n",
            "Epoch 902/2000\n",
            " - 2s - loss: 1.7386e-04 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9967\n",
            "\n",
            "Epoch 00902: loss did not improve from 0.00009\n",
            "Epoch 903/2000\n",
            " - 2s - loss: 9.6139e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9967\n",
            "\n",
            "Epoch 00903: loss did not improve from 0.00009\n",
            "Epoch 904/2000\n",
            " - 2s - loss: 1.3421e-04 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9967\n",
            "\n",
            "Epoch 00904: loss did not improve from 0.00009\n",
            "Epoch 905/2000\n",
            " - 2s - loss: 1.1720e-04 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00905: loss did not improve from 0.00009\n",
            "Epoch 906/2000\n",
            " - 2s - loss: 1.4218e-04 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00906: loss did not improve from 0.00009\n",
            "Epoch 907/2000\n",
            " - 2s - loss: 1.3350e-04 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00907: loss did not improve from 0.00009\n",
            "Epoch 908/2000\n",
            " - 2s - loss: 1.2723e-04 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00908: loss did not improve from 0.00009\n",
            "Epoch 909/2000\n",
            " - 2s - loss: 1.4728e-04 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00909: loss did not improve from 0.00009\n",
            "Epoch 910/2000\n",
            " - 2s - loss: 1.4456e-04 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00910: loss did not improve from 0.00009\n",
            "Epoch 911/2000\n",
            " - 2s - loss: 9.8666e-05 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00911: loss did not improve from 0.00009\n",
            "Epoch 912/2000\n",
            " - 2s - loss: 1.9197e-04 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00912: loss did not improve from 0.00009\n",
            "Epoch 913/2000\n",
            " - 2s - loss: 1.1524e-04 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00913: loss did not improve from 0.00009\n",
            "Epoch 914/2000\n",
            " - 2s - loss: 1.2464e-04 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00914: loss did not improve from 0.00009\n",
            "Epoch 915/2000\n",
            " - 2s - loss: 1.3625e-04 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00915: loss did not improve from 0.00009\n",
            "Epoch 916/2000\n",
            " - 2s - loss: 1.7162e-04 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00916: loss did not improve from 0.00009\n",
            "Epoch 917/2000\n",
            " - 2s - loss: 1.4218e-04 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00917: loss did not improve from 0.00009\n",
            "Epoch 918/2000\n",
            " - 2s - loss: 9.7218e-05 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00918: loss did not improve from 0.00009\n",
            "Epoch 919/2000\n",
            " - 2s - loss: 1.2384e-04 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00919: loss did not improve from 0.00009\n",
            "Epoch 920/2000\n",
            " - 2s - loss: 1.1398e-04 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00920: loss did not improve from 0.00009\n",
            "Epoch 921/2000\n",
            " - 2s - loss: 1.3086e-04 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00921: loss did not improve from 0.00009\n",
            "Epoch 922/2000\n",
            " - 2s - loss: 1.0296e-04 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00922: loss did not improve from 0.00009\n",
            "Epoch 923/2000\n",
            " - 2s - loss: 1.2457e-04 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00923: loss did not improve from 0.00009\n",
            "Epoch 924/2000\n",
            " - 2s - loss: 1.5838e-04 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00924: loss did not improve from 0.00009\n",
            "Epoch 925/2000\n",
            " - 2s - loss: 1.1910e-04 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00925: loss did not improve from 0.00009\n",
            "Epoch 926/2000\n",
            " - 2s - loss: 1.0463e-04 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00926: loss did not improve from 0.00009\n",
            "Epoch 927/2000\n",
            " - 2s - loss: 1.2499e-04 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00927: loss did not improve from 0.00009\n",
            "Epoch 928/2000\n",
            " - 2s - loss: 1.1817e-04 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00928: loss did not improve from 0.00009\n",
            "Epoch 929/2000\n",
            " - 2s - loss: 1.6942e-04 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00929: loss did not improve from 0.00009\n",
            "Epoch 930/2000\n",
            " - 2s - loss: 1.3968e-04 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00930: loss did not improve from 0.00009\n",
            "Epoch 931/2000\n",
            " - 2s - loss: 1.3911e-04 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00931: loss did not improve from 0.00009\n",
            "Epoch 932/2000\n",
            " - 2s - loss: 1.3357e-04 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00932: loss did not improve from 0.00009\n",
            "Epoch 933/2000\n",
            " - 2s - loss: 1.2066e-04 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00933: loss did not improve from 0.00009\n",
            "Epoch 934/2000\n",
            " - 2s - loss: 1.4360e-04 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00934: loss did not improve from 0.00009\n",
            "Epoch 935/2000\n",
            " - 2s - loss: 7.8018e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00935: loss improved from 0.00009 to 0.00008, saving model to ./weights/lstmfcn_128_cells_weights/CBF_weights.h5\n",
            "Epoch 936/2000\n",
            " - 2s - loss: 1.4522e-04 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00936: loss did not improve from 0.00008\n",
            "Epoch 937/2000\n",
            " - 2s - loss: 1.4533e-04 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00937: loss did not improve from 0.00008\n",
            "Epoch 938/2000\n",
            " - 2s - loss: 1.2377e-04 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00938: loss did not improve from 0.00008\n",
            "Epoch 939/2000\n",
            " - 2s - loss: 1.3014e-04 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00939: loss did not improve from 0.00008\n",
            "Epoch 940/2000\n",
            " - 2s - loss: 1.2522e-04 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00940: loss did not improve from 0.00008\n",
            "Epoch 941/2000\n",
            " - 2s - loss: 1.3931e-04 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00941: loss did not improve from 0.00008\n",
            "Epoch 942/2000\n",
            " - 2s - loss: 1.0544e-04 - accuracy: 1.0000 - val_loss: 0.0236 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00942: loss did not improve from 0.00008\n",
            "Epoch 943/2000\n",
            " - 2s - loss: 7.7467e-05 - accuracy: 1.0000 - val_loss: 0.0236 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00943: loss improved from 0.00008 to 0.00008, saving model to ./weights/lstmfcn_128_cells_weights/CBF_weights.h5\n",
            "Epoch 944/2000\n",
            " - 2s - loss: 7.5924e-05 - accuracy: 1.0000 - val_loss: 0.0236 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00944: loss improved from 0.00008 to 0.00008, saving model to ./weights/lstmfcn_128_cells_weights/CBF_weights.h5\n",
            "Epoch 945/2000\n",
            " - 2s - loss: 1.4781e-04 - accuracy: 1.0000 - val_loss: 0.0236 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00945: loss did not improve from 0.00008\n",
            "Epoch 946/2000\n",
            " - 2s - loss: 1.2875e-04 - accuracy: 1.0000 - val_loss: 0.0236 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00946: loss did not improve from 0.00008\n",
            "Epoch 947/2000\n",
            " - 2s - loss: 1.7204e-04 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00947: loss did not improve from 0.00008\n",
            "Epoch 948/2000\n",
            " - 2s - loss: 1.3328e-04 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00948: loss did not improve from 0.00008\n",
            "Epoch 949/2000\n",
            " - 2s - loss: 1.3518e-04 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00949: loss did not improve from 0.00008\n",
            "Epoch 950/2000\n",
            " - 2s - loss: 1.2147e-04 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00950: loss did not improve from 0.00008\n",
            "Epoch 951/2000\n",
            " - 2s - loss: 1.1011e-04 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00951: loss did not improve from 0.00008\n",
            "Epoch 952/2000\n",
            " - 2s - loss: 1.2014e-04 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00952: loss did not improve from 0.00008\n",
            "Epoch 953/2000\n",
            " - 2s - loss: 1.1043e-04 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00953: loss did not improve from 0.00008\n",
            "Epoch 954/2000\n",
            " - 2s - loss: 1.3807e-04 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00954: loss did not improve from 0.00008\n",
            "Epoch 955/2000\n",
            " - 2s - loss: 8.6801e-05 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00955: loss did not improve from 0.00008\n",
            "Epoch 956/2000\n",
            " - 2s - loss: 1.2674e-04 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00956: loss did not improve from 0.00008\n",
            "Epoch 957/2000\n",
            " - 2s - loss: 8.6960e-05 - accuracy: 1.0000 - val_loss: 0.0236 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00957: loss did not improve from 0.00008\n",
            "Epoch 958/2000\n",
            " - 2s - loss: 1.0686e-04 - accuracy: 1.0000 - val_loss: 0.0236 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00958: loss did not improve from 0.00008\n",
            "Epoch 959/2000\n",
            " - 2s - loss: 8.2313e-05 - accuracy: 1.0000 - val_loss: 0.0237 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00959: loss did not improve from 0.00008\n",
            "Epoch 960/2000\n",
            " - 2s - loss: 1.1113e-04 - accuracy: 1.0000 - val_loss: 0.0238 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00960: loss did not improve from 0.00008\n",
            "Epoch 961/2000\n",
            " - 2s - loss: 1.0934e-04 - accuracy: 1.0000 - val_loss: 0.0238 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00961: loss did not improve from 0.00008\n",
            "Epoch 962/2000\n",
            " - 2s - loss: 9.1906e-05 - accuracy: 1.0000 - val_loss: 0.0238 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00962: loss did not improve from 0.00008\n",
            "Epoch 963/2000\n",
            " - 2s - loss: 1.2344e-04 - accuracy: 1.0000 - val_loss: 0.0239 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00963: loss did not improve from 0.00008\n",
            "Epoch 964/2000\n",
            " - 2s - loss: 1.2568e-04 - accuracy: 1.0000 - val_loss: 0.0240 - val_accuracy: 0.9933\n",
            "\n",
            "Epoch 00964: loss did not improve from 0.00008\n",
            "Epoch 965/2000\n",
            " - 2s - loss: 1.7176e-04 - accuracy: 1.0000 - val_loss: 0.0240 - val_accuracy: 0.9933\n",
            "\n",
            "Epoch 00965: loss did not improve from 0.00008\n",
            "Epoch 966/2000\n",
            " - 2s - loss: 1.1399e-04 - accuracy: 1.0000 - val_loss: 0.0241 - val_accuracy: 0.9933\n",
            "\n",
            "Epoch 00966: loss did not improve from 0.00008\n",
            "Epoch 967/2000\n",
            " - 2s - loss: 9.4604e-05 - accuracy: 1.0000 - val_loss: 0.0241 - val_accuracy: 0.9933\n",
            "\n",
            "Epoch 00967: loss did not improve from 0.00008\n",
            "Epoch 968/2000\n",
            " - 2s - loss: 1.0942e-04 - accuracy: 1.0000 - val_loss: 0.0242 - val_accuracy: 0.9933\n",
            "\n",
            "Epoch 00968: loss did not improve from 0.00008\n",
            "Epoch 969/2000\n",
            " - 2s - loss: 1.1580e-04 - accuracy: 1.0000 - val_loss: 0.0243 - val_accuracy: 0.9933\n",
            "\n",
            "Epoch 00969: loss did not improve from 0.00008\n",
            "Epoch 970/2000\n",
            " - 2s - loss: 1.3268e-04 - accuracy: 1.0000 - val_loss: 0.0243 - val_accuracy: 0.9922\n",
            "\n",
            "Epoch 00970: loss did not improve from 0.00008\n",
            "Epoch 971/2000\n",
            " - 2s - loss: 1.0704e-04 - accuracy: 1.0000 - val_loss: 0.0244 - val_accuracy: 0.9922\n",
            "\n",
            "Epoch 00971: loss did not improve from 0.00008\n",
            "Epoch 972/2000\n",
            " - 2s - loss: 9.1344e-05 - accuracy: 1.0000 - val_loss: 0.0244 - val_accuracy: 0.9922\n",
            "\n",
            "Epoch 00972: loss did not improve from 0.00008\n",
            "Epoch 973/2000\n",
            " - 2s - loss: 1.4058e-04 - accuracy: 1.0000 - val_loss: 0.0244 - val_accuracy: 0.9922\n",
            "\n",
            "Epoch 00973: loss did not improve from 0.00008\n",
            "Epoch 974/2000\n",
            " - 2s - loss: 1.7813e-04 - accuracy: 1.0000 - val_loss: 0.0244 - val_accuracy: 0.9922\n",
            "\n",
            "Epoch 00974: loss did not improve from 0.00008\n",
            "Epoch 975/2000\n",
            " - 2s - loss: 1.4783e-04 - accuracy: 1.0000 - val_loss: 0.0243 - val_accuracy: 0.9933\n",
            "\n",
            "Epoch 00975: loss did not improve from 0.00008\n",
            "Epoch 976/2000\n",
            " - 2s - loss: 1.1850e-04 - accuracy: 1.0000 - val_loss: 0.0242 - val_accuracy: 0.9933\n",
            "\n",
            "Epoch 00976: loss did not improve from 0.00008\n",
            "Epoch 977/2000\n",
            " - 2s - loss: 9.4708e-05 - accuracy: 1.0000 - val_loss: 0.0241 - val_accuracy: 0.9933\n",
            "\n",
            "Epoch 00977: loss did not improve from 0.00008\n",
            "Epoch 978/2000\n",
            " - 2s - loss: 1.4174e-04 - accuracy: 1.0000 - val_loss: 0.0240 - val_accuracy: 0.9933\n",
            "\n",
            "Epoch 00978: loss did not improve from 0.00008\n",
            "Epoch 979/2000\n",
            " - 2s - loss: 1.2279e-04 - accuracy: 1.0000 - val_loss: 0.0239 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00979: loss did not improve from 0.00008\n",
            "Epoch 980/2000\n",
            " - 2s - loss: 1.1891e-04 - accuracy: 1.0000 - val_loss: 0.0239 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00980: loss did not improve from 0.00008\n",
            "Epoch 981/2000\n",
            " - 2s - loss: 9.9202e-05 - accuracy: 1.0000 - val_loss: 0.0238 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00981: loss did not improve from 0.00008\n",
            "Epoch 982/2000\n",
            " - 2s - loss: 8.9539e-05 - accuracy: 1.0000 - val_loss: 0.0237 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00982: loss did not improve from 0.00008\n",
            "Epoch 983/2000\n",
            " - 2s - loss: 1.6306e-04 - accuracy: 1.0000 - val_loss: 0.0237 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00983: loss did not improve from 0.00008\n",
            "Epoch 984/2000\n",
            " - 2s - loss: 1.9045e-04 - accuracy: 1.0000 - val_loss: 0.0237 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00984: loss did not improve from 0.00008\n",
            "Epoch 985/2000\n",
            " - 2s - loss: 1.2399e-04 - accuracy: 1.0000 - val_loss: 0.0237 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00985: loss did not improve from 0.00008\n",
            "Epoch 986/2000\n",
            " - 2s - loss: 1.3149e-04 - accuracy: 1.0000 - val_loss: 0.0237 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00986: loss did not improve from 0.00008\n",
            "Epoch 987/2000\n",
            " - 2s - loss: 8.6277e-05 - accuracy: 1.0000 - val_loss: 0.0238 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00987: loss did not improve from 0.00008\n",
            "Epoch 988/2000\n",
            " - 2s - loss: 9.8168e-05 - accuracy: 1.0000 - val_loss: 0.0238 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00988: loss did not improve from 0.00008\n",
            "Epoch 989/2000\n",
            " - 2s - loss: 1.0440e-04 - accuracy: 1.0000 - val_loss: 0.0238 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00989: loss did not improve from 0.00008\n",
            "Epoch 990/2000\n",
            " - 2s - loss: 1.2433e-04 - accuracy: 1.0000 - val_loss: 0.0238 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00990: loss did not improve from 0.00008\n",
            "Epoch 991/2000\n",
            " - 2s - loss: 1.5664e-04 - accuracy: 1.0000 - val_loss: 0.0238 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00991: loss did not improve from 0.00008\n",
            "Epoch 992/2000\n",
            " - 2s - loss: 1.4248e-04 - accuracy: 1.0000 - val_loss: 0.0239 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00992: loss did not improve from 0.00008\n",
            "Epoch 993/2000\n",
            " - 2s - loss: 1.4315e-04 - accuracy: 1.0000 - val_loss: 0.0238 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00993: loss did not improve from 0.00008\n",
            "Epoch 994/2000\n",
            " - 2s - loss: 1.2111e-04 - accuracy: 1.0000 - val_loss: 0.0238 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00994: loss did not improve from 0.00008\n",
            "Epoch 995/2000\n",
            " - 2s - loss: 1.3896e-04 - accuracy: 1.0000 - val_loss: 0.0237 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00995: loss did not improve from 0.00008\n",
            "Epoch 996/2000\n",
            " - 2s - loss: 1.0671e-04 - accuracy: 1.0000 - val_loss: 0.0236 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00996: loss did not improve from 0.00008\n",
            "Epoch 997/2000\n",
            " - 2s - loss: 1.5096e-04 - accuracy: 1.0000 - val_loss: 0.0236 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00997: loss did not improve from 0.00008\n",
            "Epoch 998/2000\n",
            " - 2s - loss: 9.8470e-05 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00998: loss did not improve from 0.00008\n",
            "Epoch 999/2000\n",
            " - 2s - loss: 1.2016e-04 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 00999: loss did not improve from 0.00008\n",
            "Epoch 1000/2000\n",
            " - 2s - loss: 1.0802e-04 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01000: loss did not improve from 0.00008\n",
            "Epoch 1001/2000\n",
            " - 2s - loss: 1.4500e-04 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01001: loss did not improve from 0.00008\n",
            "Epoch 1002/2000\n",
            " - 2s - loss: 1.6815e-04 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01002: loss did not improve from 0.00008\n",
            "Epoch 1003/2000\n",
            " - 2s - loss: 2.0830e-04 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01003: loss did not improve from 0.00008\n",
            "Epoch 1004/2000\n",
            " - 2s - loss: 1.1248e-04 - accuracy: 1.0000 - val_loss: 0.0236 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01004: loss did not improve from 0.00008\n",
            "Epoch 1005/2000\n",
            " - 2s - loss: 1.2033e-04 - accuracy: 1.0000 - val_loss: 0.0237 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01005: loss did not improve from 0.00008\n",
            "Epoch 1006/2000\n",
            " - 2s - loss: 1.4187e-04 - accuracy: 1.0000 - val_loss: 0.0238 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01006: loss did not improve from 0.00008\n",
            "Epoch 1007/2000\n",
            " - 2s - loss: 1.1311e-04 - accuracy: 1.0000 - val_loss: 0.0238 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01007: loss did not improve from 0.00008\n",
            "Epoch 1008/2000\n",
            " - 2s - loss: 9.0290e-05 - accuracy: 1.0000 - val_loss: 0.0238 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01008: loss did not improve from 0.00008\n",
            "Epoch 1009/2000\n",
            " - 2s - loss: 1.0199e-04 - accuracy: 1.0000 - val_loss: 0.0238 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01009: loss did not improve from 0.00008\n",
            "Epoch 1010/2000\n",
            " - 2s - loss: 1.1245e-04 - accuracy: 1.0000 - val_loss: 0.0238 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01010: loss did not improve from 0.00008\n",
            "Epoch 1011/2000\n",
            " - 2s - loss: 1.3055e-04 - accuracy: 1.0000 - val_loss: 0.0237 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01011: loss did not improve from 0.00008\n",
            "Epoch 1012/2000\n",
            " - 2s - loss: 1.3552e-04 - accuracy: 1.0000 - val_loss: 0.0236 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01012: loss did not improve from 0.00008\n",
            "Epoch 1013/2000\n",
            " - 2s - loss: 9.6951e-05 - accuracy: 1.0000 - val_loss: 0.0236 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01013: loss did not improve from 0.00008\n",
            "Epoch 1014/2000\n",
            " - 2s - loss: 1.4637e-04 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01014: loss did not improve from 0.00008\n",
            "Epoch 1015/2000\n",
            " - 2s - loss: 1.2726e-04 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01015: loss did not improve from 0.00008\n",
            "Epoch 1016/2000\n",
            " - 2s - loss: 1.3574e-04 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01016: loss did not improve from 0.00008\n",
            "Epoch 1017/2000\n",
            " - 2s - loss: 1.5199e-04 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01017: loss did not improve from 0.00008\n",
            "Epoch 1018/2000\n",
            " - 2s - loss: 1.2281e-04 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01018: loss did not improve from 0.00008\n",
            "Epoch 1019/2000\n",
            " - 2s - loss: 1.5336e-04 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01019: loss did not improve from 0.00008\n",
            "Epoch 1020/2000\n",
            " - 2s - loss: 1.3986e-04 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01020: loss did not improve from 0.00008\n",
            "Epoch 1021/2000\n",
            " - 2s - loss: 1.2967e-04 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01021: loss did not improve from 0.00008\n",
            "Epoch 1022/2000\n",
            " - 2s - loss: 1.2652e-04 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01022: loss did not improve from 0.00008\n",
            "Epoch 1023/2000\n",
            " - 2s - loss: 1.3094e-04 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01023: loss did not improve from 0.00008\n",
            "Epoch 1024/2000\n",
            " - 2s - loss: 1.3156e-04 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9956\n",
            "\n",
            "Epoch 01024: loss did not improve from 0.00008\n",
            "Epoch 1025/2000\n",
            " - 2s - loss: 9.2167e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9956\n",
            "\n",
            "Epoch 01025: loss did not improve from 0.00008\n",
            "Epoch 1026/2000\n",
            " - 2s - loss: 1.1564e-04 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9956\n",
            "\n",
            "Epoch 01026: loss did not improve from 0.00008\n",
            "Epoch 1027/2000\n",
            " - 2s - loss: 1.3947e-04 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9956\n",
            "\n",
            "Epoch 01027: loss did not improve from 0.00008\n",
            "Epoch 1028/2000\n",
            " - 2s - loss: 8.7919e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9956\n",
            "\n",
            "Epoch 01028: loss did not improve from 0.00008\n",
            "Epoch 1029/2000\n",
            " - 2s - loss: 1.0973e-04 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9967\n",
            "\n",
            "Epoch 01029: loss did not improve from 0.00008\n",
            "Epoch 1030/2000\n",
            " - 2s - loss: 9.0745e-05 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9967\n",
            "\n",
            "Epoch 01030: loss did not improve from 0.00008\n",
            "Epoch 1031/2000\n",
            " - 2s - loss: 1.0240e-04 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9967\n",
            "\n",
            "Epoch 01031: loss did not improve from 0.00008\n",
            "Epoch 1032/2000\n",
            " - 2s - loss: 1.2258e-04 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9967\n",
            "\n",
            "Epoch 01032: loss did not improve from 0.00008\n",
            "Epoch 1033/2000\n",
            " - 2s - loss: 8.8188e-05 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9967\n",
            "\n",
            "Epoch 01033: loss did not improve from 0.00008\n",
            "Epoch 1034/2000\n",
            " - 2s - loss: 1.2723e-04 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9967\n",
            "\n",
            "Epoch 01034: loss did not improve from 0.00008\n",
            "Epoch 1035/2000\n",
            " - 2s - loss: 1.2663e-04 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9967\n",
            "\n",
            "Epoch 01035: loss did not improve from 0.00008\n",
            "\n",
            "Epoch 01035: ReduceLROnPlateau reducing learning rate to 0.0002500000136362085.\n",
            "Epoch 1036/2000\n",
            " - 2s - loss: 1.1628e-04 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9967\n",
            "\n",
            "Epoch 01036: loss did not improve from 0.00008\n",
            "Epoch 1037/2000\n",
            " - 2s - loss: 9.7684e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9967\n",
            "\n",
            "Epoch 01037: loss did not improve from 0.00008\n",
            "Epoch 1038/2000\n",
            " - 2s - loss: 1.1649e-04 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9967\n",
            "\n",
            "Epoch 01038: loss did not improve from 0.00008\n",
            "Epoch 1039/2000\n",
            " - 2s - loss: 7.6781e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9967\n",
            "\n",
            "Epoch 01039: loss did not improve from 0.00008\n",
            "Epoch 1040/2000\n",
            " - 2s - loss: 1.1386e-04 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9956\n",
            "\n",
            "Epoch 01040: loss did not improve from 0.00008\n",
            "Epoch 1041/2000\n",
            " - 2s - loss: 9.0372e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9956\n",
            "\n",
            "Epoch 01041: loss did not improve from 0.00008\n",
            "Epoch 1042/2000\n",
            " - 2s - loss: 1.0380e-04 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9956\n",
            "\n",
            "Epoch 01042: loss did not improve from 0.00008\n",
            "Epoch 1043/2000\n",
            " - 2s - loss: 7.1219e-05 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9956\n",
            "\n",
            "Epoch 01043: loss improved from 0.00008 to 0.00007, saving model to ./weights/lstmfcn_128_cells_weights/CBF_weights.h5\n",
            "Epoch 1044/2000\n",
            " - 2s - loss: 1.6590e-04 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9956\n",
            "\n",
            "Epoch 01044: loss did not improve from 0.00007\n",
            "Epoch 1045/2000\n",
            " - 2s - loss: 9.0162e-05 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9956\n",
            "\n",
            "Epoch 01045: loss did not improve from 0.00007\n",
            "Epoch 1046/2000\n",
            " - 2s - loss: 1.5275e-04 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01046: loss did not improve from 0.00007\n",
            "Epoch 1047/2000\n",
            " - 2s - loss: 1.2167e-04 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01047: loss did not improve from 0.00007\n",
            "Epoch 1048/2000\n",
            " - 2s - loss: 9.5520e-05 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01048: loss did not improve from 0.00007\n",
            "Epoch 1049/2000\n",
            " - 2s - loss: 9.4518e-05 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01049: loss did not improve from 0.00007\n",
            "Epoch 1050/2000\n",
            " - 2s - loss: 1.0786e-04 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01050: loss did not improve from 0.00007\n",
            "Epoch 1051/2000\n",
            " - 2s - loss: 9.5097e-05 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01051: loss did not improve from 0.00007\n",
            "Epoch 1052/2000\n",
            " - 2s - loss: 1.1614e-04 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01052: loss did not improve from 0.00007\n",
            "Epoch 1053/2000\n",
            " - 2s - loss: 1.2416e-04 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01053: loss did not improve from 0.00007\n",
            "Epoch 1054/2000\n",
            " - 2s - loss: 1.2892e-04 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01054: loss did not improve from 0.00007\n",
            "Epoch 1055/2000\n",
            " - 2s - loss: 1.0284e-04 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01055: loss did not improve from 0.00007\n",
            "Epoch 1056/2000\n",
            " - 2s - loss: 1.2262e-04 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01056: loss did not improve from 0.00007\n",
            "Epoch 1057/2000\n",
            " - 2s - loss: 1.2113e-04 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01057: loss did not improve from 0.00007\n",
            "Epoch 1058/2000\n",
            " - 2s - loss: 8.5498e-05 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01058: loss did not improve from 0.00007\n",
            "Epoch 1059/2000\n",
            " - 2s - loss: 1.1040e-04 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01059: loss did not improve from 0.00007\n",
            "Epoch 1060/2000\n",
            " - 2s - loss: 1.1688e-04 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01060: loss did not improve from 0.00007\n",
            "Epoch 1061/2000\n",
            " - 2s - loss: 1.1463e-04 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01061: loss did not improve from 0.00007\n",
            "Epoch 1062/2000\n",
            " - 2s - loss: 8.4410e-05 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01062: loss did not improve from 0.00007\n",
            "Epoch 1063/2000\n",
            " - 2s - loss: 1.3578e-04 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01063: loss did not improve from 0.00007\n",
            "Epoch 1064/2000\n",
            " - 2s - loss: 1.4494e-04 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01064: loss did not improve from 0.00007\n",
            "Epoch 1065/2000\n",
            " - 2s - loss: 9.9665e-05 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01065: loss did not improve from 0.00007\n",
            "Epoch 1066/2000\n",
            " - 2s - loss: 1.1326e-04 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01066: loss did not improve from 0.00007\n",
            "Epoch 1067/2000\n",
            " - 2s - loss: 8.5852e-05 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01067: loss did not improve from 0.00007\n",
            "Epoch 1068/2000\n",
            " - 2s - loss: 1.1735e-04 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01068: loss did not improve from 0.00007\n",
            "Epoch 1069/2000\n",
            " - 2s - loss: 8.4655e-05 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01069: loss did not improve from 0.00007\n",
            "Epoch 1070/2000\n",
            " - 2s - loss: 1.1576e-04 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01070: loss did not improve from 0.00007\n",
            "Epoch 1071/2000\n",
            " - 2s - loss: 9.6469e-05 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01071: loss did not improve from 0.00007\n",
            "Epoch 1072/2000\n",
            " - 2s - loss: 9.6738e-05 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01072: loss did not improve from 0.00007\n",
            "Epoch 1073/2000\n",
            " - 2s - loss: 8.9194e-05 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01073: loss did not improve from 0.00007\n",
            "Epoch 1074/2000\n",
            " - 2s - loss: 1.0528e-04 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01074: loss did not improve from 0.00007\n",
            "Epoch 1075/2000\n",
            " - 2s - loss: 1.3818e-04 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01075: loss did not improve from 0.00007\n",
            "Epoch 1076/2000\n",
            " - 2s - loss: 1.0346e-04 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01076: loss did not improve from 0.00007\n",
            "Epoch 1077/2000\n",
            " - 2s - loss: 8.1810e-05 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01077: loss did not improve from 0.00007\n",
            "Epoch 1078/2000\n",
            " - 2s - loss: 9.5268e-05 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01078: loss did not improve from 0.00007\n",
            "Epoch 1079/2000\n",
            " - 2s - loss: 1.0795e-04 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01079: loss did not improve from 0.00007\n",
            "Epoch 1080/2000\n",
            " - 2s - loss: 1.0317e-04 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01080: loss did not improve from 0.00007\n",
            "Epoch 1081/2000\n",
            " - 2s - loss: 1.2690e-04 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01081: loss did not improve from 0.00007\n",
            "Epoch 1082/2000\n",
            " - 2s - loss: 1.3813e-04 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01082: loss did not improve from 0.00007\n",
            "Epoch 1083/2000\n",
            " - 2s - loss: 1.3707e-04 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01083: loss did not improve from 0.00007\n",
            "Epoch 1084/2000\n",
            " - 2s - loss: 9.3842e-05 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01084: loss did not improve from 0.00007\n",
            "Epoch 1085/2000\n",
            " - 2s - loss: 1.0895e-04 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01085: loss did not improve from 0.00007\n",
            "Epoch 1086/2000\n",
            " - 2s - loss: 1.3266e-04 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01086: loss did not improve from 0.00007\n",
            "Epoch 1087/2000\n",
            " - 2s - loss: 6.6058e-05 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01087: loss improved from 0.00007 to 0.00007, saving model to ./weights/lstmfcn_128_cells_weights/CBF_weights.h5\n",
            "Epoch 1088/2000\n",
            " - 2s - loss: 1.0171e-04 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01088: loss did not improve from 0.00007\n",
            "Epoch 1089/2000\n",
            " - 2s - loss: 7.4759e-05 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01089: loss did not improve from 0.00007\n",
            "Epoch 1090/2000\n",
            " - 2s - loss: 9.5650e-05 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01090: loss did not improve from 0.00007\n",
            "Epoch 1091/2000\n",
            " - 2s - loss: 1.2970e-04 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01091: loss did not improve from 0.00007\n",
            "Epoch 1092/2000\n",
            " - 2s - loss: 1.3746e-04 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01092: loss did not improve from 0.00007\n",
            "Epoch 1093/2000\n",
            " - 2s - loss: 1.3840e-04 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01093: loss did not improve from 0.00007\n",
            "Epoch 1094/2000\n",
            " - 2s - loss: 1.6026e-04 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01094: loss did not improve from 0.00007\n",
            "Epoch 1095/2000\n",
            " - 2s - loss: 1.1166e-04 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01095: loss did not improve from 0.00007\n",
            "Epoch 1096/2000\n",
            " - 2s - loss: 1.4799e-04 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01096: loss did not improve from 0.00007\n",
            "Epoch 1097/2000\n",
            " - 2s - loss: 1.0277e-04 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01097: loss did not improve from 0.00007\n",
            "Epoch 1098/2000\n",
            " - 2s - loss: 1.3136e-04 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01098: loss did not improve from 0.00007\n",
            "Epoch 1099/2000\n",
            " - 2s - loss: 1.2572e-04 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01099: loss did not improve from 0.00007\n",
            "Epoch 1100/2000\n",
            " - 2s - loss: 9.7217e-05 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01100: loss did not improve from 0.00007\n",
            "Epoch 1101/2000\n",
            " - 2s - loss: 9.8342e-05 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01101: loss did not improve from 0.00007\n",
            "Epoch 1102/2000\n",
            " - 2s - loss: 1.0787e-04 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01102: loss did not improve from 0.00007\n",
            "Epoch 1103/2000\n",
            " - 2s - loss: 1.2371e-04 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01103: loss did not improve from 0.00007\n",
            "Epoch 1104/2000\n",
            " - 2s - loss: 1.0013e-04 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01104: loss did not improve from 0.00007\n",
            "Epoch 1105/2000\n",
            " - 2s - loss: 9.4992e-05 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01105: loss did not improve from 0.00007\n",
            "Epoch 1106/2000\n",
            " - 2s - loss: 9.8751e-05 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01106: loss did not improve from 0.00007\n",
            "Epoch 1107/2000\n",
            " - 2s - loss: 8.6845e-05 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01107: loss did not improve from 0.00007\n",
            "Epoch 1108/2000\n",
            " - 2s - loss: 8.5704e-05 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01108: loss did not improve from 0.00007\n",
            "Epoch 1109/2000\n",
            " - 2s - loss: 1.3747e-04 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01109: loss did not improve from 0.00007\n",
            "Epoch 1110/2000\n",
            " - 2s - loss: 1.0094e-04 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01110: loss did not improve from 0.00007\n",
            "Epoch 1111/2000\n",
            " - 2s - loss: 1.0604e-04 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01111: loss did not improve from 0.00007\n",
            "Epoch 1112/2000\n",
            " - 2s - loss: 1.2847e-04 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01112: loss did not improve from 0.00007\n",
            "Epoch 1113/2000\n",
            " - 2s - loss: 8.3603e-05 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01113: loss did not improve from 0.00007\n",
            "Epoch 1114/2000\n",
            " - 2s - loss: 9.8343e-05 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01114: loss did not improve from 0.00007\n",
            "Epoch 1115/2000\n",
            " - 2s - loss: 9.2923e-05 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01115: loss did not improve from 0.00007\n",
            "Epoch 1116/2000\n",
            " - 2s - loss: 1.4894e-04 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01116: loss did not improve from 0.00007\n",
            "Epoch 1117/2000\n",
            " - 2s - loss: 1.4615e-04 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01117: loss did not improve from 0.00007\n",
            "Epoch 1118/2000\n",
            " - 2s - loss: 1.4972e-04 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01118: loss did not improve from 0.00007\n",
            "Epoch 1119/2000\n",
            " - 2s - loss: 1.5207e-04 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01119: loss did not improve from 0.00007\n",
            "Epoch 1120/2000\n",
            " - 2s - loss: 1.2638e-04 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01120: loss did not improve from 0.00007\n",
            "Epoch 1121/2000\n",
            " - 2s - loss: 7.6241e-05 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01121: loss did not improve from 0.00007\n",
            "Epoch 1122/2000\n",
            " - 2s - loss: 1.3267e-04 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01122: loss did not improve from 0.00007\n",
            "Epoch 1123/2000\n",
            " - 2s - loss: 1.1524e-04 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01123: loss did not improve from 0.00007\n",
            "Epoch 1124/2000\n",
            " - 2s - loss: 8.1872e-05 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01124: loss did not improve from 0.00007\n",
            "Epoch 1125/2000\n",
            " - 2s - loss: 1.2873e-04 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01125: loss did not improve from 0.00007\n",
            "Epoch 1126/2000\n",
            " - 2s - loss: 1.1775e-04 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01126: loss did not improve from 0.00007\n",
            "Epoch 1127/2000\n",
            " - 2s - loss: 1.0107e-04 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01127: loss did not improve from 0.00007\n",
            "Epoch 1128/2000\n",
            " - 2s - loss: 1.1213e-04 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01128: loss did not improve from 0.00007\n",
            "Epoch 1129/2000\n",
            " - 2s - loss: 1.6795e-04 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01129: loss did not improve from 0.00007\n",
            "Epoch 1130/2000\n",
            " - 2s - loss: 1.2789e-04 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01130: loss did not improve from 0.00007\n",
            "Epoch 1131/2000\n",
            " - 2s - loss: 8.8490e-05 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01131: loss did not improve from 0.00007\n",
            "Epoch 1132/2000\n",
            " - 2s - loss: 1.1711e-04 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01132: loss did not improve from 0.00007\n",
            "Epoch 1133/2000\n",
            " - 2s - loss: 9.7365e-05 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01133: loss did not improve from 0.00007\n",
            "Epoch 1134/2000\n",
            " - 2s - loss: 1.6787e-04 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01134: loss did not improve from 0.00007\n",
            "Epoch 1135/2000\n",
            " - 2s - loss: 9.5082e-05 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01135: loss did not improve from 0.00007\n",
            "\n",
            "Epoch 01135: ReduceLROnPlateau reducing learning rate to 0.0001984251409207129.\n",
            "Epoch 1136/2000\n",
            " - 2s - loss: 8.3674e-05 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01136: loss did not improve from 0.00007\n",
            "Epoch 1137/2000\n",
            " - 2s - loss: 1.0102e-04 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01137: loss did not improve from 0.00007\n",
            "Epoch 1138/2000\n",
            " - 2s - loss: 9.3053e-05 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01138: loss did not improve from 0.00007\n",
            "Epoch 1139/2000\n",
            " - 2s - loss: 1.6115e-04 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01139: loss did not improve from 0.00007\n",
            "Epoch 1140/2000\n",
            " - 2s - loss: 9.6042e-05 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01140: loss did not improve from 0.00007\n",
            "Epoch 1141/2000\n",
            " - 2s - loss: 1.1983e-04 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01141: loss did not improve from 0.00007\n",
            "Epoch 1142/2000\n",
            " - 2s - loss: 9.4460e-05 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01142: loss did not improve from 0.00007\n",
            "Epoch 1143/2000\n",
            " - 2s - loss: 1.2230e-04 - accuracy: 1.0000 - val_loss: 0.0236 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01143: loss did not improve from 0.00007\n",
            "Epoch 1144/2000\n",
            " - 2s - loss: 9.7509e-05 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01144: loss did not improve from 0.00007\n",
            "Epoch 1145/2000\n",
            " - 2s - loss: 9.3158e-05 - accuracy: 1.0000 - val_loss: 0.0236 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01145: loss did not improve from 0.00007\n",
            "Epoch 1146/2000\n",
            " - 2s - loss: 9.0544e-05 - accuracy: 1.0000 - val_loss: 0.0236 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01146: loss did not improve from 0.00007\n",
            "Epoch 1147/2000\n",
            " - 2s - loss: 9.7458e-05 - accuracy: 1.0000 - val_loss: 0.0236 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01147: loss did not improve from 0.00007\n",
            "Epoch 1148/2000\n",
            " - 2s - loss: 1.7177e-04 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01148: loss did not improve from 0.00007\n",
            "Epoch 1149/2000\n",
            " - 2s - loss: 1.1100e-04 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01149: loss did not improve from 0.00007\n",
            "Epoch 1150/2000\n",
            " - 2s - loss: 9.2801e-05 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01150: loss did not improve from 0.00007\n",
            "Epoch 1151/2000\n",
            " - 2s - loss: 1.4330e-04 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01151: loss did not improve from 0.00007\n",
            "Epoch 1152/2000\n",
            " - 2s - loss: 1.6044e-04 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01152: loss did not improve from 0.00007\n",
            "Epoch 1153/2000\n",
            " - 2s - loss: 1.1627e-04 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01153: loss did not improve from 0.00007\n",
            "Epoch 1154/2000\n",
            " - 2s - loss: 1.0599e-04 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01154: loss did not improve from 0.00007\n",
            "Epoch 1155/2000\n",
            " - 2s - loss: 1.0049e-04 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01155: loss did not improve from 0.00007\n",
            "Epoch 1156/2000\n",
            " - 2s - loss: 8.9659e-05 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01156: loss did not improve from 0.00007\n",
            "Epoch 1157/2000\n",
            " - 2s - loss: 8.5366e-05 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01157: loss did not improve from 0.00007\n",
            "Epoch 1158/2000\n",
            " - 2s - loss: 1.1592e-04 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01158: loss did not improve from 0.00007\n",
            "Epoch 1159/2000\n",
            " - 2s - loss: 1.0775e-04 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01159: loss did not improve from 0.00007\n",
            "Epoch 1160/2000\n",
            " - 2s - loss: 7.8011e-05 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01160: loss did not improve from 0.00007\n",
            "Epoch 1161/2000\n",
            " - 2s - loss: 7.7724e-05 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01161: loss did not improve from 0.00007\n",
            "Epoch 1162/2000\n",
            " - 2s - loss: 8.7091e-05 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01162: loss did not improve from 0.00007\n",
            "Epoch 1163/2000\n",
            " - 2s - loss: 1.2559e-04 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01163: loss did not improve from 0.00007\n",
            "Epoch 1164/2000\n",
            " - 2s - loss: 8.3194e-05 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01164: loss did not improve from 0.00007\n",
            "Epoch 1165/2000\n",
            " - 2s - loss: 1.1697e-04 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01165: loss did not improve from 0.00007\n",
            "Epoch 1166/2000\n",
            " - 2s - loss: 1.3097e-04 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01166: loss did not improve from 0.00007\n",
            "Epoch 1167/2000\n",
            " - 2s - loss: 1.1948e-04 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01167: loss did not improve from 0.00007\n",
            "Epoch 1168/2000\n",
            " - 2s - loss: 1.1789e-04 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01168: loss did not improve from 0.00007\n",
            "Epoch 1169/2000\n",
            " - 2s - loss: 8.4511e-05 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01169: loss did not improve from 0.00007\n",
            "Epoch 1170/2000\n",
            " - 2s - loss: 8.2506e-05 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01170: loss did not improve from 0.00007\n",
            "Epoch 1171/2000\n",
            " - 2s - loss: 1.1539e-04 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01171: loss did not improve from 0.00007\n",
            "Epoch 1172/2000\n",
            " - 2s - loss: 9.1504e-05 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01172: loss did not improve from 0.00007\n",
            "Epoch 1173/2000\n",
            " - 2s - loss: 6.6828e-05 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01173: loss did not improve from 0.00007\n",
            "Epoch 1174/2000\n",
            " - 2s - loss: 8.6972e-05 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01174: loss did not improve from 0.00007\n",
            "Epoch 1175/2000\n",
            " - 2s - loss: 1.2273e-04 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01175: loss did not improve from 0.00007\n",
            "Epoch 1176/2000\n",
            " - 2s - loss: 1.2420e-04 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01176: loss did not improve from 0.00007\n",
            "Epoch 1177/2000\n",
            " - 2s - loss: 1.4595e-04 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01177: loss did not improve from 0.00007\n",
            "Epoch 1178/2000\n",
            " - 2s - loss: 1.3691e-04 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01178: loss did not improve from 0.00007\n",
            "Epoch 1179/2000\n",
            " - 2s - loss: 7.4066e-05 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01179: loss did not improve from 0.00007\n",
            "Epoch 1180/2000\n",
            " - 2s - loss: 1.0018e-04 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01180: loss did not improve from 0.00007\n",
            "Epoch 1181/2000\n",
            " - 2s - loss: 8.6757e-05 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01181: loss did not improve from 0.00007\n",
            "Epoch 1182/2000\n",
            " - 2s - loss: 2.3282e-04 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01182: loss did not improve from 0.00007\n",
            "Epoch 1183/2000\n",
            " - 2s - loss: 9.5346e-05 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01183: loss did not improve from 0.00007\n",
            "Epoch 1184/2000\n",
            " - 2s - loss: 6.9253e-05 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01184: loss did not improve from 0.00007\n",
            "Epoch 1185/2000\n",
            " - 2s - loss: 7.6205e-05 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01185: loss did not improve from 0.00007\n",
            "Epoch 1186/2000\n",
            " - 2s - loss: 9.3762e-05 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01186: loss did not improve from 0.00007\n",
            "Epoch 1187/2000\n",
            " - 2s - loss: 7.6460e-05 - accuracy: 1.0000 - val_loss: 0.0236 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01187: loss did not improve from 0.00007\n",
            "Epoch 1188/2000\n",
            " - 2s - loss: 9.8679e-05 - accuracy: 1.0000 - val_loss: 0.0236 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01188: loss did not improve from 0.00007\n",
            "Epoch 1189/2000\n",
            " - 2s - loss: 1.0795e-04 - accuracy: 1.0000 - val_loss: 0.0237 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01189: loss did not improve from 0.00007\n",
            "Epoch 1190/2000\n",
            " - 2s - loss: 1.1423e-04 - accuracy: 1.0000 - val_loss: 0.0237 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01190: loss did not improve from 0.00007\n",
            "Epoch 1191/2000\n",
            " - 2s - loss: 9.3414e-05 - accuracy: 1.0000 - val_loss: 0.0238 - val_accuracy: 0.9933\n",
            "\n",
            "Epoch 01191: loss did not improve from 0.00007\n",
            "Epoch 1192/2000\n",
            " - 2s - loss: 1.1066e-04 - accuracy: 1.0000 - val_loss: 0.0238 - val_accuracy: 0.9933\n",
            "\n",
            "Epoch 01192: loss did not improve from 0.00007\n",
            "Epoch 1193/2000\n",
            " - 2s - loss: 1.1268e-04 - accuracy: 1.0000 - val_loss: 0.0238 - val_accuracy: 0.9933\n",
            "\n",
            "Epoch 01193: loss did not improve from 0.00007\n",
            "Epoch 1194/2000\n",
            " - 2s - loss: 7.4895e-05 - accuracy: 1.0000 - val_loss: 0.0239 - val_accuracy: 0.9933\n",
            "\n",
            "Epoch 01194: loss did not improve from 0.00007\n",
            "Epoch 1195/2000\n",
            " - 2s - loss: 9.3754e-05 - accuracy: 1.0000 - val_loss: 0.0239 - val_accuracy: 0.9933\n",
            "\n",
            "Epoch 01195: loss did not improve from 0.00007\n",
            "Epoch 1196/2000\n",
            " - 2s - loss: 1.0510e-04 - accuracy: 1.0000 - val_loss: 0.0239 - val_accuracy: 0.9933\n",
            "\n",
            "Epoch 01196: loss did not improve from 0.00007\n",
            "Epoch 1197/2000\n",
            " - 2s - loss: 8.3214e-05 - accuracy: 1.0000 - val_loss: 0.0239 - val_accuracy: 0.9933\n",
            "\n",
            "Epoch 01197: loss did not improve from 0.00007\n",
            "Epoch 1198/2000\n",
            " - 2s - loss: 1.0249e-04 - accuracy: 1.0000 - val_loss: 0.0239 - val_accuracy: 0.9933\n",
            "\n",
            "Epoch 01198: loss did not improve from 0.00007\n",
            "Epoch 1199/2000\n",
            " - 2s - loss: 1.1429e-04 - accuracy: 1.0000 - val_loss: 0.0239 - val_accuracy: 0.9933\n",
            "\n",
            "Epoch 01199: loss did not improve from 0.00007\n",
            "Epoch 1200/2000\n",
            " - 2s - loss: 8.4669e-05 - accuracy: 1.0000 - val_loss: 0.0238 - val_accuracy: 0.9933\n",
            "\n",
            "Epoch 01200: loss did not improve from 0.00007\n",
            "Epoch 1201/2000\n",
            " - 2s - loss: 9.8508e-05 - accuracy: 1.0000 - val_loss: 0.0238 - val_accuracy: 0.9933\n",
            "\n",
            "Epoch 01201: loss did not improve from 0.00007\n",
            "Epoch 1202/2000\n",
            " - 2s - loss: 1.0826e-04 - accuracy: 1.0000 - val_loss: 0.0237 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01202: loss did not improve from 0.00007\n",
            "Epoch 1203/2000\n",
            " - 2s - loss: 1.3743e-04 - accuracy: 1.0000 - val_loss: 0.0236 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01203: loss did not improve from 0.00007\n",
            "Epoch 1204/2000\n",
            " - 2s - loss: 8.2360e-05 - accuracy: 1.0000 - val_loss: 0.0236 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01204: loss did not improve from 0.00007\n",
            "Epoch 1205/2000\n",
            " - 2s - loss: 1.0048e-04 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01205: loss did not improve from 0.00007\n",
            "Epoch 1206/2000\n",
            " - 2s - loss: 8.6009e-05 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01206: loss did not improve from 0.00007\n",
            "Epoch 1207/2000\n",
            " - 2s - loss: 1.0017e-04 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01207: loss did not improve from 0.00007\n",
            "Epoch 1208/2000\n",
            " - 2s - loss: 8.6805e-05 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01208: loss did not improve from 0.00007\n",
            "Epoch 1209/2000\n",
            " - 2s - loss: 1.5323e-04 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01209: loss did not improve from 0.00007\n",
            "Epoch 1210/2000\n",
            " - 2s - loss: 7.8942e-05 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01210: loss did not improve from 0.00007\n",
            "Epoch 1211/2000\n",
            " - 2s - loss: 8.3848e-05 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01211: loss did not improve from 0.00007\n",
            "Epoch 1212/2000\n",
            " - 2s - loss: 8.4227e-05 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01212: loss did not improve from 0.00007\n",
            "Epoch 1213/2000\n",
            " - 2s - loss: 1.0715e-04 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01213: loss did not improve from 0.00007\n",
            "Epoch 1214/2000\n",
            " - 2s - loss: 1.0661e-04 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01214: loss did not improve from 0.00007\n",
            "Epoch 1215/2000\n",
            " - 2s - loss: 8.8603e-05 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01215: loss did not improve from 0.00007\n",
            "Epoch 1216/2000\n",
            " - 2s - loss: 1.2932e-04 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01216: loss did not improve from 0.00007\n",
            "Epoch 1217/2000\n",
            " - 2s - loss: 1.3351e-04 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01217: loss did not improve from 0.00007\n",
            "Epoch 1218/2000\n",
            " - 2s - loss: 8.7866e-05 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01218: loss did not improve from 0.00007\n",
            "Epoch 1219/2000\n",
            " - 2s - loss: 9.9066e-05 - accuracy: 1.0000 - val_loss: 0.0236 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01219: loss did not improve from 0.00007\n",
            "Epoch 1220/2000\n",
            " - 2s - loss: 9.3077e-05 - accuracy: 1.0000 - val_loss: 0.0236 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01220: loss did not improve from 0.00007\n",
            "Epoch 1221/2000\n",
            " - 2s - loss: 8.2388e-05 - accuracy: 1.0000 - val_loss: 0.0237 - val_accuracy: 0.9933\n",
            "\n",
            "Epoch 01221: loss did not improve from 0.00007\n",
            "Epoch 1222/2000\n",
            " - 2s - loss: 7.9219e-05 - accuracy: 1.0000 - val_loss: 0.0238 - val_accuracy: 0.9933\n",
            "\n",
            "Epoch 01222: loss did not improve from 0.00007\n",
            "Epoch 1223/2000\n",
            " - 2s - loss: 1.3284e-04 - accuracy: 1.0000 - val_loss: 0.0238 - val_accuracy: 0.9933\n",
            "\n",
            "Epoch 01223: loss did not improve from 0.00007\n",
            "Epoch 1224/2000\n",
            " - 2s - loss: 8.7170e-05 - accuracy: 1.0000 - val_loss: 0.0239 - val_accuracy: 0.9933\n",
            "\n",
            "Epoch 01224: loss did not improve from 0.00007\n",
            "Epoch 1225/2000\n",
            " - 2s - loss: 1.1826e-04 - accuracy: 1.0000 - val_loss: 0.0239 - val_accuracy: 0.9933\n",
            "\n",
            "Epoch 01225: loss did not improve from 0.00007\n",
            "Epoch 1226/2000\n",
            " - 2s - loss: 9.8259e-05 - accuracy: 1.0000 - val_loss: 0.0239 - val_accuracy: 0.9933\n",
            "\n",
            "Epoch 01226: loss did not improve from 0.00007\n",
            "Epoch 1227/2000\n",
            " - 2s - loss: 1.2938e-04 - accuracy: 1.0000 - val_loss: 0.0239 - val_accuracy: 0.9933\n",
            "\n",
            "Epoch 01227: loss did not improve from 0.00007\n",
            "Epoch 1228/2000\n",
            " - 2s - loss: 1.7117e-04 - accuracy: 1.0000 - val_loss: 0.0239 - val_accuracy: 0.9933\n",
            "\n",
            "Epoch 01228: loss did not improve from 0.00007\n",
            "Epoch 1229/2000\n",
            " - 2s - loss: 8.8395e-05 - accuracy: 1.0000 - val_loss: 0.0239 - val_accuracy: 0.9933\n",
            "\n",
            "Epoch 01229: loss did not improve from 0.00007\n",
            "Epoch 1230/2000\n",
            " - 2s - loss: 8.9351e-05 - accuracy: 1.0000 - val_loss: 0.0239 - val_accuracy: 0.9933\n",
            "\n",
            "Epoch 01230: loss did not improve from 0.00007\n",
            "Epoch 1231/2000\n",
            " - 2s - loss: 1.1099e-04 - accuracy: 1.0000 - val_loss: 0.0239 - val_accuracy: 0.9933\n",
            "\n",
            "Epoch 01231: loss did not improve from 0.00007\n",
            "Epoch 1232/2000\n",
            " - 2s - loss: 1.0366e-04 - accuracy: 1.0000 - val_loss: 0.0239 - val_accuracy: 0.9933\n",
            "\n",
            "Epoch 01232: loss did not improve from 0.00007\n",
            "Epoch 1233/2000\n",
            " - 2s - loss: 1.0307e-04 - accuracy: 1.0000 - val_loss: 0.0238 - val_accuracy: 0.9933\n",
            "\n",
            "Epoch 01233: loss did not improve from 0.00007\n",
            "Epoch 1234/2000\n",
            " - 2s - loss: 9.3519e-05 - accuracy: 1.0000 - val_loss: 0.0238 - val_accuracy: 0.9933\n",
            "\n",
            "Epoch 01234: loss did not improve from 0.00007\n",
            "Epoch 1235/2000\n",
            " - 2s - loss: 1.0040e-04 - accuracy: 1.0000 - val_loss: 0.0237 - val_accuracy: 0.9933\n",
            "\n",
            "Epoch 01235: loss did not improve from 0.00007\n",
            "\n",
            "Epoch 01235: ReduceLROnPlateau reducing learning rate to 0.00015749014038805994.\n",
            "Epoch 1236/2000\n",
            " - 2s - loss: 1.0293e-04 - accuracy: 1.0000 - val_loss: 0.0237 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01236: loss did not improve from 0.00007\n",
            "Epoch 1237/2000\n",
            " - 2s - loss: 1.0592e-04 - accuracy: 1.0000 - val_loss: 0.0237 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01237: loss did not improve from 0.00007\n",
            "Epoch 1238/2000\n",
            " - 2s - loss: 1.6426e-04 - accuracy: 1.0000 - val_loss: 0.0237 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01238: loss did not improve from 0.00007\n",
            "Epoch 1239/2000\n",
            " - 2s - loss: 7.0336e-05 - accuracy: 1.0000 - val_loss: 0.0237 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01239: loss did not improve from 0.00007\n",
            "Epoch 1240/2000\n",
            " - 2s - loss: 1.1228e-04 - accuracy: 1.0000 - val_loss: 0.0237 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01240: loss did not improve from 0.00007\n",
            "Epoch 1241/2000\n",
            " - 2s - loss: 1.1572e-04 - accuracy: 1.0000 - val_loss: 0.0237 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01241: loss did not improve from 0.00007\n",
            "Epoch 1242/2000\n",
            " - 2s - loss: 1.0533e-04 - accuracy: 1.0000 - val_loss: 0.0236 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01242: loss did not improve from 0.00007\n",
            "Epoch 1243/2000\n",
            " - 2s - loss: 1.3930e-04 - accuracy: 1.0000 - val_loss: 0.0236 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01243: loss did not improve from 0.00007\n",
            "Epoch 1244/2000\n",
            " - 2s - loss: 1.1794e-04 - accuracy: 1.0000 - val_loss: 0.0236 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01244: loss did not improve from 0.00007\n",
            "Epoch 1245/2000\n",
            " - 2s - loss: 1.0734e-04 - accuracy: 1.0000 - val_loss: 0.0236 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01245: loss did not improve from 0.00007\n",
            "Epoch 1246/2000\n",
            " - 2s - loss: 8.9363e-05 - accuracy: 1.0000 - val_loss: 0.0236 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01246: loss did not improve from 0.00007\n",
            "Epoch 1247/2000\n",
            " - 2s - loss: 1.0064e-04 - accuracy: 1.0000 - val_loss: 0.0236 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01247: loss did not improve from 0.00007\n",
            "Epoch 1248/2000\n",
            " - 2s - loss: 1.0618e-04 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01248: loss did not improve from 0.00007\n",
            "Epoch 1249/2000\n",
            " - 2s - loss: 1.1600e-04 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01249: loss did not improve from 0.00007\n",
            "Epoch 1250/2000\n",
            " - 2s - loss: 1.1319e-04 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01250: loss did not improve from 0.00007\n",
            "Epoch 1251/2000\n",
            " - 2s - loss: 1.0933e-04 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01251: loss did not improve from 0.00007\n",
            "Epoch 1252/2000\n",
            " - 2s - loss: 7.1935e-05 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01252: loss did not improve from 0.00007\n",
            "Epoch 1253/2000\n",
            " - 2s - loss: 7.9188e-05 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01253: loss did not improve from 0.00007\n",
            "Epoch 1254/2000\n",
            " - 2s - loss: 1.0159e-04 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01254: loss did not improve from 0.00007\n",
            "Epoch 1255/2000\n",
            " - 2s - loss: 8.9393e-05 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01255: loss did not improve from 0.00007\n",
            "Epoch 1256/2000\n",
            " - 2s - loss: 1.0689e-04 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01256: loss did not improve from 0.00007\n",
            "Epoch 1257/2000\n",
            " - 2s - loss: 8.4965e-05 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01257: loss did not improve from 0.00007\n",
            "Epoch 1258/2000\n",
            " - 2s - loss: 8.9796e-05 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01258: loss did not improve from 0.00007\n",
            "Epoch 1259/2000\n",
            " - 2s - loss: 7.7508e-05 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01259: loss did not improve from 0.00007\n",
            "Epoch 1260/2000\n",
            " - 2s - loss: 1.2409e-04 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01260: loss did not improve from 0.00007\n",
            "Epoch 1261/2000\n",
            " - 2s - loss: 7.7440e-05 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01261: loss did not improve from 0.00007\n",
            "Epoch 1262/2000\n",
            " - 2s - loss: 1.0522e-04 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01262: loss did not improve from 0.00007\n",
            "Epoch 1263/2000\n",
            " - 2s - loss: 1.1740e-04 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01263: loss did not improve from 0.00007\n",
            "Epoch 1264/2000\n",
            " - 2s - loss: 7.8433e-05 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01264: loss did not improve from 0.00007\n",
            "Epoch 1265/2000\n",
            " - 2s - loss: 1.2996e-04 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01265: loss did not improve from 0.00007\n",
            "Epoch 1266/2000\n",
            " - 2s - loss: 9.0974e-05 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01266: loss did not improve from 0.00007\n",
            "Epoch 1267/2000\n",
            " - 2s - loss: 1.4508e-04 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01267: loss did not improve from 0.00007\n",
            "Epoch 1268/2000\n",
            " - 2s - loss: 7.6781e-05 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01268: loss did not improve from 0.00007\n",
            "Epoch 1269/2000\n",
            " - 2s - loss: 8.6114e-05 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01269: loss did not improve from 0.00007\n",
            "Epoch 1270/2000\n",
            " - 2s - loss: 1.1807e-04 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01270: loss did not improve from 0.00007\n",
            "Epoch 1271/2000\n",
            " - 2s - loss: 7.8327e-05 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01271: loss did not improve from 0.00007\n",
            "Epoch 1272/2000\n",
            " - 2s - loss: 1.0699e-04 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01272: loss did not improve from 0.00007\n",
            "Epoch 1273/2000\n",
            " - 2s - loss: 8.7286e-05 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01273: loss did not improve from 0.00007\n",
            "Epoch 1274/2000\n",
            " - 2s - loss: 9.6515e-05 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01274: loss did not improve from 0.00007\n",
            "Epoch 1275/2000\n",
            " - 2s - loss: 7.6968e-05 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01275: loss did not improve from 0.00007\n",
            "Epoch 1276/2000\n",
            " - 2s - loss: 1.0544e-04 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01276: loss did not improve from 0.00007\n",
            "Epoch 1277/2000\n",
            " - 2s - loss: 1.1348e-04 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01277: loss did not improve from 0.00007\n",
            "Epoch 1278/2000\n",
            " - 2s - loss: 9.5980e-05 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01278: loss did not improve from 0.00007\n",
            "Epoch 1279/2000\n",
            " - 2s - loss: 9.1721e-05 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01279: loss did not improve from 0.00007\n",
            "Epoch 1280/2000\n",
            " - 2s - loss: 9.9546e-05 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01280: loss did not improve from 0.00007\n",
            "Epoch 1281/2000\n",
            " - 2s - loss: 9.8908e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01281: loss did not improve from 0.00007\n",
            "Epoch 1282/2000\n",
            " - 2s - loss: 9.6749e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01282: loss did not improve from 0.00007\n",
            "Epoch 1283/2000\n",
            " - 2s - loss: 8.0461e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01283: loss did not improve from 0.00007\n",
            "Epoch 1284/2000\n",
            " - 2s - loss: 7.6133e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01284: loss did not improve from 0.00007\n",
            "Epoch 1285/2000\n",
            " - 2s - loss: 1.1768e-04 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01285: loss did not improve from 0.00007\n",
            "Epoch 1286/2000\n",
            " - 2s - loss: 6.2622e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01286: loss improved from 0.00007 to 0.00006, saving model to ./weights/lstmfcn_128_cells_weights/CBF_weights.h5\n",
            "Epoch 1287/2000\n",
            " - 2s - loss: 1.1172e-04 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01287: loss did not improve from 0.00006\n",
            "Epoch 1288/2000\n",
            " - 2s - loss: 9.6592e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01288: loss did not improve from 0.00006\n",
            "Epoch 1289/2000\n",
            " - 2s - loss: 1.0104e-04 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01289: loss did not improve from 0.00006\n",
            "Epoch 1290/2000\n",
            " - 2s - loss: 1.1808e-04 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01290: loss did not improve from 0.00006\n",
            "Epoch 1291/2000\n",
            " - 2s - loss: 1.0270e-04 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01291: loss did not improve from 0.00006\n",
            "Epoch 1292/2000\n",
            " - 2s - loss: 8.9328e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01292: loss did not improve from 0.00006\n",
            "Epoch 1293/2000\n",
            " - 2s - loss: 7.7337e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01293: loss did not improve from 0.00006\n",
            "Epoch 1294/2000\n",
            " - 2s - loss: 1.3945e-04 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01294: loss did not improve from 0.00006\n",
            "Epoch 1295/2000\n",
            " - 2s - loss: 1.0968e-04 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01295: loss did not improve from 0.00006\n",
            "Epoch 1296/2000\n",
            " - 2s - loss: 9.7767e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01296: loss did not improve from 0.00006\n",
            "Epoch 1297/2000\n",
            " - 2s - loss: 6.8434e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01297: loss did not improve from 0.00006\n",
            "Epoch 1298/2000\n",
            " - 2s - loss: 1.6311e-04 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01298: loss did not improve from 0.00006\n",
            "Epoch 1299/2000\n",
            " - 2s - loss: 8.4945e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01299: loss did not improve from 0.00006\n",
            "Epoch 1300/2000\n",
            " - 2s - loss: 1.0021e-04 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01300: loss did not improve from 0.00006\n",
            "Epoch 1301/2000\n",
            " - 2s - loss: 1.2384e-04 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01301: loss did not improve from 0.00006\n",
            "Epoch 1302/2000\n",
            " - 2s - loss: 1.1145e-04 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01302: loss did not improve from 0.00006\n",
            "Epoch 1303/2000\n",
            " - 2s - loss: 1.0458e-04 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01303: loss did not improve from 0.00006\n",
            "Epoch 1304/2000\n",
            " - 2s - loss: 7.6318e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01304: loss did not improve from 0.00006\n",
            "Epoch 1305/2000\n",
            " - 2s - loss: 1.0423e-04 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01305: loss did not improve from 0.00006\n",
            "Epoch 1306/2000\n",
            " - 2s - loss: 1.1293e-04 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01306: loss did not improve from 0.00006\n",
            "Epoch 1307/2000\n",
            " - 2s - loss: 1.2201e-04 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01307: loss did not improve from 0.00006\n",
            "Epoch 1308/2000\n",
            " - 2s - loss: 1.2479e-04 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01308: loss did not improve from 0.00006\n",
            "Epoch 1309/2000\n",
            " - 2s - loss: 1.4409e-04 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01309: loss did not improve from 0.00006\n",
            "Epoch 1310/2000\n",
            " - 2s - loss: 1.4474e-04 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01310: loss did not improve from 0.00006\n",
            "Epoch 1311/2000\n",
            " - 2s - loss: 9.8548e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01311: loss did not improve from 0.00006\n",
            "Epoch 1312/2000\n",
            " - 2s - loss: 9.6425e-05 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01312: loss did not improve from 0.00006\n",
            "Epoch 1313/2000\n",
            " - 2s - loss: 1.1136e-04 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01313: loss did not improve from 0.00006\n",
            "Epoch 1314/2000\n",
            " - 2s - loss: 8.9901e-05 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01314: loss did not improve from 0.00006\n",
            "Epoch 1315/2000\n",
            " - 2s - loss: 1.1015e-04 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01315: loss did not improve from 0.00006\n",
            "Epoch 1316/2000\n",
            " - 2s - loss: 1.0717e-04 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01316: loss did not improve from 0.00006\n",
            "Epoch 1317/2000\n",
            " - 2s - loss: 8.3190e-05 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01317: loss did not improve from 0.00006\n",
            "Epoch 1318/2000\n",
            " - 2s - loss: 8.9099e-05 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01318: loss did not improve from 0.00006\n",
            "Epoch 1319/2000\n",
            " - 2s - loss: 1.0400e-04 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01319: loss did not improve from 0.00006\n",
            "Epoch 1320/2000\n",
            " - 2s - loss: 1.0888e-04 - accuracy: 1.0000 - val_loss: 0.0236 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01320: loss did not improve from 0.00006\n",
            "Epoch 1321/2000\n",
            " - 2s - loss: 9.4825e-05 - accuracy: 1.0000 - val_loss: 0.0236 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01321: loss did not improve from 0.00006\n",
            "Epoch 1322/2000\n",
            " - 2s - loss: 1.0246e-04 - accuracy: 1.0000 - val_loss: 0.0237 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01322: loss did not improve from 0.00006\n",
            "Epoch 1323/2000\n",
            " - 2s - loss: 1.3437e-04 - accuracy: 1.0000 - val_loss: 0.0237 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01323: loss did not improve from 0.00006\n",
            "Epoch 1324/2000\n",
            " - 2s - loss: 1.1816e-04 - accuracy: 1.0000 - val_loss: 0.0237 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01324: loss did not improve from 0.00006\n",
            "Epoch 1325/2000\n",
            " - 2s - loss: 1.4473e-04 - accuracy: 1.0000 - val_loss: 0.0237 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01325: loss did not improve from 0.00006\n",
            "Epoch 1326/2000\n",
            " - 2s - loss: 1.0926e-04 - accuracy: 1.0000 - val_loss: 0.0238 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01326: loss did not improve from 0.00006\n",
            "Epoch 1327/2000\n",
            " - 2s - loss: 7.0972e-05 - accuracy: 1.0000 - val_loss: 0.0238 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01327: loss did not improve from 0.00006\n",
            "Epoch 1328/2000\n",
            " - 2s - loss: 8.8479e-05 - accuracy: 1.0000 - val_loss: 0.0238 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01328: loss did not improve from 0.00006\n",
            "Epoch 1329/2000\n",
            " - 2s - loss: 1.1166e-04 - accuracy: 1.0000 - val_loss: 0.0238 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01329: loss did not improve from 0.00006\n",
            "Epoch 1330/2000\n",
            " - 2s - loss: 1.1005e-04 - accuracy: 1.0000 - val_loss: 0.0238 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01330: loss did not improve from 0.00006\n",
            "Epoch 1331/2000\n",
            " - 2s - loss: 8.4411e-05 - accuracy: 1.0000 - val_loss: 0.0238 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01331: loss did not improve from 0.00006\n",
            "Epoch 1332/2000\n",
            " - 2s - loss: 8.8727e-05 - accuracy: 1.0000 - val_loss: 0.0238 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01332: loss did not improve from 0.00006\n",
            "Epoch 1333/2000\n",
            " - 2s - loss: 1.3453e-04 - accuracy: 1.0000 - val_loss: 0.0238 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01333: loss did not improve from 0.00006\n",
            "Epoch 1334/2000\n",
            " - 2s - loss: 8.8951e-05 - accuracy: 1.0000 - val_loss: 0.0238 - val_accuracy: 0.9933\n",
            "\n",
            "Epoch 01334: loss did not improve from 0.00006\n",
            "Epoch 1335/2000\n",
            " - 2s - loss: 1.0701e-04 - accuracy: 1.0000 - val_loss: 0.0237 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01335: loss did not improve from 0.00006\n",
            "\n",
            "Epoch 01335: ReduceLROnPlateau reducing learning rate to 0.00012500000681810424.\n",
            "Epoch 1336/2000\n",
            " - 2s - loss: 9.9704e-05 - accuracy: 1.0000 - val_loss: 0.0237 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01336: loss did not improve from 0.00006\n",
            "Epoch 1337/2000\n",
            " - 2s - loss: 7.8382e-05 - accuracy: 1.0000 - val_loss: 0.0237 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01337: loss did not improve from 0.00006\n",
            "Epoch 1338/2000\n",
            " - 2s - loss: 1.1203e-04 - accuracy: 1.0000 - val_loss: 0.0237 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01338: loss did not improve from 0.00006\n",
            "Epoch 1339/2000\n",
            " - 2s - loss: 1.5242e-04 - accuracy: 1.0000 - val_loss: 0.0236 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01339: loss did not improve from 0.00006\n",
            "Epoch 1340/2000\n",
            " - 2s - loss: 9.6059e-05 - accuracy: 1.0000 - val_loss: 0.0236 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01340: loss did not improve from 0.00006\n",
            "Epoch 1341/2000\n",
            " - 2s - loss: 1.3467e-04 - accuracy: 1.0000 - val_loss: 0.0236 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01341: loss did not improve from 0.00006\n",
            "Epoch 1342/2000\n",
            " - 2s - loss: 9.3297e-05 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01342: loss did not improve from 0.00006\n",
            "Epoch 1343/2000\n",
            " - 2s - loss: 8.7826e-05 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01343: loss did not improve from 0.00006\n",
            "Epoch 1344/2000\n",
            " - 2s - loss: 9.5415e-05 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01344: loss did not improve from 0.00006\n",
            "Epoch 1345/2000\n",
            " - 2s - loss: 7.8014e-05 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01345: loss did not improve from 0.00006\n",
            "Epoch 1346/2000\n",
            " - 2s - loss: 1.0010e-04 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01346: loss did not improve from 0.00006\n",
            "Epoch 1347/2000\n",
            " - 2s - loss: 6.7189e-05 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01347: loss did not improve from 0.00006\n",
            "Epoch 1348/2000\n",
            " - 2s - loss: 6.4274e-05 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01348: loss did not improve from 0.00006\n",
            "Epoch 1349/2000\n",
            " - 2s - loss: 1.2953e-04 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01349: loss did not improve from 0.00006\n",
            "Epoch 1350/2000\n",
            " - 2s - loss: 6.8544e-05 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01350: loss did not improve from 0.00006\n",
            "Epoch 1351/2000\n",
            " - 2s - loss: 1.1527e-04 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01351: loss did not improve from 0.00006\n",
            "Epoch 1352/2000\n",
            " - 2s - loss: 1.0602e-04 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01352: loss did not improve from 0.00006\n",
            "Epoch 1353/2000\n",
            " - 2s - loss: 9.1745e-05 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01353: loss did not improve from 0.00006\n",
            "Epoch 1354/2000\n",
            " - 2s - loss: 1.2467e-04 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01354: loss did not improve from 0.00006\n",
            "Epoch 1355/2000\n",
            " - 2s - loss: 1.1266e-04 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01355: loss did not improve from 0.00006\n",
            "Epoch 1356/2000\n",
            " - 2s - loss: 9.7497e-05 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01356: loss did not improve from 0.00006\n",
            "Epoch 1357/2000\n",
            " - 2s - loss: 7.2312e-05 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01357: loss did not improve from 0.00006\n",
            "Epoch 1358/2000\n",
            " - 2s - loss: 1.2453e-04 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01358: loss did not improve from 0.00006\n",
            "Epoch 1359/2000\n",
            " - 2s - loss: 1.0192e-04 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01359: loss did not improve from 0.00006\n",
            "Epoch 1360/2000\n",
            " - 2s - loss: 1.0368e-04 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01360: loss did not improve from 0.00006\n",
            "Epoch 1361/2000\n",
            " - 2s - loss: 9.2066e-05 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01361: loss did not improve from 0.00006\n",
            "Epoch 1362/2000\n",
            " - 2s - loss: 6.6468e-05 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01362: loss did not improve from 0.00006\n",
            "Epoch 1363/2000\n",
            " - 2s - loss: 9.8504e-05 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01363: loss did not improve from 0.00006\n",
            "Epoch 1364/2000\n",
            " - 2s - loss: 1.0561e-04 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01364: loss did not improve from 0.00006\n",
            "Epoch 1365/2000\n",
            " - 2s - loss: 1.2459e-04 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01365: loss did not improve from 0.00006\n",
            "Epoch 1366/2000\n",
            " - 2s - loss: 1.0702e-04 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01366: loss did not improve from 0.00006\n",
            "Epoch 1367/2000\n",
            " - 2s - loss: 1.2731e-04 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01367: loss did not improve from 0.00006\n",
            "Epoch 1368/2000\n",
            " - 2s - loss: 1.1490e-04 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01368: loss did not improve from 0.00006\n",
            "Epoch 1369/2000\n",
            " - 2s - loss: 1.2117e-04 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01369: loss did not improve from 0.00006\n",
            "Epoch 1370/2000\n",
            " - 2s - loss: 1.0128e-04 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01370: loss did not improve from 0.00006\n",
            "Epoch 1371/2000\n",
            " - 2s - loss: 7.9692e-05 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01371: loss did not improve from 0.00006\n",
            "Epoch 1372/2000\n",
            " - 2s - loss: 8.7901e-05 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01372: loss did not improve from 0.00006\n",
            "Epoch 1373/2000\n",
            " - 2s - loss: 1.0645e-04 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01373: loss did not improve from 0.00006\n",
            "Epoch 1374/2000\n",
            " - 2s - loss: 7.0225e-05 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01374: loss did not improve from 0.00006\n",
            "Epoch 1375/2000\n",
            " - 2s - loss: 8.5511e-05 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01375: loss did not improve from 0.00006\n",
            "Epoch 1376/2000\n",
            " - 2s - loss: 8.7052e-05 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01376: loss did not improve from 0.00006\n",
            "Epoch 1377/2000\n",
            " - 2s - loss: 1.2636e-04 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01377: loss did not improve from 0.00006\n",
            "Epoch 1378/2000\n",
            " - 2s - loss: 7.9996e-05 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01378: loss did not improve from 0.00006\n",
            "Epoch 1379/2000\n",
            " - 2s - loss: 9.7459e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01379: loss did not improve from 0.00006\n",
            "Epoch 1380/2000\n",
            " - 2s - loss: 1.0936e-04 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01380: loss did not improve from 0.00006\n",
            "Epoch 1381/2000\n",
            " - 2s - loss: 1.4466e-04 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01381: loss did not improve from 0.00006\n",
            "Epoch 1382/2000\n",
            " - 2s - loss: 1.0516e-04 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01382: loss did not improve from 0.00006\n",
            "Epoch 1383/2000\n",
            " - 2s - loss: 1.3876e-04 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01383: loss did not improve from 0.00006\n",
            "Epoch 1384/2000\n",
            " - 2s - loss: 1.0051e-04 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01384: loss did not improve from 0.00006\n",
            "Epoch 1385/2000\n",
            " - 2s - loss: 1.3119e-04 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01385: loss did not improve from 0.00006\n",
            "Epoch 1386/2000\n",
            " - 2s - loss: 1.0455e-04 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01386: loss did not improve from 0.00006\n",
            "Epoch 1387/2000\n",
            " - 2s - loss: 8.2935e-05 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01387: loss did not improve from 0.00006\n",
            "Epoch 1388/2000\n",
            " - 2s - loss: 9.7205e-05 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01388: loss did not improve from 0.00006\n",
            "Epoch 1389/2000\n",
            " - 2s - loss: 8.3189e-05 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01389: loss did not improve from 0.00006\n",
            "Epoch 1390/2000\n",
            " - 2s - loss: 7.9208e-05 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01390: loss did not improve from 0.00006\n",
            "Epoch 1391/2000\n",
            " - 2s - loss: 6.2176e-05 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01391: loss improved from 0.00006 to 0.00006, saving model to ./weights/lstmfcn_128_cells_weights/CBF_weights.h5\n",
            "Epoch 1392/2000\n",
            " - 2s - loss: 9.3393e-05 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01392: loss did not improve from 0.00006\n",
            "Epoch 1393/2000\n",
            " - 2s - loss: 9.4881e-05 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01393: loss did not improve from 0.00006\n",
            "Epoch 1394/2000\n",
            " - 2s - loss: 8.0230e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01394: loss did not improve from 0.00006\n",
            "Epoch 1395/2000\n",
            " - 2s - loss: 1.0631e-04 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01395: loss did not improve from 0.00006\n",
            "Epoch 1396/2000\n",
            " - 2s - loss: 8.8531e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01396: loss did not improve from 0.00006\n",
            "Epoch 1397/2000\n",
            " - 2s - loss: 9.8802e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01397: loss did not improve from 0.00006\n",
            "Epoch 1398/2000\n",
            " - 2s - loss: 1.0876e-04 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01398: loss did not improve from 0.00006\n",
            "Epoch 1399/2000\n",
            " - 2s - loss: 8.0312e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01399: loss did not improve from 0.00006\n",
            "Epoch 1400/2000\n",
            " - 2s - loss: 9.2761e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01400: loss did not improve from 0.00006\n",
            "Epoch 1401/2000\n",
            " - 2s - loss: 1.0209e-04 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01401: loss did not improve from 0.00006\n",
            "Epoch 1402/2000\n",
            " - 2s - loss: 1.4088e-04 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01402: loss did not improve from 0.00006\n",
            "Epoch 1403/2000\n",
            " - 2s - loss: 9.5894e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01403: loss did not improve from 0.00006\n",
            "Epoch 1404/2000\n",
            " - 2s - loss: 8.9623e-05 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01404: loss did not improve from 0.00006\n",
            "Epoch 1405/2000\n",
            " - 2s - loss: 1.1250e-04 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01405: loss did not improve from 0.00006\n",
            "Epoch 1406/2000\n",
            " - 2s - loss: 8.4169e-05 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01406: loss did not improve from 0.00006\n",
            "Epoch 1407/2000\n",
            " - 2s - loss: 9.7091e-05 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01407: loss did not improve from 0.00006\n",
            "Epoch 1408/2000\n",
            " - 2s - loss: 8.9904e-05 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01408: loss did not improve from 0.00006\n",
            "Epoch 1409/2000\n",
            " - 2s - loss: 7.6895e-05 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01409: loss did not improve from 0.00006\n",
            "Epoch 1410/2000\n",
            " - 2s - loss: 1.6142e-04 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01410: loss did not improve from 0.00006\n",
            "Epoch 1411/2000\n",
            " - 2s - loss: 9.2501e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01411: loss did not improve from 0.00006\n",
            "Epoch 1412/2000\n",
            " - 2s - loss: 1.0926e-04 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01412: loss did not improve from 0.00006\n",
            "Epoch 1413/2000\n",
            " - 2s - loss: 8.9686e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01413: loss did not improve from 0.00006\n",
            "Epoch 1414/2000\n",
            " - 2s - loss: 6.0014e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01414: loss improved from 0.00006 to 0.00006, saving model to ./weights/lstmfcn_128_cells_weights/CBF_weights.h5\n",
            "Epoch 1415/2000\n",
            " - 2s - loss: 7.4534e-05 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01415: loss did not improve from 0.00006\n",
            "Epoch 1416/2000\n",
            " - 2s - loss: 6.9801e-05 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01416: loss did not improve from 0.00006\n",
            "Epoch 1417/2000\n",
            " - 2s - loss: 1.2037e-04 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01417: loss did not improve from 0.00006\n",
            "Epoch 1418/2000\n",
            " - 2s - loss: 9.8028e-05 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01418: loss did not improve from 0.00006\n",
            "Epoch 1419/2000\n",
            " - 2s - loss: 7.5744e-05 - accuracy: 1.0000 - val_loss: 0.0227 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01419: loss did not improve from 0.00006\n",
            "Epoch 1420/2000\n",
            " - 2s - loss: 1.0791e-04 - accuracy: 1.0000 - val_loss: 0.0227 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01420: loss did not improve from 0.00006\n",
            "Epoch 1421/2000\n",
            " - 2s - loss: 1.2339e-04 - accuracy: 1.0000 - val_loss: 0.0227 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01421: loss did not improve from 0.00006\n",
            "Epoch 1422/2000\n",
            " - 2s - loss: 7.4183e-05 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01422: loss did not improve from 0.00006\n",
            "Epoch 1423/2000\n",
            " - 2s - loss: 1.0999e-04 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01423: loss did not improve from 0.00006\n",
            "Epoch 1424/2000\n",
            " - 2s - loss: 8.2895e-05 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01424: loss did not improve from 0.00006\n",
            "Epoch 1425/2000\n",
            " - 2s - loss: 7.2004e-05 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01425: loss did not improve from 0.00006\n",
            "Epoch 1426/2000\n",
            " - 2s - loss: 1.0323e-04 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01426: loss did not improve from 0.00006\n",
            "Epoch 1427/2000\n",
            " - 2s - loss: 1.1601e-04 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01427: loss did not improve from 0.00006\n",
            "Epoch 1428/2000\n",
            " - 2s - loss: 7.1581e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01428: loss did not improve from 0.00006\n",
            "Epoch 1429/2000\n",
            " - 2s - loss: 8.1833e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01429: loss did not improve from 0.00006\n",
            "Epoch 1430/2000\n",
            " - 2s - loss: 1.0163e-04 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01430: loss did not improve from 0.00006\n",
            "Epoch 1431/2000\n",
            " - 2s - loss: 1.3355e-04 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01431: loss did not improve from 0.00006\n",
            "Epoch 1432/2000\n",
            " - 2s - loss: 7.1104e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01432: loss did not improve from 0.00006\n",
            "Epoch 1433/2000\n",
            " - 2s - loss: 1.0500e-04 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01433: loss did not improve from 0.00006\n",
            "Epoch 1434/2000\n",
            " - 2s - loss: 1.0561e-04 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01434: loss did not improve from 0.00006\n",
            "Epoch 1435/2000\n",
            " - 2s - loss: 1.0747e-04 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01435: loss did not improve from 0.00006\n",
            "\n",
            "Epoch 01435: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
            "Epoch 1436/2000\n",
            " - 2s - loss: 1.0309e-04 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01436: loss did not improve from 0.00006\n",
            "Epoch 1437/2000\n",
            " - 2s - loss: 7.5084e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01437: loss did not improve from 0.00006\n",
            "Epoch 1438/2000\n",
            " - 2s - loss: 9.4108e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01438: loss did not improve from 0.00006\n",
            "Epoch 1439/2000\n",
            " - 2s - loss: 8.5532e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01439: loss did not improve from 0.00006\n",
            "Epoch 1440/2000\n",
            " - 2s - loss: 7.9670e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01440: loss did not improve from 0.00006\n",
            "Epoch 1441/2000\n",
            " - 2s - loss: 8.7051e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01441: loss did not improve from 0.00006\n",
            "Epoch 1442/2000\n",
            " - 2s - loss: 6.6907e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01442: loss did not improve from 0.00006\n",
            "Epoch 1443/2000\n",
            " - 2s - loss: 6.9725e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01443: loss did not improve from 0.00006\n",
            "Epoch 1444/2000\n",
            " - 2s - loss: 1.3301e-04 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01444: loss did not improve from 0.00006\n",
            "Epoch 1445/2000\n",
            " - 2s - loss: 9.1385e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01445: loss did not improve from 0.00006\n",
            "Epoch 1446/2000\n",
            " - 2s - loss: 1.3018e-04 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01446: loss did not improve from 0.00006\n",
            "Epoch 1447/2000\n",
            " - 2s - loss: 9.9514e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01447: loss did not improve from 0.00006\n",
            "Epoch 1448/2000\n",
            " - 2s - loss: 8.3620e-05 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01448: loss did not improve from 0.00006\n",
            "Epoch 1449/2000\n",
            " - 2s - loss: 7.2797e-05 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01449: loss did not improve from 0.00006\n",
            "Epoch 1450/2000\n",
            " - 2s - loss: 1.0138e-04 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01450: loss did not improve from 0.00006\n",
            "Epoch 1451/2000\n",
            " - 2s - loss: 8.6177e-05 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01451: loss did not improve from 0.00006\n",
            "Epoch 1452/2000\n",
            " - 2s - loss: 1.3320e-04 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01452: loss did not improve from 0.00006\n",
            "Epoch 1453/2000\n",
            " - 2s - loss: 9.3198e-05 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01453: loss did not improve from 0.00006\n",
            "Epoch 1454/2000\n",
            " - 2s - loss: 7.8121e-05 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01454: loss did not improve from 0.00006\n",
            "Epoch 1455/2000\n",
            " - 2s - loss: 9.3639e-05 - accuracy: 1.0000 - val_loss: 0.0227 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01455: loss did not improve from 0.00006\n",
            "Epoch 1456/2000\n",
            " - 2s - loss: 1.2578e-04 - accuracy: 1.0000 - val_loss: 0.0227 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01456: loss did not improve from 0.00006\n",
            "Epoch 1457/2000\n",
            " - 2s - loss: 5.9776e-05 - accuracy: 1.0000 - val_loss: 0.0227 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01457: loss improved from 0.00006 to 0.00006, saving model to ./weights/lstmfcn_128_cells_weights/CBF_weights.h5\n",
            "Epoch 1458/2000\n",
            " - 2s - loss: 1.1255e-04 - accuracy: 1.0000 - val_loss: 0.0227 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01458: loss did not improve from 0.00006\n",
            "Epoch 1459/2000\n",
            " - 2s - loss: 9.4167e-05 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01459: loss did not improve from 0.00006\n",
            "Epoch 1460/2000\n",
            " - 2s - loss: 9.8415e-05 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01460: loss did not improve from 0.00006\n",
            "Epoch 1461/2000\n",
            " - 2s - loss: 1.0368e-04 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01461: loss did not improve from 0.00006\n",
            "Epoch 1462/2000\n",
            " - 2s - loss: 1.0494e-04 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01462: loss did not improve from 0.00006\n",
            "Epoch 1463/2000\n",
            " - 2s - loss: 8.4827e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01463: loss did not improve from 0.00006\n",
            "Epoch 1464/2000\n",
            " - 2s - loss: 1.5123e-04 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01464: loss did not improve from 0.00006\n",
            "Epoch 1465/2000\n",
            " - 2s - loss: 9.4677e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01465: loss did not improve from 0.00006\n",
            "Epoch 1466/2000\n",
            " - 2s - loss: 1.1003e-04 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01466: loss did not improve from 0.00006\n",
            "Epoch 1467/2000\n",
            " - 2s - loss: 1.1050e-04 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01467: loss did not improve from 0.00006\n",
            "Epoch 1468/2000\n",
            " - 2s - loss: 9.3363e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01468: loss did not improve from 0.00006\n",
            "Epoch 1469/2000\n",
            " - 2s - loss: 1.1865e-04 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01469: loss did not improve from 0.00006\n",
            "Epoch 1470/2000\n",
            " - 2s - loss: 9.9412e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01470: loss did not improve from 0.00006\n",
            "Epoch 1471/2000\n",
            " - 2s - loss: 1.0035e-04 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01471: loss did not improve from 0.00006\n",
            "Epoch 1472/2000\n",
            " - 2s - loss: 1.5536e-04 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01472: loss did not improve from 0.00006\n",
            "Epoch 1473/2000\n",
            " - 2s - loss: 1.3399e-04 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01473: loss did not improve from 0.00006\n",
            "Epoch 1474/2000\n",
            " - 2s - loss: 1.1596e-04 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01474: loss did not improve from 0.00006\n",
            "Epoch 1475/2000\n",
            " - 2s - loss: 9.2259e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01475: loss did not improve from 0.00006\n",
            "Epoch 1476/2000\n",
            " - 2s - loss: 9.8860e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01476: loss did not improve from 0.00006\n",
            "Epoch 1477/2000\n",
            " - 2s - loss: 1.0879e-04 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01477: loss did not improve from 0.00006\n",
            "Epoch 1478/2000\n",
            " - 2s - loss: 1.5248e-04 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01478: loss did not improve from 0.00006\n",
            "Epoch 1479/2000\n",
            " - 2s - loss: 1.0146e-04 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01479: loss did not improve from 0.00006\n",
            "Epoch 1480/2000\n",
            " - 2s - loss: 1.3285e-04 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01480: loss did not improve from 0.00006\n",
            "Epoch 1481/2000\n",
            " - 2s - loss: 1.3946e-04 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01481: loss did not improve from 0.00006\n",
            "Epoch 1482/2000\n",
            " - 2s - loss: 7.9053e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01482: loss did not improve from 0.00006\n",
            "Epoch 1483/2000\n",
            " - 2s - loss: 1.0139e-04 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01483: loss did not improve from 0.00006\n",
            "Epoch 1484/2000\n",
            " - 2s - loss: 9.8644e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01484: loss did not improve from 0.00006\n",
            "Epoch 1485/2000\n",
            " - 2s - loss: 1.1896e-04 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01485: loss did not improve from 0.00006\n",
            "Epoch 1486/2000\n",
            " - 2s - loss: 9.7967e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01486: loss did not improve from 0.00006\n",
            "Epoch 1487/2000\n",
            " - 2s - loss: 1.0011e-04 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01487: loss did not improve from 0.00006\n",
            "Epoch 1488/2000\n",
            " - 2s - loss: 6.5287e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01488: loss did not improve from 0.00006\n",
            "Epoch 1489/2000\n",
            " - 2s - loss: 9.7621e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01489: loss did not improve from 0.00006\n",
            "Epoch 1490/2000\n",
            " - 2s - loss: 8.9539e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01490: loss did not improve from 0.00006\n",
            "Epoch 1491/2000\n",
            " - 2s - loss: 6.5041e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01491: loss did not improve from 0.00006\n",
            "Epoch 1492/2000\n",
            " - 2s - loss: 1.0167e-04 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01492: loss did not improve from 0.00006\n",
            "Epoch 1493/2000\n",
            " - 2s - loss: 8.0205e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01493: loss did not improve from 0.00006\n",
            "Epoch 1494/2000\n",
            " - 2s - loss: 8.6402e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01494: loss did not improve from 0.00006\n",
            "Epoch 1495/2000\n",
            " - 2s - loss: 9.0616e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01495: loss did not improve from 0.00006\n",
            "Epoch 1496/2000\n",
            " - 2s - loss: 1.0150e-04 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01496: loss did not improve from 0.00006\n",
            "Epoch 1497/2000\n",
            " - 2s - loss: 9.7035e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01497: loss did not improve from 0.00006\n",
            "Epoch 1498/2000\n",
            " - 2s - loss: 1.0185e-04 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01498: loss did not improve from 0.00006\n",
            "Epoch 1499/2000\n",
            " - 2s - loss: 9.5799e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01499: loss did not improve from 0.00006\n",
            "Epoch 1500/2000\n",
            " - 2s - loss: 5.8847e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01500: loss improved from 0.00006 to 0.00006, saving model to ./weights/lstmfcn_128_cells_weights/CBF_weights.h5\n",
            "Epoch 1501/2000\n",
            " - 2s - loss: 9.5777e-05 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01501: loss did not improve from 0.00006\n",
            "Epoch 1502/2000\n",
            " - 2s - loss: 9.7411e-05 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01502: loss did not improve from 0.00006\n",
            "Epoch 1503/2000\n",
            " - 2s - loss: 7.6622e-05 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01503: loss did not improve from 0.00006\n",
            "Epoch 1504/2000\n",
            " - 2s - loss: 1.1459e-04 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01504: loss did not improve from 0.00006\n",
            "Epoch 1505/2000\n",
            " - 2s - loss: 7.6154e-05 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01505: loss did not improve from 0.00006\n",
            "Epoch 1506/2000\n",
            " - 2s - loss: 1.0915e-04 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01506: loss did not improve from 0.00006\n",
            "Epoch 1507/2000\n",
            " - 2s - loss: 9.0868e-05 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01507: loss did not improve from 0.00006\n",
            "Epoch 1508/2000\n",
            " - 2s - loss: 8.5164e-05 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01508: loss did not improve from 0.00006\n",
            "Epoch 1509/2000\n",
            " - 2s - loss: 9.5025e-05 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01509: loss did not improve from 0.00006\n",
            "Epoch 1510/2000\n",
            " - 2s - loss: 7.8124e-05 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01510: loss did not improve from 0.00006\n",
            "Epoch 1511/2000\n",
            " - 2s - loss: 7.0048e-05 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01511: loss did not improve from 0.00006\n",
            "Epoch 1512/2000\n",
            " - 2s - loss: 8.3297e-05 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01512: loss did not improve from 0.00006\n",
            "Epoch 1513/2000\n",
            " - 2s - loss: 1.0471e-04 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01513: loss did not improve from 0.00006\n",
            "Epoch 1514/2000\n",
            " - 2s - loss: 9.8287e-05 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01514: loss did not improve from 0.00006\n",
            "Epoch 1515/2000\n",
            " - 2s - loss: 1.4428e-04 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01515: loss did not improve from 0.00006\n",
            "Epoch 1516/2000\n",
            " - 2s - loss: 8.5185e-05 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01516: loss did not improve from 0.00006\n",
            "Epoch 1517/2000\n",
            " - 2s - loss: 9.6289e-05 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01517: loss did not improve from 0.00006\n",
            "Epoch 1518/2000\n",
            " - 2s - loss: 8.8511e-05 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01518: loss did not improve from 0.00006\n",
            "Epoch 1519/2000\n",
            " - 2s - loss: 1.4073e-04 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01519: loss did not improve from 0.00006\n",
            "Epoch 1520/2000\n",
            " - 2s - loss: 1.0047e-04 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01520: loss did not improve from 0.00006\n",
            "Epoch 1521/2000\n",
            " - 2s - loss: 9.9484e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01521: loss did not improve from 0.00006\n",
            "Epoch 1522/2000\n",
            " - 2s - loss: 6.8478e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01522: loss did not improve from 0.00006\n",
            "Epoch 1523/2000\n",
            " - 2s - loss: 1.4177e-04 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01523: loss did not improve from 0.00006\n",
            "Epoch 1524/2000\n",
            " - 2s - loss: 8.0043e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01524: loss did not improve from 0.00006\n",
            "Epoch 1525/2000\n",
            " - 2s - loss: 1.0683e-04 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01525: loss did not improve from 0.00006\n",
            "Epoch 1526/2000\n",
            " - 2s - loss: 7.8132e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01526: loss did not improve from 0.00006\n",
            "Epoch 1527/2000\n",
            " - 2s - loss: 8.6793e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01527: loss did not improve from 0.00006\n",
            "Epoch 1528/2000\n",
            " - 2s - loss: 1.2750e-04 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01528: loss did not improve from 0.00006\n",
            "Epoch 1529/2000\n",
            " - 2s - loss: 1.0558e-04 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01529: loss did not improve from 0.00006\n",
            "Epoch 1530/2000\n",
            " - 2s - loss: 1.0719e-04 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01530: loss did not improve from 0.00006\n",
            "Epoch 1531/2000\n",
            " - 2s - loss: 1.1599e-04 - accuracy: 1.0000 - val_loss: 0.0227 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01531: loss did not improve from 0.00006\n",
            "Epoch 1532/2000\n",
            " - 2s - loss: 8.8162e-05 - accuracy: 1.0000 - val_loss: 0.0227 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01532: loss did not improve from 0.00006\n",
            "Epoch 1533/2000\n",
            " - 2s - loss: 1.0203e-04 - accuracy: 1.0000 - val_loss: 0.0227 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01533: loss did not improve from 0.00006\n",
            "Epoch 1534/2000\n",
            " - 2s - loss: 1.0095e-04 - accuracy: 1.0000 - val_loss: 0.0226 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01534: loss did not improve from 0.00006\n",
            "Epoch 1535/2000\n",
            " - 2s - loss: 8.7592e-05 - accuracy: 1.0000 - val_loss: 0.0226 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01535: loss did not improve from 0.00006\n",
            "Epoch 1536/2000\n",
            " - 2s - loss: 1.0221e-04 - accuracy: 1.0000 - val_loss: 0.0226 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01536: loss did not improve from 0.00006\n",
            "Epoch 1537/2000\n",
            " - 2s - loss: 9.5454e-05 - accuracy: 1.0000 - val_loss: 0.0226 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01537: loss did not improve from 0.00006\n",
            "Epoch 1538/2000\n",
            " - 2s - loss: 9.3158e-05 - accuracy: 1.0000 - val_loss: 0.0226 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01538: loss did not improve from 0.00006\n",
            "Epoch 1539/2000\n",
            " - 2s - loss: 9.0441e-05 - accuracy: 1.0000 - val_loss: 0.0226 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01539: loss did not improve from 0.00006\n",
            "Epoch 1540/2000\n",
            " - 2s - loss: 1.0258e-04 - accuracy: 1.0000 - val_loss: 0.0226 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01540: loss did not improve from 0.00006\n",
            "Epoch 1541/2000\n",
            " - 2s - loss: 8.1566e-05 - accuracy: 1.0000 - val_loss: 0.0227 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01541: loss did not improve from 0.00006\n",
            "Epoch 1542/2000\n",
            " - 2s - loss: 9.6555e-05 - accuracy: 1.0000 - val_loss: 0.0227 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01542: loss did not improve from 0.00006\n",
            "Epoch 1543/2000\n",
            " - 2s - loss: 9.7821e-05 - accuracy: 1.0000 - val_loss: 0.0227 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01543: loss did not improve from 0.00006\n",
            "Epoch 1544/2000\n",
            " - 2s - loss: 7.8418e-05 - accuracy: 1.0000 - val_loss: 0.0227 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01544: loss did not improve from 0.00006\n",
            "Epoch 1545/2000\n",
            " - 2s - loss: 8.7890e-05 - accuracy: 1.0000 - val_loss: 0.0227 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01545: loss did not improve from 0.00006\n",
            "Epoch 1546/2000\n",
            " - 2s - loss: 9.6788e-05 - accuracy: 1.0000 - val_loss: 0.0227 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01546: loss did not improve from 0.00006\n",
            "Epoch 1547/2000\n",
            " - 2s - loss: 6.7056e-05 - accuracy: 1.0000 - val_loss: 0.0227 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01547: loss did not improve from 0.00006\n",
            "Epoch 1548/2000\n",
            " - 2s - loss: 7.0949e-05 - accuracy: 1.0000 - val_loss: 0.0227 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01548: loss did not improve from 0.00006\n",
            "Epoch 1549/2000\n",
            " - 2s - loss: 7.1862e-05 - accuracy: 1.0000 - val_loss: 0.0227 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01549: loss did not improve from 0.00006\n",
            "Epoch 1550/2000\n",
            " - 2s - loss: 1.1755e-04 - accuracy: 1.0000 - val_loss: 0.0227 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01550: loss did not improve from 0.00006\n",
            "Epoch 1551/2000\n",
            " - 2s - loss: 9.7985e-05 - accuracy: 1.0000 - val_loss: 0.0227 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01551: loss did not improve from 0.00006\n",
            "Epoch 1552/2000\n",
            " - 2s - loss: 7.5540e-05 - accuracy: 1.0000 - val_loss: 0.0227 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01552: loss did not improve from 0.00006\n",
            "Epoch 1553/2000\n",
            " - 2s - loss: 9.6317e-05 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01553: loss did not improve from 0.00006\n",
            "Epoch 1554/2000\n",
            " - 2s - loss: 9.8844e-05 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01554: loss did not improve from 0.00006\n",
            "Epoch 1555/2000\n",
            " - 2s - loss: 8.8470e-05 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01555: loss did not improve from 0.00006\n",
            "Epoch 1556/2000\n",
            " - 2s - loss: 8.2680e-05 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01556: loss did not improve from 0.00006\n",
            "Epoch 1557/2000\n",
            " - 2s - loss: 7.8314e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01557: loss did not improve from 0.00006\n",
            "Epoch 1558/2000\n",
            " - 2s - loss: 9.1458e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01558: loss did not improve from 0.00006\n",
            "Epoch 1559/2000\n",
            " - 2s - loss: 8.4680e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01559: loss did not improve from 0.00006\n",
            "Epoch 1560/2000\n",
            " - 2s - loss: 1.0910e-04 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01560: loss did not improve from 0.00006\n",
            "Epoch 1561/2000\n",
            " - 2s - loss: 9.9024e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01561: loss did not improve from 0.00006\n",
            "Epoch 1562/2000\n",
            " - 2s - loss: 9.8045e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01562: loss did not improve from 0.00006\n",
            "Epoch 1563/2000\n",
            " - 2s - loss: 7.5802e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01563: loss did not improve from 0.00006\n",
            "Epoch 1564/2000\n",
            " - 2s - loss: 6.8681e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01564: loss did not improve from 0.00006\n",
            "Epoch 1565/2000\n",
            " - 2s - loss: 1.1090e-04 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01565: loss did not improve from 0.00006\n",
            "Epoch 1566/2000\n",
            " - 2s - loss: 1.0864e-04 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01566: loss did not improve from 0.00006\n",
            "Epoch 1567/2000\n",
            " - 2s - loss: 6.9979e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01567: loss did not improve from 0.00006\n",
            "Epoch 1568/2000\n",
            " - 2s - loss: 7.8810e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01568: loss did not improve from 0.00006\n",
            "Epoch 1569/2000\n",
            " - 2s - loss: 7.1119e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01569: loss did not improve from 0.00006\n",
            "Epoch 1570/2000\n",
            " - 2s - loss: 1.0715e-04 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01570: loss did not improve from 0.00006\n",
            "Epoch 1571/2000\n",
            " - 2s - loss: 7.0710e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01571: loss did not improve from 0.00006\n",
            "Epoch 1572/2000\n",
            " - 2s - loss: 1.0498e-04 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01572: loss did not improve from 0.00006\n",
            "Epoch 1573/2000\n",
            " - 2s - loss: 9.1920e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01573: loss did not improve from 0.00006\n",
            "Epoch 1574/2000\n",
            " - 2s - loss: 6.9521e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01574: loss did not improve from 0.00006\n",
            "Epoch 1575/2000\n",
            " - 2s - loss: 7.7532e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01575: loss did not improve from 0.00006\n",
            "Epoch 1576/2000\n",
            " - 2s - loss: 1.2491e-04 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01576: loss did not improve from 0.00006\n",
            "Epoch 1577/2000\n",
            " - 2s - loss: 1.2061e-04 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01577: loss did not improve from 0.00006\n",
            "Epoch 1578/2000\n",
            " - 2s - loss: 1.0387e-04 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01578: loss did not improve from 0.00006\n",
            "Epoch 1579/2000\n",
            " - 2s - loss: 9.1961e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01579: loss did not improve from 0.00006\n",
            "Epoch 1580/2000\n",
            " - 2s - loss: 5.8501e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01580: loss improved from 0.00006 to 0.00006, saving model to ./weights/lstmfcn_128_cells_weights/CBF_weights.h5\n",
            "Epoch 1581/2000\n",
            " - 2s - loss: 1.0865e-04 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01581: loss did not improve from 0.00006\n",
            "Epoch 1582/2000\n",
            " - 2s - loss: 1.0587e-04 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01582: loss did not improve from 0.00006\n",
            "Epoch 1583/2000\n",
            " - 2s - loss: 7.7775e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01583: loss did not improve from 0.00006\n",
            "Epoch 1584/2000\n",
            " - 2s - loss: 1.0215e-04 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01584: loss did not improve from 0.00006\n",
            "Epoch 1585/2000\n",
            " - 2s - loss: 7.9602e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01585: loss did not improve from 0.00006\n",
            "Epoch 1586/2000\n",
            " - 2s - loss: 9.8341e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01586: loss did not improve from 0.00006\n",
            "Epoch 1587/2000\n",
            " - 2s - loss: 9.2874e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01587: loss did not improve from 0.00006\n",
            "Epoch 1588/2000\n",
            " - 2s - loss: 6.1910e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01588: loss did not improve from 0.00006\n",
            "Epoch 1589/2000\n",
            " - 2s - loss: 1.1484e-04 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01589: loss did not improve from 0.00006\n",
            "Epoch 1590/2000\n",
            " - 2s - loss: 7.1366e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01590: loss did not improve from 0.00006\n",
            "Epoch 1591/2000\n",
            " - 2s - loss: 7.5898e-05 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01591: loss did not improve from 0.00006\n",
            "Epoch 1592/2000\n",
            " - 2s - loss: 1.1470e-04 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01592: loss did not improve from 0.00006\n",
            "Epoch 1593/2000\n",
            " - 2s - loss: 6.5064e-05 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01593: loss did not improve from 0.00006\n",
            "Epoch 1594/2000\n",
            " - 2s - loss: 1.2607e-04 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01594: loss did not improve from 0.00006\n",
            "Epoch 1595/2000\n",
            " - 2s - loss: 8.0429e-05 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01595: loss did not improve from 0.00006\n",
            "Epoch 1596/2000\n",
            " - 2s - loss: 7.4084e-05 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01596: loss did not improve from 0.00006\n",
            "Epoch 1597/2000\n",
            " - 2s - loss: 1.2629e-04 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01597: loss did not improve from 0.00006\n",
            "Epoch 1598/2000\n",
            " - 2s - loss: 7.8057e-05 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01598: loss did not improve from 0.00006\n",
            "Epoch 1599/2000\n",
            " - 2s - loss: 5.3451e-05 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01599: loss improved from 0.00006 to 0.00005, saving model to ./weights/lstmfcn_128_cells_weights/CBF_weights.h5\n",
            "Epoch 1600/2000\n",
            " - 2s - loss: 8.8973e-05 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01600: loss did not improve from 0.00005\n",
            "Epoch 1601/2000\n",
            " - 2s - loss: 1.1160e-04 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01601: loss did not improve from 0.00005\n",
            "Epoch 1602/2000\n",
            " - 2s - loss: 1.0598e-04 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01602: loss did not improve from 0.00005\n",
            "Epoch 1603/2000\n",
            " - 2s - loss: 7.8195e-05 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01603: loss did not improve from 0.00005\n",
            "Epoch 1604/2000\n",
            " - 2s - loss: 9.0184e-05 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01604: loss did not improve from 0.00005\n",
            "Epoch 1605/2000\n",
            " - 2s - loss: 8.7478e-05 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01605: loss did not improve from 0.00005\n",
            "Epoch 1606/2000\n",
            " - 2s - loss: 6.8486e-05 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01606: loss did not improve from 0.00005\n",
            "Epoch 1607/2000\n",
            " - 2s - loss: 8.2596e-05 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01607: loss did not improve from 0.00005\n",
            "Epoch 1608/2000\n",
            " - 2s - loss: 7.6071e-05 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01608: loss did not improve from 0.00005\n",
            "Epoch 1609/2000\n",
            " - 2s - loss: 6.7548e-05 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01609: loss did not improve from 0.00005\n",
            "Epoch 1610/2000\n",
            " - 2s - loss: 7.6916e-05 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01610: loss did not improve from 0.00005\n",
            "Epoch 1611/2000\n",
            " - 2s - loss: 9.3340e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01611: loss did not improve from 0.00005\n",
            "Epoch 1612/2000\n",
            " - 2s - loss: 8.5848e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01612: loss did not improve from 0.00005\n",
            "Epoch 1613/2000\n",
            " - 2s - loss: 9.1679e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01613: loss did not improve from 0.00005\n",
            "Epoch 1614/2000\n",
            " - 2s - loss: 8.3837e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01614: loss did not improve from 0.00005\n",
            "Epoch 1615/2000\n",
            " - 2s - loss: 1.1180e-04 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01615: loss did not improve from 0.00005\n",
            "Epoch 1616/2000\n",
            " - 2s - loss: 1.8784e-04 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01616: loss did not improve from 0.00005\n",
            "Epoch 1617/2000\n",
            " - 2s - loss: 7.8816e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01617: loss did not improve from 0.00005\n",
            "Epoch 1618/2000\n",
            " - 2s - loss: 9.8948e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01618: loss did not improve from 0.00005\n",
            "Epoch 1619/2000\n",
            " - 2s - loss: 8.6571e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01619: loss did not improve from 0.00005\n",
            "Epoch 1620/2000\n",
            " - 2s - loss: 8.8349e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01620: loss did not improve from 0.00005\n",
            "Epoch 1621/2000\n",
            " - 2s - loss: 9.1446e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01621: loss did not improve from 0.00005\n",
            "Epoch 1622/2000\n",
            " - 2s - loss: 5.7325e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01622: loss did not improve from 0.00005\n",
            "Epoch 1623/2000\n",
            " - 2s - loss: 7.8508e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01623: loss did not improve from 0.00005\n",
            "Epoch 1624/2000\n",
            " - 2s - loss: 9.2458e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01624: loss did not improve from 0.00005\n",
            "Epoch 1625/2000\n",
            " - 2s - loss: 6.4143e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01625: loss did not improve from 0.00005\n",
            "Epoch 1626/2000\n",
            " - 2s - loss: 9.6825e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01626: loss did not improve from 0.00005\n",
            "Epoch 1627/2000\n",
            " - 2s - loss: 1.0084e-04 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01627: loss did not improve from 0.00005\n",
            "Epoch 1628/2000\n",
            " - 2s - loss: 8.3296e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01628: loss did not improve from 0.00005\n",
            "Epoch 1629/2000\n",
            " - 2s - loss: 1.2927e-04 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01629: loss did not improve from 0.00005\n",
            "Epoch 1630/2000\n",
            " - 2s - loss: 1.1955e-04 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01630: loss did not improve from 0.00005\n",
            "Epoch 1631/2000\n",
            " - 2s - loss: 1.4684e-04 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01631: loss did not improve from 0.00005\n",
            "Epoch 1632/2000\n",
            " - 2s - loss: 7.4663e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01632: loss did not improve from 0.00005\n",
            "Epoch 1633/2000\n",
            " - 2s - loss: 9.8693e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01633: loss did not improve from 0.00005\n",
            "Epoch 1634/2000\n",
            " - 2s - loss: 7.9336e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01634: loss did not improve from 0.00005\n",
            "Epoch 1635/2000\n",
            " - 2s - loss: 8.9678e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01635: loss did not improve from 0.00005\n",
            "Epoch 1636/2000\n",
            " - 2s - loss: 6.4790e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01636: loss did not improve from 0.00005\n",
            "Epoch 1637/2000\n",
            " - 2s - loss: 6.8710e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01637: loss did not improve from 0.00005\n",
            "Epoch 1638/2000\n",
            " - 2s - loss: 1.2321e-04 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01638: loss did not improve from 0.00005\n",
            "Epoch 1639/2000\n",
            " - 2s - loss: 6.6757e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01639: loss did not improve from 0.00005\n",
            "Epoch 1640/2000\n",
            " - 2s - loss: 8.0302e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01640: loss did not improve from 0.00005\n",
            "Epoch 1641/2000\n",
            " - 2s - loss: 1.3603e-04 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01641: loss did not improve from 0.00005\n",
            "Epoch 1642/2000\n",
            " - 2s - loss: 8.7725e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01642: loss did not improve from 0.00005\n",
            "Epoch 1643/2000\n",
            " - 2s - loss: 7.1060e-05 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01643: loss did not improve from 0.00005\n",
            "Epoch 1644/2000\n",
            " - 2s - loss: 8.7108e-05 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01644: loss did not improve from 0.00005\n",
            "Epoch 1645/2000\n",
            " - 2s - loss: 9.4922e-05 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01645: loss did not improve from 0.00005\n",
            "Epoch 1646/2000\n",
            " - 2s - loss: 1.3012e-04 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01646: loss did not improve from 0.00005\n",
            "Epoch 1647/2000\n",
            " - 2s - loss: 5.8163e-05 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01647: loss did not improve from 0.00005\n",
            "Epoch 1648/2000\n",
            " - 2s - loss: 1.0160e-04 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01648: loss did not improve from 0.00005\n",
            "Epoch 1649/2000\n",
            " - 2s - loss: 6.5056e-05 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01649: loss did not improve from 0.00005\n",
            "Epoch 1650/2000\n",
            " - 2s - loss: 7.7595e-05 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01650: loss did not improve from 0.00005\n",
            "Epoch 1651/2000\n",
            " - 2s - loss: 6.3297e-05 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01651: loss did not improve from 0.00005\n",
            "Epoch 1652/2000\n",
            " - 2s - loss: 6.6881e-05 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01652: loss did not improve from 0.00005\n",
            "Epoch 1653/2000\n",
            " - 2s - loss: 8.8178e-05 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01653: loss did not improve from 0.00005\n",
            "Epoch 1654/2000\n",
            " - 2s - loss: 9.4019e-05 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01654: loss did not improve from 0.00005\n",
            "Epoch 1655/2000\n",
            " - 2s - loss: 6.8823e-05 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01655: loss did not improve from 0.00005\n",
            "Epoch 1656/2000\n",
            " - 2s - loss: 9.7238e-05 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01656: loss did not improve from 0.00005\n",
            "Epoch 1657/2000\n",
            " - 2s - loss: 9.3646e-05 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01657: loss did not improve from 0.00005\n",
            "Epoch 1658/2000\n",
            " - 2s - loss: 9.9778e-05 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01658: loss did not improve from 0.00005\n",
            "Epoch 1659/2000\n",
            " - 2s - loss: 8.4378e-05 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01659: loss did not improve from 0.00005\n",
            "Epoch 1660/2000\n",
            " - 2s - loss: 8.8365e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01660: loss did not improve from 0.00005\n",
            "Epoch 1661/2000\n",
            " - 2s - loss: 1.0370e-04 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01661: loss did not improve from 0.00005\n",
            "Epoch 1662/2000\n",
            " - 2s - loss: 7.0427e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01662: loss did not improve from 0.00005\n",
            "Epoch 1663/2000\n",
            " - 2s - loss: 1.0525e-04 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01663: loss did not improve from 0.00005\n",
            "Epoch 1664/2000\n",
            " - 2s - loss: 7.2476e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01664: loss did not improve from 0.00005\n",
            "Epoch 1665/2000\n",
            " - 2s - loss: 1.1608e-04 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01665: loss did not improve from 0.00005\n",
            "Epoch 1666/2000\n",
            " - 2s - loss: 6.9467e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01666: loss did not improve from 0.00005\n",
            "Epoch 1667/2000\n",
            " - 2s - loss: 9.1252e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01667: loss did not improve from 0.00005\n",
            "Epoch 1668/2000\n",
            " - 2s - loss: 1.0815e-04 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01668: loss did not improve from 0.00005\n",
            "Epoch 1669/2000\n",
            " - 2s - loss: 9.0687e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01669: loss did not improve from 0.00005\n",
            "Epoch 1670/2000\n",
            " - 2s - loss: 7.5657e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01670: loss did not improve from 0.00005\n",
            "Epoch 1671/2000\n",
            " - 2s - loss: 7.8925e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01671: loss did not improve from 0.00005\n",
            "Epoch 1672/2000\n",
            " - 2s - loss: 7.0217e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01672: loss did not improve from 0.00005\n",
            "Epoch 1673/2000\n",
            " - 2s - loss: 9.6228e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01673: loss did not improve from 0.00005\n",
            "Epoch 1674/2000\n",
            " - 2s - loss: 6.8064e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01674: loss did not improve from 0.00005\n",
            "Epoch 1675/2000\n",
            " - 2s - loss: 8.8337e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01675: loss did not improve from 0.00005\n",
            "Epoch 1676/2000\n",
            " - 2s - loss: 1.0022e-04 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01676: loss did not improve from 0.00005\n",
            "Epoch 1677/2000\n",
            " - 2s - loss: 1.0867e-04 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01677: loss did not improve from 0.00005\n",
            "Epoch 1678/2000\n",
            " - 2s - loss: 7.8717e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01678: loss did not improve from 0.00005\n",
            "Epoch 1679/2000\n",
            " - 2s - loss: 5.8091e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01679: loss did not improve from 0.00005\n",
            "Epoch 1680/2000\n",
            " - 2s - loss: 7.3694e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01680: loss did not improve from 0.00005\n",
            "Epoch 1681/2000\n",
            " - 2s - loss: 1.0785e-04 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01681: loss did not improve from 0.00005\n",
            "Epoch 1682/2000\n",
            " - 2s - loss: 7.1848e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01682: loss did not improve from 0.00005\n",
            "Epoch 1683/2000\n",
            " - 2s - loss: 1.0810e-04 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01683: loss did not improve from 0.00005\n",
            "Epoch 1684/2000\n",
            " - 2s - loss: 7.1072e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01684: loss did not improve from 0.00005\n",
            "Epoch 1685/2000\n",
            " - 2s - loss: 8.2619e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01685: loss did not improve from 0.00005\n",
            "Epoch 1686/2000\n",
            " - 2s - loss: 1.0932e-04 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01686: loss did not improve from 0.00005\n",
            "Epoch 1687/2000\n",
            " - 2s - loss: 1.0052e-04 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01687: loss did not improve from 0.00005\n",
            "Epoch 1688/2000\n",
            " - 2s - loss: 5.9653e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01688: loss did not improve from 0.00005\n",
            "Epoch 1689/2000\n",
            " - 2s - loss: 9.9703e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01689: loss did not improve from 0.00005\n",
            "Epoch 1690/2000\n",
            " - 2s - loss: 7.3658e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01690: loss did not improve from 0.00005\n",
            "Epoch 1691/2000\n",
            " - 2s - loss: 7.1353e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01691: loss did not improve from 0.00005\n",
            "Epoch 1692/2000\n",
            " - 2s - loss: 1.1157e-04 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01692: loss did not improve from 0.00005\n",
            "Epoch 1693/2000\n",
            " - 2s - loss: 8.7756e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01693: loss did not improve from 0.00005\n",
            "Epoch 1694/2000\n",
            " - 2s - loss: 8.3452e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01694: loss did not improve from 0.00005\n",
            "Epoch 1695/2000\n",
            " - 2s - loss: 8.7289e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01695: loss did not improve from 0.00005\n",
            "Epoch 1696/2000\n",
            " - 2s - loss: 1.0064e-04 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01696: loss did not improve from 0.00005\n",
            "Epoch 1697/2000\n",
            " - 2s - loss: 1.4818e-04 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01697: loss did not improve from 0.00005\n",
            "Epoch 1698/2000\n",
            " - 2s - loss: 7.0106e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01698: loss did not improve from 0.00005\n",
            "Epoch 1699/2000\n",
            " - 2s - loss: 1.1320e-04 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01699: loss did not improve from 0.00005\n",
            "Epoch 1700/2000\n",
            " - 2s - loss: 1.4198e-04 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01700: loss did not improve from 0.00005\n",
            "Epoch 1701/2000\n",
            " - 2s - loss: 6.4005e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01701: loss did not improve from 0.00005\n",
            "Epoch 1702/2000\n",
            " - 2s - loss: 5.9005e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01702: loss did not improve from 0.00005\n",
            "Epoch 1703/2000\n",
            " - 2s - loss: 1.1470e-04 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01703: loss did not improve from 0.00005\n",
            "Epoch 1704/2000\n",
            " - 2s - loss: 8.8326e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01704: loss did not improve from 0.00005\n",
            "Epoch 1705/2000\n",
            " - 2s - loss: 7.8510e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01705: loss did not improve from 0.00005\n",
            "Epoch 1706/2000\n",
            " - 2s - loss: 7.1370e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01706: loss did not improve from 0.00005\n",
            "Epoch 1707/2000\n",
            " - 2s - loss: 1.1620e-04 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01707: loss did not improve from 0.00005\n",
            "Epoch 1708/2000\n",
            " - 2s - loss: 8.0063e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01708: loss did not improve from 0.00005\n",
            "Epoch 1709/2000\n",
            " - 2s - loss: 7.8184e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01709: loss did not improve from 0.00005\n",
            "Epoch 1710/2000\n",
            " - 2s - loss: 8.4406e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01710: loss did not improve from 0.00005\n",
            "Epoch 1711/2000\n",
            " - 2s - loss: 1.0066e-04 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01711: loss did not improve from 0.00005\n",
            "Epoch 1712/2000\n",
            " - 2s - loss: 8.1277e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01712: loss did not improve from 0.00005\n",
            "Epoch 1713/2000\n",
            " - 2s - loss: 6.8148e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01713: loss did not improve from 0.00005\n",
            "Epoch 1714/2000\n",
            " - 2s - loss: 9.0398e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01714: loss did not improve from 0.00005\n",
            "Epoch 1715/2000\n",
            " - 2s - loss: 6.9813e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01715: loss did not improve from 0.00005\n",
            "Epoch 1716/2000\n",
            " - 2s - loss: 1.0908e-04 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01716: loss did not improve from 0.00005\n",
            "Epoch 1717/2000\n",
            " - 2s - loss: 1.0402e-04 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01717: loss did not improve from 0.00005\n",
            "Epoch 1718/2000\n",
            " - 2s - loss: 6.9884e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01718: loss did not improve from 0.00005\n",
            "Epoch 1719/2000\n",
            " - 2s - loss: 6.1592e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01719: loss did not improve from 0.00005\n",
            "Epoch 1720/2000\n",
            " - 2s - loss: 1.1001e-04 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01720: loss did not improve from 0.00005\n",
            "Epoch 1721/2000\n",
            " - 2s - loss: 1.0130e-04 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01721: loss did not improve from 0.00005\n",
            "Epoch 1722/2000\n",
            " - 2s - loss: 8.4231e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01722: loss did not improve from 0.00005\n",
            "Epoch 1723/2000\n",
            " - 2s - loss: 1.0393e-04 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01723: loss did not improve from 0.00005\n",
            "Epoch 1724/2000\n",
            " - 2s - loss: 6.4961e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01724: loss did not improve from 0.00005\n",
            "Epoch 1725/2000\n",
            " - 2s - loss: 9.7489e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01725: loss did not improve from 0.00005\n",
            "Epoch 1726/2000\n",
            " - 2s - loss: 7.9165e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01726: loss did not improve from 0.00005\n",
            "Epoch 1727/2000\n",
            " - 2s - loss: 1.7516e-04 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01727: loss did not improve from 0.00005\n",
            "Epoch 1728/2000\n",
            " - 2s - loss: 8.2688e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01728: loss did not improve from 0.00005\n",
            "Epoch 1729/2000\n",
            " - 2s - loss: 9.0186e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01729: loss did not improve from 0.00005\n",
            "Epoch 1730/2000\n",
            " - 2s - loss: 8.0256e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01730: loss did not improve from 0.00005\n",
            "Epoch 1731/2000\n",
            " - 2s - loss: 7.5052e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01731: loss did not improve from 0.00005\n",
            "Epoch 1732/2000\n",
            " - 2s - loss: 9.3604e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01732: loss did not improve from 0.00005\n",
            "Epoch 1733/2000\n",
            " - 2s - loss: 9.9640e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01733: loss did not improve from 0.00005\n",
            "Epoch 1734/2000\n",
            " - 2s - loss: 7.8077e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01734: loss did not improve from 0.00005\n",
            "Epoch 1735/2000\n",
            " - 2s - loss: 7.8686e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01735: loss did not improve from 0.00005\n",
            "Epoch 1736/2000\n",
            " - 2s - loss: 8.4286e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01736: loss did not improve from 0.00005\n",
            "Epoch 1737/2000\n",
            " - 2s - loss: 6.0933e-05 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01737: loss did not improve from 0.00005\n",
            "Epoch 1738/2000\n",
            " - 2s - loss: 9.1298e-05 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01738: loss did not improve from 0.00005\n",
            "Epoch 1739/2000\n",
            " - 2s - loss: 6.7711e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01739: loss did not improve from 0.00005\n",
            "Epoch 1740/2000\n",
            " - 2s - loss: 7.4921e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01740: loss did not improve from 0.00005\n",
            "Epoch 1741/2000\n",
            " - 2s - loss: 8.0722e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01741: loss did not improve from 0.00005\n",
            "Epoch 1742/2000\n",
            " - 2s - loss: 5.9287e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01742: loss did not improve from 0.00005\n",
            "Epoch 1743/2000\n",
            " - 2s - loss: 9.4487e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01743: loss did not improve from 0.00005\n",
            "Epoch 1744/2000\n",
            " - 2s - loss: 9.6534e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01744: loss did not improve from 0.00005\n",
            "Epoch 1745/2000\n",
            " - 2s - loss: 1.0551e-04 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01745: loss did not improve from 0.00005\n",
            "Epoch 1746/2000\n",
            " - 2s - loss: 7.2388e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01746: loss did not improve from 0.00005\n",
            "Epoch 1747/2000\n",
            " - 2s - loss: 8.5858e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01747: loss did not improve from 0.00005\n",
            "Epoch 1748/2000\n",
            " - 2s - loss: 7.8625e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01748: loss did not improve from 0.00005\n",
            "Epoch 1749/2000\n",
            " - 2s - loss: 7.0039e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01749: loss did not improve from 0.00005\n",
            "Epoch 1750/2000\n",
            " - 2s - loss: 9.4863e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01750: loss did not improve from 0.00005\n",
            "Epoch 1751/2000\n",
            " - 2s - loss: 8.7914e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01751: loss did not improve from 0.00005\n",
            "Epoch 1752/2000\n",
            " - 2s - loss: 7.6589e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01752: loss did not improve from 0.00005\n",
            "Epoch 1753/2000\n",
            " - 2s - loss: 6.9780e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01753: loss did not improve from 0.00005\n",
            "Epoch 1754/2000\n",
            " - 2s - loss: 8.8696e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01754: loss did not improve from 0.00005\n",
            "Epoch 1755/2000\n",
            " - 2s - loss: 7.1297e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01755: loss did not improve from 0.00005\n",
            "Epoch 1756/2000\n",
            " - 2s - loss: 8.0627e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01756: loss did not improve from 0.00005\n",
            "Epoch 1757/2000\n",
            " - 2s - loss: 1.1027e-04 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01757: loss did not improve from 0.00005\n",
            "Epoch 1758/2000\n",
            " - 2s - loss: 8.6377e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01758: loss did not improve from 0.00005\n",
            "Epoch 1759/2000\n",
            " - 2s - loss: 7.6146e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01759: loss did not improve from 0.00005\n",
            "Epoch 1760/2000\n",
            " - 2s - loss: 6.5326e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01760: loss did not improve from 0.00005\n",
            "Epoch 1761/2000\n",
            " - 2s - loss: 6.6328e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01761: loss did not improve from 0.00005\n",
            "Epoch 1762/2000\n",
            " - 2s - loss: 6.9439e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01762: loss did not improve from 0.00005\n",
            "Epoch 1763/2000\n",
            " - 2s - loss: 9.0367e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01763: loss did not improve from 0.00005\n",
            "Epoch 1764/2000\n",
            " - 2s - loss: 7.9511e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01764: loss did not improve from 0.00005\n",
            "Epoch 1765/2000\n",
            " - 2s - loss: 7.6801e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01765: loss did not improve from 0.00005\n",
            "Epoch 1766/2000\n",
            " - 2s - loss: 6.9958e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01766: loss did not improve from 0.00005\n",
            "Epoch 1767/2000\n",
            " - 2s - loss: 7.7111e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01767: loss did not improve from 0.00005\n",
            "Epoch 1768/2000\n",
            " - 2s - loss: 6.1040e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01768: loss did not improve from 0.00005\n",
            "Epoch 1769/2000\n",
            " - 2s - loss: 6.5282e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01769: loss did not improve from 0.00005\n",
            "Epoch 1770/2000\n",
            " - 2s - loss: 8.7626e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01770: loss did not improve from 0.00005\n",
            "Epoch 1771/2000\n",
            " - 2s - loss: 8.5109e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01771: loss did not improve from 0.00005\n",
            "Epoch 1772/2000\n",
            " - 2s - loss: 1.2381e-04 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01772: loss did not improve from 0.00005\n",
            "Epoch 1773/2000\n",
            " - 2s - loss: 7.6121e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01773: loss did not improve from 0.00005\n",
            "Epoch 1774/2000\n",
            " - 2s - loss: 9.7997e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01774: loss did not improve from 0.00005\n",
            "Epoch 1775/2000\n",
            " - 2s - loss: 1.5593e-04 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01775: loss did not improve from 0.00005\n",
            "Epoch 1776/2000\n",
            " - 2s - loss: 7.2146e-05 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01776: loss did not improve from 0.00005\n",
            "Epoch 1777/2000\n",
            " - 2s - loss: 7.5622e-05 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01777: loss did not improve from 0.00005\n",
            "Epoch 1778/2000\n",
            " - 2s - loss: 9.4846e-05 - accuracy: 1.0000 - val_loss: 0.0227 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01778: loss did not improve from 0.00005\n",
            "Epoch 1779/2000\n",
            " - 2s - loss: 9.3094e-05 - accuracy: 1.0000 - val_loss: 0.0226 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01779: loss did not improve from 0.00005\n",
            "Epoch 1780/2000\n",
            " - 2s - loss: 8.4219e-05 - accuracy: 1.0000 - val_loss: 0.0226 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01780: loss did not improve from 0.00005\n",
            "Epoch 1781/2000\n",
            " - 2s - loss: 7.2824e-05 - accuracy: 1.0000 - val_loss: 0.0225 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01781: loss did not improve from 0.00005\n",
            "Epoch 1782/2000\n",
            " - 2s - loss: 7.5013e-05 - accuracy: 1.0000 - val_loss: 0.0225 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01782: loss did not improve from 0.00005\n",
            "Epoch 1783/2000\n",
            " - 2s - loss: 7.7446e-05 - accuracy: 1.0000 - val_loss: 0.0225 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01783: loss did not improve from 0.00005\n",
            "Epoch 1784/2000\n",
            " - 2s - loss: 1.0466e-04 - accuracy: 1.0000 - val_loss: 0.0224 - val_accuracy: 0.9956\n",
            "\n",
            "Epoch 01784: loss did not improve from 0.00005\n",
            "Epoch 1785/2000\n",
            " - 2s - loss: 6.9176e-05 - accuracy: 1.0000 - val_loss: 0.0224 - val_accuracy: 0.9956\n",
            "\n",
            "Epoch 01785: loss did not improve from 0.00005\n",
            "Epoch 1786/2000\n",
            " - 2s - loss: 7.3824e-05 - accuracy: 1.0000 - val_loss: 0.0224 - val_accuracy: 0.9956\n",
            "\n",
            "Epoch 01786: loss did not improve from 0.00005\n",
            "Epoch 1787/2000\n",
            " - 2s - loss: 5.3884e-05 - accuracy: 1.0000 - val_loss: 0.0224 - val_accuracy: 0.9956\n",
            "\n",
            "Epoch 01787: loss did not improve from 0.00005\n",
            "Epoch 1788/2000\n",
            " - 2s - loss: 1.1371e-04 - accuracy: 1.0000 - val_loss: 0.0224 - val_accuracy: 0.9956\n",
            "\n",
            "Epoch 01788: loss did not improve from 0.00005\n",
            "Epoch 1789/2000\n",
            " - 2s - loss: 7.0992e-05 - accuracy: 1.0000 - val_loss: 0.0224 - val_accuracy: 0.9956\n",
            "\n",
            "Epoch 01789: loss did not improve from 0.00005\n",
            "Epoch 1790/2000\n",
            " - 2s - loss: 7.8978e-05 - accuracy: 1.0000 - val_loss: 0.0224 - val_accuracy: 0.9956\n",
            "\n",
            "Epoch 01790: loss did not improve from 0.00005\n",
            "Epoch 1791/2000\n",
            " - 2s - loss: 7.5241e-05 - accuracy: 1.0000 - val_loss: 0.0225 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01791: loss did not improve from 0.00005\n",
            "Epoch 1792/2000\n",
            " - 2s - loss: 5.7686e-05 - accuracy: 1.0000 - val_loss: 0.0225 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01792: loss did not improve from 0.00005\n",
            "Epoch 1793/2000\n",
            " - 2s - loss: 6.2971e-05 - accuracy: 1.0000 - val_loss: 0.0225 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01793: loss did not improve from 0.00005\n",
            "Epoch 1794/2000\n",
            " - 2s - loss: 9.2709e-05 - accuracy: 1.0000 - val_loss: 0.0225 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01794: loss did not improve from 0.00005\n",
            "Epoch 1795/2000\n",
            " - 2s - loss: 7.4753e-05 - accuracy: 1.0000 - val_loss: 0.0225 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01795: loss did not improve from 0.00005\n",
            "Epoch 1796/2000\n",
            " - 2s - loss: 8.4282e-05 - accuracy: 1.0000 - val_loss: 0.0226 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01796: loss did not improve from 0.00005\n",
            "Epoch 1797/2000\n",
            " - 2s - loss: 8.1982e-05 - accuracy: 1.0000 - val_loss: 0.0226 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01797: loss did not improve from 0.00005\n",
            "Epoch 1798/2000\n",
            " - 2s - loss: 6.4147e-05 - accuracy: 1.0000 - val_loss: 0.0226 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01798: loss did not improve from 0.00005\n",
            "Epoch 1799/2000\n",
            " - 2s - loss: 8.5375e-05 - accuracy: 1.0000 - val_loss: 0.0226 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01799: loss did not improve from 0.00005\n",
            "Epoch 1800/2000\n",
            " - 2s - loss: 1.5113e-04 - accuracy: 1.0000 - val_loss: 0.0226 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01800: loss did not improve from 0.00005\n",
            "Epoch 1801/2000\n",
            " - 2s - loss: 9.3592e-05 - accuracy: 1.0000 - val_loss: 0.0227 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01801: loss did not improve from 0.00005\n",
            "Epoch 1802/2000\n",
            " - 2s - loss: 1.0187e-04 - accuracy: 1.0000 - val_loss: 0.0227 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01802: loss did not improve from 0.00005\n",
            "Epoch 1803/2000\n",
            " - 2s - loss: 8.5195e-05 - accuracy: 1.0000 - val_loss: 0.0227 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01803: loss did not improve from 0.00005\n",
            "Epoch 1804/2000\n",
            " - 2s - loss: 7.5437e-05 - accuracy: 1.0000 - val_loss: 0.0227 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01804: loss did not improve from 0.00005\n",
            "Epoch 1805/2000\n",
            " - 2s - loss: 1.2303e-04 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01805: loss did not improve from 0.00005\n",
            "Epoch 1806/2000\n",
            " - 2s - loss: 1.1249e-04 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01806: loss did not improve from 0.00005\n",
            "Epoch 1807/2000\n",
            " - 2s - loss: 1.0002e-04 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01807: loss did not improve from 0.00005\n",
            "Epoch 1808/2000\n",
            " - 2s - loss: 7.0191e-05 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01808: loss did not improve from 0.00005\n",
            "Epoch 1809/2000\n",
            " - 2s - loss: 7.6114e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01809: loss did not improve from 0.00005\n",
            "Epoch 1810/2000\n",
            " - 2s - loss: 7.5673e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01810: loss did not improve from 0.00005\n",
            "Epoch 1811/2000\n",
            " - 2s - loss: 8.6407e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01811: loss did not improve from 0.00005\n",
            "Epoch 1812/2000\n",
            " - 2s - loss: 7.0836e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01812: loss did not improve from 0.00005\n",
            "Epoch 1813/2000\n",
            " - 2s - loss: 6.2532e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01813: loss did not improve from 0.00005\n",
            "Epoch 1814/2000\n",
            " - 2s - loss: 8.2292e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01814: loss did not improve from 0.00005\n",
            "Epoch 1815/2000\n",
            " - 2s - loss: 6.4937e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01815: loss did not improve from 0.00005\n",
            "Epoch 1816/2000\n",
            " - 2s - loss: 9.7913e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01816: loss did not improve from 0.00005\n",
            "Epoch 1817/2000\n",
            " - 2s - loss: 6.1638e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01817: loss did not improve from 0.00005\n",
            "Epoch 1818/2000\n",
            " - 2s - loss: 7.6355e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01818: loss did not improve from 0.00005\n",
            "Epoch 1819/2000\n",
            " - 2s - loss: 8.2177e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01819: loss did not improve from 0.00005\n",
            "Epoch 1820/2000\n",
            " - 2s - loss: 8.9829e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01820: loss did not improve from 0.00005\n",
            "Epoch 1821/2000\n",
            " - 2s - loss: 6.8100e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01821: loss did not improve from 0.00005\n",
            "Epoch 1822/2000\n",
            " - 2s - loss: 6.4719e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01822: loss did not improve from 0.00005\n",
            "Epoch 1823/2000\n",
            " - 2s - loss: 7.6792e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01823: loss did not improve from 0.00005\n",
            "Epoch 1824/2000\n",
            " - 2s - loss: 9.2207e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01824: loss did not improve from 0.00005\n",
            "Epoch 1825/2000\n",
            " - 2s - loss: 6.7869e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01825: loss did not improve from 0.00005\n",
            "Epoch 1826/2000\n",
            " - 2s - loss: 7.7468e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01826: loss did not improve from 0.00005\n",
            "Epoch 1827/2000\n",
            " - 2s - loss: 6.4749e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01827: loss did not improve from 0.00005\n",
            "Epoch 1828/2000\n",
            " - 2s - loss: 6.8004e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01828: loss did not improve from 0.00005\n",
            "Epoch 1829/2000\n",
            " - 2s - loss: 7.4066e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01829: loss did not improve from 0.00005\n",
            "Epoch 1830/2000\n",
            " - 2s - loss: 7.7781e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01830: loss did not improve from 0.00005\n",
            "Epoch 1831/2000\n",
            " - 2s - loss: 1.1135e-04 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01831: loss did not improve from 0.00005\n",
            "Epoch 1832/2000\n",
            " - 2s - loss: 1.1943e-04 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01832: loss did not improve from 0.00005\n",
            "Epoch 1833/2000\n",
            " - 2s - loss: 8.3178e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01833: loss did not improve from 0.00005\n",
            "Epoch 1834/2000\n",
            " - 2s - loss: 6.9542e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01834: loss did not improve from 0.00005\n",
            "Epoch 1835/2000\n",
            " - 2s - loss: 7.1334e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01835: loss did not improve from 0.00005\n",
            "Epoch 1836/2000\n",
            " - 2s - loss: 8.5596e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01836: loss did not improve from 0.00005\n",
            "Epoch 1837/2000\n",
            " - 2s - loss: 1.0332e-04 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01837: loss did not improve from 0.00005\n",
            "Epoch 1838/2000\n",
            " - 2s - loss: 9.5374e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01838: loss did not improve from 0.00005\n",
            "Epoch 1839/2000\n",
            " - 2s - loss: 9.5781e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01839: loss did not improve from 0.00005\n",
            "Epoch 1840/2000\n",
            " - 2s - loss: 8.7643e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01840: loss did not improve from 0.00005\n",
            "Epoch 1841/2000\n",
            " - 2s - loss: 6.7631e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01841: loss did not improve from 0.00005\n",
            "Epoch 1842/2000\n",
            " - 2s - loss: 1.2468e-04 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01842: loss did not improve from 0.00005\n",
            "Epoch 1843/2000\n",
            " - 2s - loss: 8.4399e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01843: loss did not improve from 0.00005\n",
            "Epoch 1844/2000\n",
            " - 2s - loss: 5.6002e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01844: loss did not improve from 0.00005\n",
            "Epoch 1845/2000\n",
            " - 2s - loss: 7.6772e-05 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01845: loss did not improve from 0.00005\n",
            "Epoch 1846/2000\n",
            " - 2s - loss: 6.1712e-05 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01846: loss did not improve from 0.00005\n",
            "Epoch 1847/2000\n",
            " - 2s - loss: 8.9568e-05 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01847: loss did not improve from 0.00005\n",
            "Epoch 1848/2000\n",
            " - 2s - loss: 5.3165e-05 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01848: loss improved from 0.00005 to 0.00005, saving model to ./weights/lstmfcn_128_cells_weights/CBF_weights.h5\n",
            "Epoch 1849/2000\n",
            " - 2s - loss: 7.3423e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01849: loss did not improve from 0.00005\n",
            "Epoch 1850/2000\n",
            " - 2s - loss: 6.7226e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01850: loss did not improve from 0.00005\n",
            "Epoch 1851/2000\n",
            " - 2s - loss: 9.9441e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01851: loss did not improve from 0.00005\n",
            "Epoch 1852/2000\n",
            " - 2s - loss: 8.0147e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01852: loss did not improve from 0.00005\n",
            "Epoch 1853/2000\n",
            " - 2s - loss: 1.3915e-04 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01853: loss did not improve from 0.00005\n",
            "Epoch 1854/2000\n",
            " - 2s - loss: 8.5730e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01854: loss did not improve from 0.00005\n",
            "Epoch 1855/2000\n",
            " - 2s - loss: 6.3376e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01855: loss did not improve from 0.00005\n",
            "Epoch 1856/2000\n",
            " - 2s - loss: 7.0909e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01856: loss did not improve from 0.00005\n",
            "Epoch 1857/2000\n",
            " - 2s - loss: 7.1043e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01857: loss did not improve from 0.00005\n",
            "Epoch 1858/2000\n",
            " - 2s - loss: 5.5179e-05 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01858: loss did not improve from 0.00005\n",
            "Epoch 1859/2000\n",
            " - 2s - loss: 5.6808e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01859: loss did not improve from 0.00005\n",
            "Epoch 1860/2000\n",
            " - 2s - loss: 1.6971e-04 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01860: loss did not improve from 0.00005\n",
            "Epoch 1861/2000\n",
            " - 2s - loss: 9.4757e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01861: loss did not improve from 0.00005\n",
            "Epoch 1862/2000\n",
            " - 2s - loss: 8.9137e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01862: loss did not improve from 0.00005\n",
            "Epoch 1863/2000\n",
            " - 2s - loss: 7.8580e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01863: loss did not improve from 0.00005\n",
            "Epoch 1864/2000\n",
            " - 2s - loss: 6.9510e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01864: loss did not improve from 0.00005\n",
            "Epoch 1865/2000\n",
            " - 2s - loss: 9.7778e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01865: loss did not improve from 0.00005\n",
            "Epoch 1866/2000\n",
            " - 2s - loss: 9.5082e-05 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01866: loss did not improve from 0.00005\n",
            "Epoch 1867/2000\n",
            " - 2s - loss: 6.2705e-05 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01867: loss did not improve from 0.00005\n",
            "Epoch 1868/2000\n",
            " - 2s - loss: 9.5096e-05 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01868: loss did not improve from 0.00005\n",
            "Epoch 1869/2000\n",
            " - 2s - loss: 1.0083e-04 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01869: loss did not improve from 0.00005\n",
            "Epoch 1870/2000\n",
            " - 2s - loss: 7.3740e-05 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01870: loss did not improve from 0.00005\n",
            "Epoch 1871/2000\n",
            " - 2s - loss: 7.2439e-05 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01871: loss did not improve from 0.00005\n",
            "Epoch 1872/2000\n",
            " - 2s - loss: 8.2798e-05 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01872: loss did not improve from 0.00005\n",
            "Epoch 1873/2000\n",
            " - 2s - loss: 8.0382e-05 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01873: loss did not improve from 0.00005\n",
            "Epoch 1874/2000\n",
            " - 2s - loss: 5.3018e-05 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01874: loss improved from 0.00005 to 0.00005, saving model to ./weights/lstmfcn_128_cells_weights/CBF_weights.h5\n",
            "Epoch 1875/2000\n",
            " - 2s - loss: 9.7426e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01875: loss did not improve from 0.00005\n",
            "Epoch 1876/2000\n",
            " - 2s - loss: 4.7952e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01876: loss improved from 0.00005 to 0.00005, saving model to ./weights/lstmfcn_128_cells_weights/CBF_weights.h5\n",
            "Epoch 1877/2000\n",
            " - 2s - loss: 6.7754e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01877: loss did not improve from 0.00005\n",
            "Epoch 1878/2000\n",
            " - 2s - loss: 8.7990e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01878: loss did not improve from 0.00005\n",
            "Epoch 1879/2000\n",
            " - 2s - loss: 5.1405e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01879: loss did not improve from 0.00005\n",
            "Epoch 1880/2000\n",
            " - 2s - loss: 7.7730e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01880: loss did not improve from 0.00005\n",
            "Epoch 1881/2000\n",
            " - 2s - loss: 1.1279e-04 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01881: loss did not improve from 0.00005\n",
            "Epoch 1882/2000\n",
            " - 2s - loss: 8.9590e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01882: loss did not improve from 0.00005\n",
            "Epoch 1883/2000\n",
            " - 2s - loss: 6.4143e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01883: loss did not improve from 0.00005\n",
            "Epoch 1884/2000\n",
            " - 2s - loss: 6.4420e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01884: loss did not improve from 0.00005\n",
            "Epoch 1885/2000\n",
            " - 2s - loss: 7.6859e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01885: loss did not improve from 0.00005\n",
            "Epoch 1886/2000\n",
            " - 2s - loss: 7.6432e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01886: loss did not improve from 0.00005\n",
            "Epoch 1887/2000\n",
            " - 2s - loss: 6.8704e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01887: loss did not improve from 0.00005\n",
            "Epoch 1888/2000\n",
            " - 2s - loss: 8.4049e-05 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01888: loss did not improve from 0.00005\n",
            "Epoch 1889/2000\n",
            " - 2s - loss: 6.7233e-05 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01889: loss did not improve from 0.00005\n",
            "Epoch 1890/2000\n",
            " - 2s - loss: 8.1938e-05 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01890: loss did not improve from 0.00005\n",
            "Epoch 1891/2000\n",
            " - 2s - loss: 1.1110e-04 - accuracy: 1.0000 - val_loss: 0.0227 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01891: loss did not improve from 0.00005\n",
            "Epoch 1892/2000\n",
            " - 2s - loss: 9.1883e-05 - accuracy: 1.0000 - val_loss: 0.0227 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01892: loss did not improve from 0.00005\n",
            "Epoch 1893/2000\n",
            " - 2s - loss: 9.7001e-05 - accuracy: 1.0000 - val_loss: 0.0227 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01893: loss did not improve from 0.00005\n",
            "Epoch 1894/2000\n",
            " - 2s - loss: 6.0726e-05 - accuracy: 1.0000 - val_loss: 0.0227 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01894: loss did not improve from 0.00005\n",
            "Epoch 1895/2000\n",
            " - 2s - loss: 7.1465e-05 - accuracy: 1.0000 - val_loss: 0.0227 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01895: loss did not improve from 0.00005\n",
            "Epoch 1896/2000\n",
            " - 2s - loss: 7.3631e-05 - accuracy: 1.0000 - val_loss: 0.0226 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01896: loss did not improve from 0.00005\n",
            "Epoch 1897/2000\n",
            " - 2s - loss: 7.7935e-05 - accuracy: 1.0000 - val_loss: 0.0226 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01897: loss did not improve from 0.00005\n",
            "Epoch 1898/2000\n",
            " - 2s - loss: 7.1040e-05 - accuracy: 1.0000 - val_loss: 0.0226 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01898: loss did not improve from 0.00005\n",
            "Epoch 1899/2000\n",
            " - 2s - loss: 6.6247e-05 - accuracy: 1.0000 - val_loss: 0.0226 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01899: loss did not improve from 0.00005\n",
            "Epoch 1900/2000\n",
            " - 2s - loss: 1.0708e-04 - accuracy: 1.0000 - val_loss: 0.0226 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01900: loss did not improve from 0.00005\n",
            "Epoch 1901/2000\n",
            " - 2s - loss: 5.3748e-05 - accuracy: 1.0000 - val_loss: 0.0226 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01901: loss did not improve from 0.00005\n",
            "Epoch 1902/2000\n",
            " - 2s - loss: 6.8140e-05 - accuracy: 1.0000 - val_loss: 0.0226 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01902: loss did not improve from 0.00005\n",
            "Epoch 1903/2000\n",
            " - 2s - loss: 8.3259e-05 - accuracy: 1.0000 - val_loss: 0.0226 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01903: loss did not improve from 0.00005\n",
            "Epoch 1904/2000\n",
            " - 2s - loss: 8.6346e-05 - accuracy: 1.0000 - val_loss: 0.0226 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01904: loss did not improve from 0.00005\n",
            "Epoch 1905/2000\n",
            " - 2s - loss: 8.9428e-05 - accuracy: 1.0000 - val_loss: 0.0226 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01905: loss did not improve from 0.00005\n",
            "Epoch 1906/2000\n",
            " - 2s - loss: 7.1649e-05 - accuracy: 1.0000 - val_loss: 0.0226 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01906: loss did not improve from 0.00005\n",
            "Epoch 1907/2000\n",
            " - 2s - loss: 1.0737e-04 - accuracy: 1.0000 - val_loss: 0.0225 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01907: loss did not improve from 0.00005\n",
            "Epoch 1908/2000\n",
            " - 2s - loss: 1.6612e-04 - accuracy: 1.0000 - val_loss: 0.0226 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01908: loss did not improve from 0.00005\n",
            "Epoch 1909/2000\n",
            " - 2s - loss: 6.3738e-05 - accuracy: 1.0000 - val_loss: 0.0226 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01909: loss did not improve from 0.00005\n",
            "Epoch 1910/2000\n",
            " - 2s - loss: 7.2220e-05 - accuracy: 1.0000 - val_loss: 0.0226 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01910: loss did not improve from 0.00005\n",
            "Epoch 1911/2000\n",
            " - 2s - loss: 7.6971e-05 - accuracy: 1.0000 - val_loss: 0.0226 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01911: loss did not improve from 0.00005\n",
            "Epoch 1912/2000\n",
            " - 2s - loss: 9.8358e-05 - accuracy: 1.0000 - val_loss: 0.0226 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01912: loss did not improve from 0.00005\n",
            "Epoch 1913/2000\n",
            " - 2s - loss: 8.4897e-05 - accuracy: 1.0000 - val_loss: 0.0226 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01913: loss did not improve from 0.00005\n",
            "Epoch 1914/2000\n",
            " - 2s - loss: 6.5877e-05 - accuracy: 1.0000 - val_loss: 0.0226 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01914: loss did not improve from 0.00005\n",
            "Epoch 1915/2000\n",
            " - 2s - loss: 7.9455e-05 - accuracy: 1.0000 - val_loss: 0.0226 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01915: loss did not improve from 0.00005\n",
            "Epoch 1916/2000\n",
            " - 2s - loss: 8.9599e-05 - accuracy: 1.0000 - val_loss: 0.0226 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01916: loss did not improve from 0.00005\n",
            "Epoch 1917/2000\n",
            " - 2s - loss: 7.1247e-05 - accuracy: 1.0000 - val_loss: 0.0226 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01917: loss did not improve from 0.00005\n",
            "Epoch 1918/2000\n",
            " - 2s - loss: 7.3434e-05 - accuracy: 1.0000 - val_loss: 0.0226 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01918: loss did not improve from 0.00005\n",
            "Epoch 1919/2000\n",
            " - 2s - loss: 8.9692e-05 - accuracy: 1.0000 - val_loss: 0.0226 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01919: loss did not improve from 0.00005\n",
            "Epoch 1920/2000\n",
            " - 2s - loss: 4.7495e-05 - accuracy: 1.0000 - val_loss: 0.0226 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01920: loss improved from 0.00005 to 0.00005, saving model to ./weights/lstmfcn_128_cells_weights/CBF_weights.h5\n",
            "Epoch 1921/2000\n",
            " - 2s - loss: 7.0639e-05 - accuracy: 1.0000 - val_loss: 0.0226 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01921: loss did not improve from 0.00005\n",
            "Epoch 1922/2000\n",
            " - 2s - loss: 8.4500e-05 - accuracy: 1.0000 - val_loss: 0.0226 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01922: loss did not improve from 0.00005\n",
            "Epoch 1923/2000\n",
            " - 2s - loss: 8.9130e-05 - accuracy: 1.0000 - val_loss: 0.0226 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01923: loss did not improve from 0.00005\n",
            "Epoch 1924/2000\n",
            " - 2s - loss: 8.1005e-05 - accuracy: 1.0000 - val_loss: 0.0225 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01924: loss did not improve from 0.00005\n",
            "Epoch 1925/2000\n",
            " - 2s - loss: 8.6022e-05 - accuracy: 1.0000 - val_loss: 0.0225 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01925: loss did not improve from 0.00005\n",
            "Epoch 1926/2000\n",
            " - 2s - loss: 6.1894e-05 - accuracy: 1.0000 - val_loss: 0.0225 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01926: loss did not improve from 0.00005\n",
            "Epoch 1927/2000\n",
            " - 2s - loss: 6.4945e-05 - accuracy: 1.0000 - val_loss: 0.0225 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01927: loss did not improve from 0.00005\n",
            "Epoch 1928/2000\n",
            " - 2s - loss: 8.6714e-05 - accuracy: 1.0000 - val_loss: 0.0225 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01928: loss did not improve from 0.00005\n",
            "Epoch 1929/2000\n",
            " - 2s - loss: 7.1027e-05 - accuracy: 1.0000 - val_loss: 0.0225 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01929: loss did not improve from 0.00005\n",
            "Epoch 1930/2000\n",
            " - 2s - loss: 1.1688e-04 - accuracy: 1.0000 - val_loss: 0.0225 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01930: loss did not improve from 0.00005\n",
            "Epoch 1931/2000\n",
            " - 2s - loss: 8.5204e-05 - accuracy: 1.0000 - val_loss: 0.0225 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01931: loss did not improve from 0.00005\n",
            "Epoch 1932/2000\n",
            " - 2s - loss: 5.3308e-05 - accuracy: 1.0000 - val_loss: 0.0225 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01932: loss did not improve from 0.00005\n",
            "Epoch 1933/2000\n",
            " - 2s - loss: 8.4120e-05 - accuracy: 1.0000 - val_loss: 0.0225 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01933: loss did not improve from 0.00005\n",
            "Epoch 1934/2000\n",
            " - 2s - loss: 9.3379e-05 - accuracy: 1.0000 - val_loss: 0.0226 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01934: loss did not improve from 0.00005\n",
            "Epoch 1935/2000\n",
            " - 2s - loss: 4.5159e-05 - accuracy: 1.0000 - val_loss: 0.0226 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01935: loss improved from 0.00005 to 0.00005, saving model to ./weights/lstmfcn_128_cells_weights/CBF_weights.h5\n",
            "Epoch 1936/2000\n",
            " - 2s - loss: 9.5003e-05 - accuracy: 1.0000 - val_loss: 0.0226 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01936: loss did not improve from 0.00005\n",
            "Epoch 1937/2000\n",
            " - 2s - loss: 6.7317e-05 - accuracy: 1.0000 - val_loss: 0.0226 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01937: loss did not improve from 0.00005\n",
            "Epoch 1938/2000\n",
            " - 2s - loss: 6.6698e-05 - accuracy: 1.0000 - val_loss: 0.0227 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01938: loss did not improve from 0.00005\n",
            "Epoch 1939/2000\n",
            " - 2s - loss: 8.5731e-05 - accuracy: 1.0000 - val_loss: 0.0227 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01939: loss did not improve from 0.00005\n",
            "Epoch 1940/2000\n",
            " - 2s - loss: 7.0653e-05 - accuracy: 1.0000 - val_loss: 0.0227 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01940: loss did not improve from 0.00005\n",
            "Epoch 1941/2000\n",
            " - 2s - loss: 6.8032e-05 - accuracy: 1.0000 - val_loss: 0.0227 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01941: loss did not improve from 0.00005\n",
            "Epoch 1942/2000\n",
            " - 2s - loss: 9.8909e-05 - accuracy: 1.0000 - val_loss: 0.0227 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01942: loss did not improve from 0.00005\n",
            "Epoch 1943/2000\n",
            " - 2s - loss: 1.0460e-04 - accuracy: 1.0000 - val_loss: 0.0227 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01943: loss did not improve from 0.00005\n",
            "Epoch 1944/2000\n",
            " - 2s - loss: 8.6587e-05 - accuracy: 1.0000 - val_loss: 0.0227 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01944: loss did not improve from 0.00005\n",
            "Epoch 1945/2000\n",
            " - 2s - loss: 1.1791e-04 - accuracy: 1.0000 - val_loss: 0.0227 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01945: loss did not improve from 0.00005\n",
            "Epoch 1946/2000\n",
            " - 2s - loss: 9.7282e-05 - accuracy: 1.0000 - val_loss: 0.0227 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01946: loss did not improve from 0.00005\n",
            "Epoch 1947/2000\n",
            " - 2s - loss: 7.6769e-05 - accuracy: 1.0000 - val_loss: 0.0227 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01947: loss did not improve from 0.00005\n",
            "Epoch 1948/2000\n",
            " - 2s - loss: 8.8632e-05 - accuracy: 1.0000 - val_loss: 0.0227 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01948: loss did not improve from 0.00005\n",
            "Epoch 1949/2000\n",
            " - 2s - loss: 7.8451e-05 - accuracy: 1.0000 - val_loss: 0.0226 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01949: loss did not improve from 0.00005\n",
            "Epoch 1950/2000\n",
            " - 2s - loss: 7.7928e-05 - accuracy: 1.0000 - val_loss: 0.0226 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01950: loss did not improve from 0.00005\n",
            "Epoch 1951/2000\n",
            " - 2s - loss: 9.0606e-05 - accuracy: 1.0000 - val_loss: 0.0226 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01951: loss did not improve from 0.00005\n",
            "Epoch 1952/2000\n",
            " - 2s - loss: 8.4683e-05 - accuracy: 1.0000 - val_loss: 0.0226 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01952: loss did not improve from 0.00005\n",
            "Epoch 1953/2000\n",
            " - 2s - loss: 6.9554e-05 - accuracy: 1.0000 - val_loss: 0.0226 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01953: loss did not improve from 0.00005\n",
            "Epoch 1954/2000\n",
            " - 2s - loss: 7.6709e-05 - accuracy: 1.0000 - val_loss: 0.0226 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01954: loss did not improve from 0.00005\n",
            "Epoch 1955/2000\n",
            " - 2s - loss: 8.3181e-05 - accuracy: 1.0000 - val_loss: 0.0225 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01955: loss did not improve from 0.00005\n",
            "Epoch 1956/2000\n",
            " - 2s - loss: 6.2378e-05 - accuracy: 1.0000 - val_loss: 0.0225 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01956: loss did not improve from 0.00005\n",
            "Epoch 1957/2000\n",
            " - 2s - loss: 7.0976e-05 - accuracy: 1.0000 - val_loss: 0.0225 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01957: loss did not improve from 0.00005\n",
            "Epoch 1958/2000\n",
            " - 2s - loss: 8.8056e-05 - accuracy: 1.0000 - val_loss: 0.0225 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01958: loss did not improve from 0.00005\n",
            "Epoch 1959/2000\n",
            " - 2s - loss: 7.6679e-05 - accuracy: 1.0000 - val_loss: 0.0225 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01959: loss did not improve from 0.00005\n",
            "Epoch 1960/2000\n",
            " - 2s - loss: 8.3580e-05 - accuracy: 1.0000 - val_loss: 0.0225 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01960: loss did not improve from 0.00005\n",
            "Epoch 1961/2000\n",
            " - 2s - loss: 1.0369e-04 - accuracy: 1.0000 - val_loss: 0.0225 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01961: loss did not improve from 0.00005\n",
            "Epoch 1962/2000\n",
            " - 2s - loss: 8.1673e-05 - accuracy: 1.0000 - val_loss: 0.0226 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01962: loss did not improve from 0.00005\n",
            "Epoch 1963/2000\n",
            " - 2s - loss: 6.6511e-05 - accuracy: 1.0000 - val_loss: 0.0226 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01963: loss did not improve from 0.00005\n",
            "Epoch 1964/2000\n",
            " - 2s - loss: 8.0062e-05 - accuracy: 1.0000 - val_loss: 0.0226 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01964: loss did not improve from 0.00005\n",
            "Epoch 1965/2000\n",
            " - 2s - loss: 8.8148e-05 - accuracy: 1.0000 - val_loss: 0.0226 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01965: loss did not improve from 0.00005\n",
            "Epoch 1966/2000\n",
            " - 2s - loss: 5.2215e-05 - accuracy: 1.0000 - val_loss: 0.0226 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01966: loss did not improve from 0.00005\n",
            "Epoch 1967/2000\n",
            " - 2s - loss: 9.3788e-05 - accuracy: 1.0000 - val_loss: 0.0227 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01967: loss did not improve from 0.00005\n",
            "Epoch 1968/2000\n",
            " - 2s - loss: 5.6809e-05 - accuracy: 1.0000 - val_loss: 0.0227 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01968: loss did not improve from 0.00005\n",
            "Epoch 1969/2000\n",
            " - 2s - loss: 9.9146e-05 - accuracy: 1.0000 - val_loss: 0.0227 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01969: loss did not improve from 0.00005\n",
            "Epoch 1970/2000\n",
            " - 2s - loss: 1.2603e-04 - accuracy: 1.0000 - val_loss: 0.0227 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01970: loss did not improve from 0.00005\n",
            "Epoch 1971/2000\n",
            " - 2s - loss: 7.0555e-05 - accuracy: 1.0000 - val_loss: 0.0227 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01971: loss did not improve from 0.00005\n",
            "Epoch 1972/2000\n",
            " - 2s - loss: 5.6368e-05 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01972: loss did not improve from 0.00005\n",
            "Epoch 1973/2000\n",
            " - 2s - loss: 8.0961e-05 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01973: loss did not improve from 0.00005\n",
            "Epoch 1974/2000\n",
            " - 2s - loss: 9.5878e-05 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01974: loss did not improve from 0.00005\n",
            "Epoch 1975/2000\n",
            " - 2s - loss: 5.3916e-05 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01975: loss did not improve from 0.00005\n",
            "Epoch 1976/2000\n",
            " - 2s - loss: 6.2323e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01976: loss did not improve from 0.00005\n",
            "Epoch 1977/2000\n",
            " - 2s - loss: 8.0931e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01977: loss did not improve from 0.00005\n",
            "Epoch 1978/2000\n",
            " - 2s - loss: 8.3041e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01978: loss did not improve from 0.00005\n",
            "Epoch 1979/2000\n",
            " - 2s - loss: 9.6969e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01979: loss did not improve from 0.00005\n",
            "Epoch 1980/2000\n",
            " - 2s - loss: 8.4910e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01980: loss did not improve from 0.00005\n",
            "Epoch 1981/2000\n",
            " - 2s - loss: 6.4079e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01981: loss did not improve from 0.00005\n",
            "Epoch 1982/2000\n",
            " - 2s - loss: 5.8122e-05 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01982: loss did not improve from 0.00005\n",
            "Epoch 1983/2000\n",
            " - 2s - loss: 9.4699e-05 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01983: loss did not improve from 0.00005\n",
            "Epoch 1984/2000\n",
            " - 2s - loss: 7.1038e-05 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01984: loss did not improve from 0.00005\n",
            "Epoch 1985/2000\n",
            " - 2s - loss: 7.6402e-05 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01985: loss did not improve from 0.00005\n",
            "Epoch 1986/2000\n",
            " - 2s - loss: 6.9116e-05 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01986: loss did not improve from 0.00005\n",
            "Epoch 1987/2000\n",
            " - 2s - loss: 6.3170e-05 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01987: loss did not improve from 0.00005\n",
            "Epoch 1988/2000\n",
            " - 2s - loss: 8.6960e-05 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01988: loss did not improve from 0.00005\n",
            "Epoch 1989/2000\n",
            " - 2s - loss: 9.1629e-05 - accuracy: 1.0000 - val_loss: 0.0227 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01989: loss did not improve from 0.00005\n",
            "Epoch 1990/2000\n",
            " - 2s - loss: 5.3265e-05 - accuracy: 1.0000 - val_loss: 0.0227 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01990: loss did not improve from 0.00005\n",
            "Epoch 1991/2000\n",
            " - 2s - loss: 9.1997e-05 - accuracy: 1.0000 - val_loss: 0.0227 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01991: loss did not improve from 0.00005\n",
            "Epoch 1992/2000\n",
            " - 2s - loss: 6.7115e-05 - accuracy: 1.0000 - val_loss: 0.0227 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01992: loss did not improve from 0.00005\n",
            "Epoch 1993/2000\n",
            " - 2s - loss: 7.3294e-05 - accuracy: 1.0000 - val_loss: 0.0227 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01993: loss did not improve from 0.00005\n",
            "Epoch 1994/2000\n",
            " - 2s - loss: 8.8068e-05 - accuracy: 1.0000 - val_loss: 0.0227 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01994: loss did not improve from 0.00005\n",
            "Epoch 1995/2000\n",
            " - 2s - loss: 5.1408e-05 - accuracy: 1.0000 - val_loss: 0.0227 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01995: loss did not improve from 0.00005\n",
            "Epoch 1996/2000\n",
            " - 2s - loss: 7.8716e-05 - accuracy: 1.0000 - val_loss: 0.0227 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01996: loss did not improve from 0.00005\n",
            "Epoch 1997/2000\n",
            " - 2s - loss: 7.5195e-05 - accuracy: 1.0000 - val_loss: 0.0227 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01997: loss did not improve from 0.00005\n",
            "Epoch 1998/2000\n",
            " - 2s - loss: 8.1983e-05 - accuracy: 1.0000 - val_loss: 0.0227 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01998: loss did not improve from 0.00005\n",
            "Epoch 1999/2000\n",
            " - 2s - loss: 6.0186e-05 - accuracy: 1.0000 - val_loss: 0.0227 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 01999: loss did not improve from 0.00005\n",
            "Epoch 2000/2000\n",
            " - 2s - loss: 6.7484e-05 - accuracy: 1.0000 - val_loss: 0.0227 - val_accuracy: 0.9944\n",
            "\n",
            "Epoch 02000: loss did not improve from 0.00005\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/CBF_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/CBF_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  30 Number of test samples :  900\n",
            "Number of classes :  3\n",
            "Sequence length :  128\n",
            "Weights loaded from  ./weights/lstmfcn_128_cells_weights/CBF_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "900/900 [==============================] - 2s 2ms/step\n",
            "\n",
            "Final Accuracy :  0.9944444298744202\n",
            "\n",
            "\n",
            "\n",
            "******************** Successes ********************\n",
            "\n",
            "39,CBF,lstmfcn_128_cells_weights/CBF,0.994444\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "******************** Failures ********************\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"gdrive/My Drive/Colab Notebooks/LSTM-FCN/all_datasets_training.py\", line 135, in <module>\n",
            "    model = model_fn(MAX_SEQUENCE_LENGTH, NB_CLASS, cell)\n",
            "  File \"gdrive/My Drive/Colab Notebooks/LSTM-FCN/all_datasets_training.py\", line 52, in generate_alstmfcn\n",
            "    x = AttentionLSTM(NUM_CELLS)(ip)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/layers/recurrent.py\", line 541, in __call__\n",
            "    return super(RNN, self).__call__(inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\", line 75, in symbolic_fn_wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py\", line 463, in __call__\n",
            "    self.build(unpack_singleton(input_shapes))\n",
            "  File \"/content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/utils/layer_utils.py\", line 601, in build\n",
            "    self.cell.build(input_shape)\n",
            "  File \"/content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/utils/layer_utils.py\", line 254, in build\n",
            "    constraint=self.bias_constraint)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py\", line 282, in add_weight\n",
            "    constraint=constraint)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\", line 620, in variable\n",
            "    value, dtype=dtype, name=name, constraint=constraint)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\", line 845, in variable\n",
            "    constraint=constraint)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\", line 261, in __call__\n",
            "    return cls._variable_v2_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\", line 255, in _variable_v2_call\n",
            "    shape=shape)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\", line 236, in <lambda>\n",
            "    previous_getter = lambda **kws: default_variable_creator_v2(None, **kws)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\", line 2647, in default_variable_creator_v2\n",
            "    shape=shape)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\", line 263, in __call__\n",
            "    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py\", line 1434, in __init__\n",
            "    distribute_strategy=distribute_strategy)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py\", line 1517, in _init_from_args\n",
            "    raise ValueError(\"Tensor-typed variable initializers must either be \"\n",
            "ValueError: Tensor-typed variable initializers must either be wrapped in an init_scope or callable (e.g., `tf.Variable(lambda : tf.truncated_normal([10, 40]))`) when building functions. Please file a feature request if this restriction inconveniences you.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beQiyAhoT2xW",
        "colab_type": "text"
      },
      "source": [
        "All datasets with 4 epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phw5Ii3niOmf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b679f67d-3a9f-44d2-e2bc-b33f5ec689fa"
      },
      "source": [
        "!python gdrive/My\\ Drive/Colab\\ Notebooks/LSTM-FCN/all_datasets_training.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "2020-07-09 15:27:51.260981: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "Num datasets :  128\n",
            "\n",
            "2020-07-09 15:27:53.785764: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2020-07-09 15:27:53.789468: E tensorflow/stream_executor/cuda/cuda_driver.cc:313] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2020-07-09 15:27:53.789520: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (2e45165a50b9): /proc/driver/nvidia/version does not exist\n",
            "2020-07-09 15:27:53.796558: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2300000000 Hz\n",
            "2020-07-09 15:27:53.796841: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1b91100 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2020-07-09 15:27:53.796887: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 176)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 176, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 176, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 176, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 176, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 176, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 176, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 176, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 176, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 176, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            5920        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 176, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 37)           5069        concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 276,717\n",
            "Trainable params: 275,693\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset Adiac ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Adiac_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Adiac_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  390 Number of test samples :  391\n",
            "Number of classes :  37\n",
            "Sequence length :  176\n",
            "Class weights :  [0.81081081 1.05405405 2.10810811 0.75289575 2.63513514 0.87837838\n",
            " 1.05405405 0.81081081 1.17117117 0.95823096 1.05405405 0.87837838\n",
            " 1.75675676 0.95823096 0.87837838 1.05405405 0.95823096 1.17117117\n",
            " 0.75289575 1.17117117 1.31756757 1.05405405 1.17117117 0.7027027\n",
            " 0.81081081 1.17117117 0.81081081 0.81081081 1.17117117 0.81081081\n",
            " 1.17117117 0.81081081 0.81081081 1.17117117 1.05405405 0.95823096\n",
            " 1.31756757]\n",
            "Train on 390 samples, validate on 391 samples\n",
            "Epoch 1/4\n",
            " - 5s - loss: 3.6956 - accuracy: 0.0333 - val_loss: 3.9475 - val_accuracy: 0.0332\n",
            "\n",
            "Epoch 00001: loss improved from inf to 3.69562, saving model to ./weights/lstmfcn_8_cells_weights/Adiac_weights.h5\n",
            "Epoch 2/4\n",
            " - 4s - loss: 3.5728 - accuracy: 0.0359 - val_loss: 4.0571 - val_accuracy: 0.0358\n",
            "\n",
            "Epoch 00002: loss improved from 3.69562 to 3.57279, saving model to ./weights/lstmfcn_8_cells_weights/Adiac_weights.h5\n",
            "Epoch 3/4\n",
            " - 4s - loss: 3.5234 - accuracy: 0.0641 - val_loss: 4.1530 - val_accuracy: 0.0307\n",
            "\n",
            "Epoch 00003: loss improved from 3.57279 to 3.52340, saving model to ./weights/lstmfcn_8_cells_weights/Adiac_weights.h5\n",
            "Epoch 4/4\n",
            " - 4s - loss: 3.4807 - accuracy: 0.0821 - val_loss: 4.1807 - val_accuracy: 0.0205\n",
            "\n",
            "Epoch 00004: loss improved from 3.52340 to 3.48067, saving model to ./weights/lstmfcn_8_cells_weights/Adiac_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Adiac_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Adiac_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  390 Number of test samples :  391\n",
            "Number of classes :  37\n",
            "Sequence length :  176\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/Adiac_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "391/391 [==============================] - 1s 2ms/step\n",
            "\n",
            "Final Accuracy :  0.020460357889533043\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 251)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 251, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 251, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 251, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 251, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 251, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 251, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 251, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 251, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 251, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            8320        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 251, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 3)            411         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 274,459\n",
            "Trainable params: 273,435\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset ArrowHead ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ArrowHead_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ArrowHead_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  36 Number of test samples :  175\n",
            "Number of classes :  3\n",
            "Sequence length :  251\n",
            "Class weights :  [1. 1. 1.]\n",
            "Train on 36 samples, validate on 175 samples\n",
            "Epoch 1/4\n",
            " - 2s - loss: 1.0956 - accuracy: 0.4444 - val_loss: 1.1713 - val_accuracy: 0.3486\n",
            "\n",
            "Epoch 00001: loss improved from inf to 1.09561, saving model to ./weights/lstmfcn_8_cells_weights/ArrowHead_weights.h5\n",
            "Epoch 2/4\n",
            " - 1s - loss: 0.9119 - accuracy: 0.5556 - val_loss: 1.1105 - val_accuracy: 0.3943\n",
            "\n",
            "Epoch 00002: loss improved from 1.09561 to 0.91188, saving model to ./weights/lstmfcn_8_cells_weights/ArrowHead_weights.h5\n",
            "Epoch 3/4\n",
            " - 1s - loss: 0.8577 - accuracy: 0.6389 - val_loss: 1.0962 - val_accuracy: 0.3943\n",
            "\n",
            "Epoch 00003: loss improved from 0.91188 to 0.85767, saving model to ./weights/lstmfcn_8_cells_weights/ArrowHead_weights.h5\n",
            "Epoch 4/4\n",
            " - 1s - loss: 0.8279 - accuracy: 0.6667 - val_loss: 1.1159 - val_accuracy: 0.3943\n",
            "\n",
            "Epoch 00004: loss improved from 0.85767 to 0.82789, saving model to ./weights/lstmfcn_8_cells_weights/ArrowHead_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ArrowHead_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ArrowHead_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  36 Number of test samples :  175\n",
            "Number of classes :  3\n",
            "Sequence length :  251\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/ArrowHead_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "175/175 [==============================] - 1s 4ms/step\n",
            "\n",
            "Final Accuracy :  0.3942857086658478\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 166)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 166, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 166, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 166, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 166, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 166, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 166, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 166, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 166, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 166, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            5600        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 166, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 3)            411         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 271,739\n",
            "Trainable params: 270,715\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset ChlorineConcentration ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ChlorineConcentration_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ChlorineConcentration_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  467 Number of test samples :  3840\n",
            "Number of classes :  3\n",
            "Sequence length :  166\n",
            "Class weights :  [1.36549708 1.71062271 0.59414758]\n",
            "Train on 467 samples, validate on 3840 samples\n",
            "Epoch 1/4\n",
            " - 12s - loss: 1.2444 - accuracy: 0.2227 - val_loss: 2.6706 - val_accuracy: 0.2367\n",
            "\n",
            "Epoch 00001: loss improved from inf to 1.24443, saving model to ./weights/lstmfcn_8_cells_weights/ChlorineConcentration_weights.h5\n",
            "Epoch 2/4\n",
            " - 11s - loss: 0.9917 - accuracy: 0.5439 - val_loss: 3.5181 - val_accuracy: 0.2367\n",
            "\n",
            "Epoch 00002: loss improved from 1.24443 to 0.99170, saving model to ./weights/lstmfcn_8_cells_weights/ChlorineConcentration_weights.h5\n",
            "Epoch 3/4\n",
            " - 11s - loss: 0.9817 - accuracy: 0.5632 - val_loss: 3.8559 - val_accuracy: 0.2370\n",
            "\n",
            "Epoch 00003: loss improved from 0.99170 to 0.98174, saving model to ./weights/lstmfcn_8_cells_weights/ChlorineConcentration_weights.h5\n",
            "Epoch 4/4\n",
            " - 11s - loss: 0.9622 - accuracy: 0.5760 - val_loss: 4.0233 - val_accuracy: 0.2378\n",
            "\n",
            "Epoch 00004: loss improved from 0.98174 to 0.96222, saving model to ./weights/lstmfcn_8_cells_weights/ChlorineConcentration_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ChlorineConcentration_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ChlorineConcentration_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  467 Number of test samples :  3840\n",
            "Number of classes :  3\n",
            "Sequence length :  166\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/ChlorineConcentration_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "3840/3840 [==============================] - 8s 2ms/step\n",
            "\n",
            "Final Accuracy :  0.23776040971279144\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 256)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 256, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 256, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 256, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 256, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 256, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 256, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 256, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 256, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 256, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            8480        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 256, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 11)           1507        concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 275,715\n",
            "Trainable params: 274,691\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset InsectWingbeatSound ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/InsectWingbeatSound_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/InsectWingbeatSound_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  220 Number of test samples :  1980\n",
            "Number of classes :  11\n",
            "Sequence length :  256\n",
            "Class weights :  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "Train on 220 samples, validate on 1980 samples\n",
            "Epoch 1/4\n",
            " - 9s - loss: 2.3955 - accuracy: 0.1227 - val_loss: 2.5079 - val_accuracy: 0.0909\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.39552, saving model to ./weights/lstmfcn_8_cells_weights/InsectWingbeatSound_weights.h5\n",
            "Epoch 2/4\n",
            " - 8s - loss: 2.2792 - accuracy: 0.2136 - val_loss: 2.5675 - val_accuracy: 0.0909\n",
            "\n",
            "Epoch 00002: loss improved from 2.39552 to 2.27921, saving model to ./weights/lstmfcn_8_cells_weights/InsectWingbeatSound_weights.h5\n",
            "Epoch 3/4\n",
            " - 8s - loss: 2.2184 - accuracy: 0.2182 - val_loss: 2.7072 - val_accuracy: 0.0909\n",
            "\n",
            "Epoch 00003: loss improved from 2.27921 to 2.21840, saving model to ./weights/lstmfcn_8_cells_weights/InsectWingbeatSound_weights.h5\n",
            "Epoch 4/4\n",
            " - 8s - loss: 2.1525 - accuracy: 0.2818 - val_loss: 2.9169 - val_accuracy: 0.0909\n",
            "\n",
            "Epoch 00004: loss improved from 2.21840 to 2.15250, saving model to ./weights/lstmfcn_8_cells_weights/InsectWingbeatSound_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/InsectWingbeatSound_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/InsectWingbeatSound_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  220 Number of test samples :  1980\n",
            "Number of classes :  11\n",
            "Sequence length :  256\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/InsectWingbeatSound_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "1980/1980 [==============================] - 6s 3ms/step\n",
            "\n",
            "Final Accuracy :  0.09090909361839294\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 319)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 319, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 319, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 319, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 319, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 319, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 319, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 319, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 319, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 319, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            10496       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 319, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 7)            959         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 277,183\n",
            "Trainable params: 276,159\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset Lighting7 ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Lighting7_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Lighting7_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  70 Number of test samples :  73\n",
            "Number of classes :  7\n",
            "Sequence length :  319\n",
            "Class weights :  [1.25       1.25       1.25       0.83333333 2.         0.52631579\n",
            " 1.        ]\n",
            "Train on 70 samples, validate on 73 samples\n",
            "Epoch 1/4\n",
            " - 2s - loss: 1.9846 - accuracy: 0.2571 - val_loss: 1.8959 - val_accuracy: 0.3014\n",
            "\n",
            "Epoch 00001: loss improved from inf to 1.98462, saving model to ./weights/lstmfcn_8_cells_weights/Lighting7_weights.h5\n",
            "Epoch 2/4\n",
            " - 1s - loss: 1.6776 - accuracy: 0.4143 - val_loss: 1.8210 - val_accuracy: 0.3014\n",
            "\n",
            "Epoch 00002: loss improved from 1.98462 to 1.67757, saving model to ./weights/lstmfcn_8_cells_weights/Lighting7_weights.h5\n",
            "Epoch 3/4\n",
            " - 1s - loss: 1.5271 - accuracy: 0.4571 - val_loss: 1.7990 - val_accuracy: 0.2877\n",
            "\n",
            "Epoch 00003: loss improved from 1.67757 to 1.52708, saving model to ./weights/lstmfcn_8_cells_weights/Lighting7_weights.h5\n",
            "Epoch 4/4\n",
            " - 1s - loss: 1.4122 - accuracy: 0.5000 - val_loss: 1.8030 - val_accuracy: 0.2877\n",
            "\n",
            "Epoch 00004: loss improved from 1.52708 to 1.41225, saving model to ./weights/lstmfcn_8_cells_weights/Lighting7_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Lighting7_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Lighting7_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  70 Number of test samples :  73\n",
            "Number of classes :  7\n",
            "Sequence length :  319\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/Lighting7_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "73/73 [==============================] - 0s 5ms/step\n",
            "\n",
            "Final Accuracy :  0.2876712381839752\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 234)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 234, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 234, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 234, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 234, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 234, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 234, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 234, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 234, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 234, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            7776        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 234, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 2)            274         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 273,778\n",
            "Trainable params: 272,754\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset Wine ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Wine_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Wine_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  57 Number of test samples :  54\n",
            "Number of classes :  2\n",
            "Sequence length :  234\n",
            "Class weights :  [0.95       1.05555556]\n",
            "Train on 57 samples, validate on 54 samples\n",
            "Epoch 1/4\n",
            " - 2s - loss: 0.7627 - accuracy: 0.4737 - val_loss: 0.9307 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.76270, saving model to ./weights/lstmfcn_8_cells_weights/Wine_weights.h5\n",
            "Epoch 2/4\n",
            " - 1s - loss: 0.7045 - accuracy: 0.4912 - val_loss: 0.9992 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00002: loss improved from 0.76270 to 0.70454, saving model to ./weights/lstmfcn_8_cells_weights/Wine_weights.h5\n",
            "Epoch 3/4\n",
            " - 1s - loss: 0.7084 - accuracy: 0.4561 - val_loss: 0.9788 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00003: loss did not improve from 0.70454\n",
            "Epoch 4/4\n",
            " - 1s - loss: 0.6726 - accuracy: 0.6491 - val_loss: 0.9626 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00004: loss improved from 0.70454 to 0.67256, saving model to ./weights/lstmfcn_8_cells_weights/Wine_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Wine_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Wine_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  57 Number of test samples :  54\n",
            "Number of classes :  2\n",
            "Sequence length :  234\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/Wine_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "54/54 [==============================] - 0s 5ms/step\n",
            "\n",
            "Final Accuracy :  0.5\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 270)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 270, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 270, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 270, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 270, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 270, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 270, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 270, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 270, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 270, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            8928        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 270, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 25)           3425        concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 278,081\n",
            "Trainable params: 277,057\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset WordsSynonyms ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/WordsSynonyms_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/WordsSynonyms_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  267 Number of test samples :  638\n",
            "Number of classes :  25\n",
            "Sequence length :  270\n",
            "Class weights :  [1.78       0.178      2.136      0.30514286 2.136      0.62823529\n",
            " 2.67       0.445      1.78       0.89       1.78       1.335\n",
            " 3.56       1.068      2.67       0.76285714 2.136      2.136\n",
            " 2.67       1.52571429 2.67       1.335      1.78       1.52571429\n",
            " 5.34      ]\n",
            "Train on 267 samples, validate on 638 samples\n",
            "Epoch 1/4\n",
            " - 6s - loss: 3.2123 - accuracy: 0.0599 - val_loss: 3.1845 - val_accuracy: 0.0502\n",
            "\n",
            "Epoch 00001: loss improved from inf to 3.21234, saving model to ./weights/lstmfcn_8_cells_weights/WordsSynonyms_weights.h5\n",
            "Epoch 2/4\n",
            " - 5s - loss: 2.8710 - accuracy: 0.2247 - val_loss: 3.2234 - val_accuracy: 0.0502\n",
            "\n",
            "Epoch 00002: loss improved from 3.21234 to 2.87103, saving model to ./weights/lstmfcn_8_cells_weights/WordsSynonyms_weights.h5\n",
            "Epoch 3/4\n",
            " - 5s - loss: 2.7424 - accuracy: 0.2697 - val_loss: 3.3185 - val_accuracy: 0.0533\n",
            "\n",
            "Epoch 00003: loss improved from 2.87103 to 2.74241, saving model to ./weights/lstmfcn_8_cells_weights/WordsSynonyms_weights.h5\n",
            "Epoch 4/4\n",
            " - 5s - loss: 2.6475 - accuracy: 0.2884 - val_loss: 3.3689 - val_accuracy: 0.1599\n",
            "\n",
            "Epoch 00004: loss improved from 2.74241 to 2.64755, saving model to ./weights/lstmfcn_8_cells_weights/WordsSynonyms_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/WordsSynonyms_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/WordsSynonyms_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  267 Number of test samples :  638\n",
            "Number of classes :  25\n",
            "Sequence length :  270\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/WordsSynonyms_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "638/638 [==============================] - 2s 3ms/step\n",
            "\n",
            "Final Accuracy :  0.15987460315227509\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 270)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 270, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 270, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 270, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 270, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 270, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 270, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 270, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 270, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 270, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            8928        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 270, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 50)           6850        concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 281,506\n",
            "Trainable params: 280,482\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset 50words ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/50words_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/50words_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  450 Number of test samples :  455\n",
            "Number of classes :  50\n",
            "Sequence length :  270\n",
            "Class weights :  [0.17307692 0.18367347 0.27272727 0.45       0.69230769 0.6\n",
            " 0.5625     0.81818182 0.81818182 0.9        0.9        0.69230769\n",
            " 0.64285714 1.125      1.28571429 1.5        1.28571429 1.125\n",
            " 1.         1.28571429 1.28571429 1.8        1.5        2.25\n",
            " 4.5        1.5        2.25       1.5        2.25       1.5\n",
            " 3.         1.28571429 1.8        3.         1.5        1.5\n",
            " 1.8        2.25       2.25       2.25       9.         4.5\n",
            " 2.25       1.5        2.25       2.25       2.25       1.8\n",
            " 4.5        4.5       ]\n",
            "Train on 450 samples, validate on 455 samples\n",
            "Epoch 1/4\n",
            " - 7s - loss: 3.7872 - accuracy: 0.0978 - val_loss: 4.0521 - val_accuracy: 0.0176\n",
            "\n",
            "Epoch 00001: loss improved from inf to 3.78719, saving model to ./weights/lstmfcn_8_cells_weights/50words_weights.h5\n",
            "Epoch 2/4\n",
            " - 6s - loss: 3.4128 - accuracy: 0.2578 - val_loss: 3.7565 - val_accuracy: 0.0571\n",
            "\n",
            "Epoch 00002: loss improved from 3.78719 to 3.41278, saving model to ./weights/lstmfcn_8_cells_weights/50words_weights.h5\n",
            "Epoch 3/4\n",
            " - 6s - loss: 3.2144 - accuracy: 0.2778 - val_loss: 3.5695 - val_accuracy: 0.1011\n",
            "\n",
            "Epoch 00003: loss improved from 3.41278 to 3.21440, saving model to ./weights/lstmfcn_8_cells_weights/50words_weights.h5\n",
            "Epoch 4/4\n",
            " - 6s - loss: 3.0845 - accuracy: 0.2867 - val_loss: 3.5210 - val_accuracy: 0.0791\n",
            "\n",
            "Epoch 00004: loss improved from 3.21440 to 3.08447, saving model to ./weights/lstmfcn_8_cells_weights/50words_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/50words_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/50words_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  450 Number of test samples :  455\n",
            "Number of classes :  50\n",
            "Sequence length :  270\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/50words_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "455/455 [==============================] - 2s 3ms/step\n",
            "\n",
            "Final Accuracy :  0.07912088185548782\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 470)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 470, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 470, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 470, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 470, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 470, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 470, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 470, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 470, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 470, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            15328       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 470, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 5)            685         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 281,741\n",
            "Trainable params: 280,717\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset Beef ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Beef_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Beef_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  30 Number of test samples :  30\n",
            "Number of classes :  5\n",
            "Sequence length :  470\n",
            "Class weights :  [1. 1. 1. 1. 1.]\n",
            "Train on 30 samples, validate on 30 samples\n",
            "Epoch 1/4\n",
            " - 2s - loss: 1.6816 - accuracy: 0.1333 - val_loss: 1.7463 - val_accuracy: 0.2000\n",
            "\n",
            "Epoch 00001: loss improved from inf to 1.68161, saving model to ./weights/lstmfcn_8_cells_weights/Beef_weights.h5\n",
            "Epoch 2/4\n",
            " - 1s - loss: 1.5574 - accuracy: 0.3000 - val_loss: 1.7465 - val_accuracy: 0.2333\n",
            "\n",
            "Epoch 00002: loss improved from 1.68161 to 1.55735, saving model to ./weights/lstmfcn_8_cells_weights/Beef_weights.h5\n",
            "Epoch 3/4\n",
            " - 1s - loss: 1.4623 - accuracy: 0.3667 - val_loss: 1.7945 - val_accuracy: 0.2000\n",
            "\n",
            "Epoch 00003: loss improved from 1.55735 to 1.46228, saving model to ./weights/lstmfcn_8_cells_weights/Beef_weights.h5\n",
            "Epoch 4/4\n",
            " - 1s - loss: 1.4381 - accuracy: 0.4000 - val_loss: 1.8861 - val_accuracy: 0.2000\n",
            "\n",
            "Epoch 00004: loss improved from 1.46228 to 1.43807, saving model to ./weights/lstmfcn_8_cells_weights/Beef_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Beef_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Beef_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  30 Number of test samples :  30\n",
            "Number of classes :  5\n",
            "Sequence length :  470\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/Beef_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "30/30 [==============================] - 0s 10ms/step\n",
            "\n",
            "Final Accuracy :  0.20000000298023224\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 80)        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 80, 1)        0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 80, 128)      1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 80, 128)      512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 80, 128)      0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 80, 256)      164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 80, 256)      1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 80, 256)      0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 80, 128)      98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 80, 128)      512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            2848        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 80, 128)      0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 3)            411         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 268,987\n",
            "Trainable params: 267,963\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset DistalPhalanxOutlineAgeGroup ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/DistalPhalanxOutlineAgeGroup_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/DistalPhalanxOutlineAgeGroup_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  139 Number of test samples :  400\n",
            "Number of classes :  3\n",
            "Sequence length :  80\n",
            "Class weights :  [3.08888889 0.78531073 0.71282051]\n",
            "Train on 139 samples, validate on 400 samples\n",
            "Epoch 1/4\n",
            " - 2s - loss: 1.1140 - accuracy: 0.4892 - val_loss: 0.7513 - val_accuracy: 0.6525\n",
            "\n",
            "Epoch 00001: loss improved from inf to 1.11395, saving model to ./weights/lstmfcn_8_cells_weights/DistalPhalanxOutlineAgeGroup_weights.h5\n",
            "Epoch 2/4\n",
            " - 1s - loss: 0.8377 - accuracy: 0.5899 - val_loss: 0.9350 - val_accuracy: 0.2825\n",
            "\n",
            "Epoch 00002: loss improved from 1.11395 to 0.83768, saving model to ./weights/lstmfcn_8_cells_weights/DistalPhalanxOutlineAgeGroup_weights.h5\n",
            "Epoch 3/4\n",
            " - 1s - loss: 0.8115 - accuracy: 0.6403 - val_loss: 1.2900 - val_accuracy: 0.2825\n",
            "\n",
            "Epoch 00003: loss improved from 0.83768 to 0.81145, saving model to ./weights/lstmfcn_8_cells_weights/DistalPhalanxOutlineAgeGroup_weights.h5\n",
            "Epoch 4/4\n",
            " - 1s - loss: 0.8230 - accuracy: 0.6331 - val_loss: 1.5983 - val_accuracy: 0.2825\n",
            "\n",
            "Epoch 00004: loss did not improve from 0.81145\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/DistalPhalanxOutlineAgeGroup_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/DistalPhalanxOutlineAgeGroup_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  139 Number of test samples :  400\n",
            "Number of classes :  3\n",
            "Sequence length :  80\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/DistalPhalanxOutlineAgeGroup_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "400/400 [==============================] - 1s 1ms/step\n",
            "\n",
            "Final Accuracy :  0.2824999988079071\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 80)        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 80, 1)        0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 80, 128)      1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 80, 128)      512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 80, 128)      0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 80, 256)      164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 80, 256)      1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 80, 256)      0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 80, 128)      98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 80, 128)      512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            2848        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 80, 128)      0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 2)            274         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 268,850\n",
            "Trainable params: 267,826\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset DistalPhalanxOutlineCorrect ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/DistalPhalanxOutlineCorrect_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/DistalPhalanxOutlineCorrect_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  276 Number of test samples :  600\n",
            "Number of classes :  2\n",
            "Sequence length :  80\n",
            "Class weights :  [1.2        0.85714286]\n",
            "Train on 276 samples, validate on 600 samples\n",
            "Epoch 1/4\n",
            " - 3s - loss: 0.6941 - accuracy: 0.5725 - val_loss: 1.0312 - val_accuracy: 0.6300\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.69410, saving model to ./weights/lstmfcn_8_cells_weights/DistalPhalanxOutlineCorrect_weights.h5\n",
            "Epoch 2/4\n",
            " - 2s - loss: 0.6147 - accuracy: 0.6558 - val_loss: 1.2876 - val_accuracy: 0.6300\n",
            "\n",
            "Epoch 00002: loss improved from 0.69410 to 0.61465, saving model to ./weights/lstmfcn_8_cells_weights/DistalPhalanxOutlineCorrect_weights.h5\n",
            "Epoch 3/4\n",
            " - 2s - loss: 0.6088 - accuracy: 0.7101 - val_loss: 1.3174 - val_accuracy: 0.6300\n",
            "\n",
            "Epoch 00003: loss improved from 0.61465 to 0.60882, saving model to ./weights/lstmfcn_8_cells_weights/DistalPhalanxOutlineCorrect_weights.h5\n",
            "Epoch 4/4\n",
            " - 2s - loss: 0.5790 - accuracy: 0.6812 - val_loss: 1.2747 - val_accuracy: 0.6300\n",
            "\n",
            "Epoch 00004: loss improved from 0.60882 to 0.57898, saving model to ./weights/lstmfcn_8_cells_weights/DistalPhalanxOutlineCorrect_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/DistalPhalanxOutlineCorrect_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/DistalPhalanxOutlineCorrect_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  276 Number of test samples :  600\n",
            "Number of classes :  2\n",
            "Sequence length :  80\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/DistalPhalanxOutlineCorrect_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "600/600 [==============================] - 1s 1ms/step\n",
            "\n",
            "Final Accuracy :  0.6299999952316284\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 80)        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 80, 1)        0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 80, 128)      1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 80, 128)      512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 80, 128)      0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 80, 256)      164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 80, 256)      1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 80, 256)      0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 80, 128)      98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 80, 128)      512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            2848        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 80, 128)      0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 6)            822         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 269,398\n",
            "Trainable params: 268,374\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset DistalPhalanxTW ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/DistalPhalanxTW_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/DistalPhalanxTW_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  139 Number of test samples :  400\n",
            "Number of classes :  6\n",
            "Sequence length :  80\n",
            "Class weights :  [1.28703704 1.21929825 0.59401709 1.78205128 2.89583333 0.5515873 ]\n",
            "Train on 139 samples, validate on 400 samples\n",
            "Epoch 1/4\n",
            " - 2s - loss: 1.8533 - accuracy: 0.1655 - val_loss: 1.4068 - val_accuracy: 0.5375\n",
            "\n",
            "Epoch 00001: loss improved from inf to 1.85327, saving model to ./weights/lstmfcn_8_cells_weights/DistalPhalanxTW_weights.h5\n",
            "Epoch 2/4\n",
            " - 1s - loss: 1.2856 - accuracy: 0.5827 - val_loss: 1.4030 - val_accuracy: 0.7375\n",
            "\n",
            "Epoch 00002: loss improved from 1.85327 to 1.28565, saving model to ./weights/lstmfcn_8_cells_weights/DistalPhalanxTW_weights.h5\n",
            "Epoch 3/4\n",
            " - 1s - loss: 1.1519 - accuracy: 0.5827 - val_loss: 1.8001 - val_accuracy: 0.2050\n",
            "\n",
            "Epoch 00003: loss improved from 1.28565 to 1.15185, saving model to ./weights/lstmfcn_8_cells_weights/DistalPhalanxTW_weights.h5\n",
            "Epoch 4/4\n",
            " - 1s - loss: 1.0885 - accuracy: 0.5899 - val_loss: 2.5460 - val_accuracy: 0.2050\n",
            "\n",
            "Epoch 00004: loss improved from 1.15185 to 1.08850, saving model to ./weights/lstmfcn_8_cells_weights/DistalPhalanxTW_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/DistalPhalanxTW_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/DistalPhalanxTW_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  139 Number of test samples :  400\n",
            "Number of classes :  6\n",
            "Sequence length :  80\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/DistalPhalanxTW_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "400/400 [==============================] - 1s 1ms/step\n",
            "\n",
            "Final Accuracy :  0.20499999821186066\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 96)        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 96, 1)        0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 96, 128)      1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 96, 128)      512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 96, 128)      0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 96, 256)      164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 96, 256)      1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 96, 256)      0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 96, 128)      98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 96, 128)      512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            3360        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 96, 128)      0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 2)            274         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 269,362\n",
            "Trainable params: 268,338\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset ECG200 ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ECG200_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ECG200_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  100 Number of test samples :  100\n",
            "Number of classes :  2\n",
            "Sequence length :  96\n",
            "Class weights :  [1.61290323 0.72463768]\n",
            "Train on 100 samples, validate on 100 samples\n",
            "Epoch 1/4\n",
            " - 2s - loss: 0.6855 - accuracy: 0.6000 - val_loss: 0.6372 - val_accuracy: 0.6400\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.68553, saving model to ./weights/lstmfcn_8_cells_weights/ECG200_weights.h5\n",
            "Epoch 2/4\n",
            " - 1s - loss: 0.5373 - accuracy: 0.7700 - val_loss: 0.6042 - val_accuracy: 0.6400\n",
            "\n",
            "Epoch 00002: loss improved from 0.68553 to 0.53725, saving model to ./weights/lstmfcn_8_cells_weights/ECG200_weights.h5\n",
            "Epoch 3/4\n",
            " - 1s - loss: 0.4906 - accuracy: 0.8000 - val_loss: 0.6031 - val_accuracy: 0.7800\n",
            "\n",
            "Epoch 00003: loss improved from 0.53725 to 0.49058, saving model to ./weights/lstmfcn_8_cells_weights/ECG200_weights.h5\n",
            "Epoch 4/4\n",
            " - 1s - loss: 0.4524 - accuracy: 0.7800 - val_loss: 0.5939 - val_accuracy: 0.7800\n",
            "\n",
            "Epoch 00004: loss improved from 0.49058 to 0.45243, saving model to ./weights/lstmfcn_8_cells_weights/ECG200_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ECG200_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ECG200_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  100 Number of test samples :  100\n",
            "Number of classes :  2\n",
            "Sequence length :  96\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/ECG200_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "100/100 [==============================] - 0s 2ms/step\n",
            "\n",
            "Final Accuracy :  0.7799999713897705\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 136)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 136, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 136, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 136, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 136, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 136, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 136, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 136, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 136, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 136, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            4640        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 136, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 2)            274         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 270,642\n",
            "Trainable params: 269,618\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset ECGFiveDays ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ECGFiveDays_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ECGFiveDays_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  23 Number of test samples :  861\n",
            "Number of classes :  2\n",
            "Sequence length :  136\n",
            "Class weights :  [0.82142857 1.27777778]\n",
            "Train on 23 samples, validate on 861 samples\n",
            "Epoch 1/4\n",
            " - 3s - loss: 0.6541 - accuracy: 0.6087 - val_loss: 0.7955 - val_accuracy: 0.5029\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.65414, saving model to ./weights/lstmfcn_8_cells_weights/ECGFiveDays_weights.h5\n",
            "Epoch 2/4\n",
            " - 2s - loss: 0.5551 - accuracy: 0.7391 - val_loss: 0.8062 - val_accuracy: 0.5029\n",
            "\n",
            "Epoch 00002: loss improved from 0.65414 to 0.55514, saving model to ./weights/lstmfcn_8_cells_weights/ECGFiveDays_weights.h5\n",
            "Epoch 3/4\n",
            " - 2s - loss: 0.5058 - accuracy: 0.7826 - val_loss: 0.8039 - val_accuracy: 0.5029\n",
            "\n",
            "Epoch 00003: loss improved from 0.55514 to 0.50578, saving model to ./weights/lstmfcn_8_cells_weights/ECGFiveDays_weights.h5\n",
            "Epoch 4/4\n",
            " - 2s - loss: 0.4407 - accuracy: 0.8261 - val_loss: 0.7888 - val_accuracy: 0.5029\n",
            "\n",
            "Epoch 00004: loss improved from 0.50578 to 0.44074, saving model to ./weights/lstmfcn_8_cells_weights/ECGFiveDays_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ECGFiveDays_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ECGFiveDays_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  23 Number of test samples :  861\n",
            "Number of classes :  2\n",
            "Sequence length :  136\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/ECGFiveDays_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "861/861 [==============================] - 2s 2ms/step\n",
            "\n",
            "Final Accuracy :  0.5029035806655884\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 512)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 512, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 512, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 512, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 512, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 512, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 512, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 512, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 512, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 512, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            16672       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 512, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 2)            274         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 282,674\n",
            "Trainable params: 281,650\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset BeetleFly ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/BeetleFly_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/BeetleFly_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  20 Number of test samples :  20\n",
            "Number of classes :  2\n",
            "Sequence length :  512\n",
            "Class weights :  [1. 1.]\n",
            "Train on 20 samples, validate on 20 samples\n",
            "Epoch 1/4\n",
            " - 2s - loss: 0.7973 - accuracy: 0.4000 - val_loss: 1.0747 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.79733, saving model to ./weights/lstmfcn_8_cells_weights/BeetleFly_weights.h5\n",
            "Epoch 2/4\n",
            " - 1s - loss: 0.5496 - accuracy: 0.8500 - val_loss: 1.2593 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00002: loss improved from 0.79733 to 0.54956, saving model to ./weights/lstmfcn_8_cells_weights/BeetleFly_weights.h5\n",
            "Epoch 3/4\n",
            " - 1s - loss: 0.4482 - accuracy: 0.9000 - val_loss: 1.3994 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00003: loss improved from 0.54956 to 0.44819, saving model to ./weights/lstmfcn_8_cells_weights/BeetleFly_weights.h5\n",
            "Epoch 4/4\n",
            " - 1s - loss: 0.4240 - accuracy: 0.8500 - val_loss: 1.4739 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00004: loss improved from 0.44819 to 0.42398, saving model to ./weights/lstmfcn_8_cells_weights/BeetleFly_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/BeetleFly_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/BeetleFly_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  20 Number of test samples :  20\n",
            "Number of classes :  2\n",
            "Sequence length :  512\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/BeetleFly_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "20/20 [==============================] - 0s 12ms/step\n",
            "\n",
            "Final Accuracy :  0.5\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 512)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 512, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 512, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 512, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 512, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 512, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 512, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 512, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 512, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 512, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            16672       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 512, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 2)            274         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 282,674\n",
            "Trainable params: 281,650\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset BirdChicken ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/BirdChicken_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/BirdChicken_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  20 Number of test samples :  20\n",
            "Number of classes :  2\n",
            "Sequence length :  512\n",
            "Class weights :  [1. 1.]\n",
            "Train on 20 samples, validate on 20 samples\n",
            "Epoch 1/4\n",
            " - 2s - loss: 0.7247 - accuracy: 0.5500 - val_loss: 0.8425 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.72466, saving model to ./weights/lstmfcn_8_cells_weights/BirdChicken_weights.h5\n",
            "Epoch 2/4\n",
            " - 1s - loss: 0.5365 - accuracy: 0.9000 - val_loss: 0.9959 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00002: loss improved from 0.72466 to 0.53648, saving model to ./weights/lstmfcn_8_cells_weights/BirdChicken_weights.h5\n",
            "Epoch 3/4\n",
            " - 1s - loss: 0.4514 - accuracy: 0.9000 - val_loss: 1.1075 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00003: loss improved from 0.53648 to 0.45143, saving model to ./weights/lstmfcn_8_cells_weights/BirdChicken_weights.h5\n",
            "Epoch 4/4\n",
            " - 1s - loss: 0.4092 - accuracy: 0.9500 - val_loss: 1.2815 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00004: loss improved from 0.45143 to 0.40923, saving model to ./weights/lstmfcn_8_cells_weights/BirdChicken_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/BirdChicken_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/BirdChicken_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  20 Number of test samples :  20\n",
            "Number of classes :  2\n",
            "Sequence length :  512\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/BirdChicken_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "20/20 [==============================] - 0s 13ms/step\n",
            "\n",
            "Final Accuracy :  0.5\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 24)        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 24, 1)        0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 24, 128)      1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 24, 128)      512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 24, 128)      0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 24, 256)      164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 24, 256)      1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 24, 256)      0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 24, 128)      98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 24, 128)      512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            1056        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 24, 128)      0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 2)            274         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 267,058\n",
            "Trainable params: 266,034\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset ItalyPowerDemand ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ItalyPowerDemand_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ItalyPowerDemand_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  67 Number of test samples :  1029\n",
            "Number of classes :  2\n",
            "Sequence length :  24\n",
            "Class weights :  [0.98529412 1.01515152]\n",
            "Train on 67 samples, validate on 1029 samples\n",
            "Epoch 1/4\n",
            " - 2s - loss: 0.7155 - accuracy: 0.5075 - val_loss: 0.6339 - val_accuracy: 0.6395\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.71553, saving model to ./weights/lstmfcn_8_cells_weights/ItalyPowerDemand_weights.h5\n",
            "Epoch 2/4\n",
            " - 0s - loss: 0.3955 - accuracy: 0.8507 - val_loss: 0.6117 - val_accuracy: 0.5607\n",
            "\n",
            "Epoch 00002: loss improved from 0.71553 to 0.39554, saving model to ./weights/lstmfcn_8_cells_weights/ItalyPowerDemand_weights.h5\n",
            "Epoch 3/4\n",
            " - 0s - loss: 0.2687 - accuracy: 0.9552 - val_loss: 0.7008 - val_accuracy: 0.5015\n",
            "\n",
            "Epoch 00003: loss improved from 0.39554 to 0.26871, saving model to ./weights/lstmfcn_8_cells_weights/ItalyPowerDemand_weights.h5\n",
            "Epoch 4/4\n",
            " - 0s - loss: 0.1894 - accuracy: 0.9701 - val_loss: 0.8368 - val_accuracy: 0.5015\n",
            "\n",
            "Epoch 00004: loss improved from 0.26871 to 0.18936, saving model to ./weights/lstmfcn_8_cells_weights/ItalyPowerDemand_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ItalyPowerDemand_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ItalyPowerDemand_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  67 Number of test samples :  1029\n",
            "Number of classes :  2\n",
            "Sequence length :  24\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/ItalyPowerDemand_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "1029/1029 [==============================] - 0s 436us/step\n",
            "\n",
            "Final Accuracy :  0.5014577507972717\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 70)        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 70, 1)        0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 70, 128)      1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 70, 128)      512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 70, 128)      0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 70, 256)      164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 70, 256)      1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 70, 256)      0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 70, 128)      98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 70, 128)      512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            2528        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 70, 128)      0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 2)            274         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 268,530\n",
            "Trainable params: 267,506\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset SonyAIBORobotSurface ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/SonyAIBORobotSurface_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/SonyAIBORobotSurface_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  20 Number of test samples :  601\n",
            "Number of classes :  2\n",
            "Sequence length :  70\n",
            "Class weights :  [1.66666667 0.71428571]\n",
            "Train on 20 samples, validate on 601 samples\n",
            "Epoch 1/4\n",
            " - 2s - loss: 0.6343 - accuracy: 0.7000 - val_loss: 0.7092 - val_accuracy: 0.4343\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.63429, saving model to ./weights/lstmfcn_8_cells_weights/SonyAIBORobotSurface_weights.h5\n",
            "Epoch 2/4\n",
            " - 1s - loss: 0.3731 - accuracy: 0.9500 - val_loss: 0.5874 - val_accuracy: 0.6156\n",
            "\n",
            "Epoch 00002: loss improved from 0.63429 to 0.37313, saving model to ./weights/lstmfcn_8_cells_weights/SonyAIBORobotSurface_weights.h5\n",
            "Epoch 3/4\n",
            " - 1s - loss: 0.2610 - accuracy: 0.9500 - val_loss: 0.4906 - val_accuracy: 0.7354\n",
            "\n",
            "Epoch 00003: loss improved from 0.37313 to 0.26095, saving model to ./weights/lstmfcn_8_cells_weights/SonyAIBORobotSurface_weights.h5\n",
            "Epoch 4/4\n",
            " - 1s - loss: 0.2097 - accuracy: 0.9500 - val_loss: 0.4037 - val_accuracy: 0.8136\n",
            "\n",
            "Epoch 00004: loss improved from 0.26095 to 0.20967, saving model to ./weights/lstmfcn_8_cells_weights/SonyAIBORobotSurface_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/SonyAIBORobotSurface_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/SonyAIBORobotSurface_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  20 Number of test samples :  601\n",
            "Number of classes :  2\n",
            "Sequence length :  70\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/SonyAIBORobotSurface_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "601/601 [==============================] - 1s 2ms/step\n",
            "\n",
            "Final Accuracy :  0.8136439323425293\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 65)        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 65, 1)        0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 65, 128)      1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 65, 128)      512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 65, 128)      0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 65, 256)      164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 65, 256)      1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 65, 256)      0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 65, 128)      98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 65, 128)      512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            2368        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 65, 128)      0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 2)            274         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 268,370\n",
            "Trainable params: 267,346\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset SonyAIBORobotSurfaceII ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/SonyAIBORobotSurfaceII_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/SonyAIBORobotSurfaceII_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  27 Number of test samples :  953\n",
            "Number of classes :  2\n",
            "Sequence length :  65\n",
            "Class weights :  [1.22727273 0.84375   ]\n",
            "Train on 27 samples, validate on 953 samples\n",
            "Epoch 1/4\n",
            " - 3s - loss: 0.7094 - accuracy: 0.5926 - val_loss: 0.5556 - val_accuracy: 0.6180\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.70939, saving model to ./weights/lstmfcn_8_cells_weights/SonyAIBORobotSurfaceII_weights.h5\n",
            "Epoch 2/4\n",
            " - 1s - loss: 0.4610 - accuracy: 0.8148 - val_loss: 0.4817 - val_accuracy: 0.6946\n",
            "\n",
            "Epoch 00002: loss improved from 0.70939 to 0.46102, saving model to ./weights/lstmfcn_8_cells_weights/SonyAIBORobotSurfaceII_weights.h5\n",
            "Epoch 3/4\n",
            " - 1s - loss: 0.3292 - accuracy: 0.9630 - val_loss: 0.4403 - val_accuracy: 0.7188\n",
            "\n",
            "Epoch 00003: loss improved from 0.46102 to 0.32920, saving model to ./weights/lstmfcn_8_cells_weights/SonyAIBORobotSurfaceII_weights.h5\n",
            "Epoch 4/4\n",
            " - 1s - loss: 0.2683 - accuracy: 0.9630 - val_loss: 0.4235 - val_accuracy: 0.7240\n",
            "\n",
            "Epoch 00004: loss improved from 0.32920 to 0.26830, saving model to ./weights/lstmfcn_8_cells_weights/SonyAIBORobotSurfaceII_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/SonyAIBORobotSurfaceII_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/SonyAIBORobotSurfaceII_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  27 Number of test samples :  953\n",
            "Number of classes :  2\n",
            "Sequence length :  65\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/SonyAIBORobotSurfaceII_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "953/953 [==============================] - 1s 898us/step\n",
            "\n",
            "Final Accuracy :  0.7240293622016907\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 80)        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 80, 1)        0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 80, 128)      1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 80, 128)      512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 80, 128)      0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 80, 256)      164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 80, 256)      1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 80, 256)      0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 80, 128)      98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 80, 128)      512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            2848        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 80, 128)      0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 3)            411         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 268,987\n",
            "Trainable params: 267,963\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset MiddlePhalanxOutlineAgeGroup ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/MiddlePhalanxOutlineAgeGroup_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/MiddlePhalanxOutlineAgeGroup_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  154 Number of test samples :  400\n",
            "Number of classes :  3\n",
            "Sequence length :  80\n",
            "Class weights :  [1.38738739 0.58333333 1.77011494]\n",
            "Train on 154 samples, validate on 400 samples\n",
            "Epoch 1/4\n",
            " - 2s - loss: 1.3107 - accuracy: 0.2403 - val_loss: 1.1233 - val_accuracy: 0.1375\n",
            "\n",
            "Epoch 00001: loss improved from inf to 1.31072, saving model to ./weights/lstmfcn_8_cells_weights/MiddlePhalanxOutlineAgeGroup_weights.h5\n",
            "Epoch 2/4\n",
            " - 1s - loss: 1.1260 - accuracy: 0.3312 - val_loss: 1.1927 - val_accuracy: 0.1375\n",
            "\n",
            "Epoch 00002: loss improved from 1.31072 to 1.12602, saving model to ./weights/lstmfcn_8_cells_weights/MiddlePhalanxOutlineAgeGroup_weights.h5\n",
            "Epoch 3/4\n",
            " - 1s - loss: 1.0191 - accuracy: 0.5390 - val_loss: 1.2434 - val_accuracy: 0.1375\n",
            "\n",
            "Epoch 00003: loss improved from 1.12602 to 1.01912, saving model to ./weights/lstmfcn_8_cells_weights/MiddlePhalanxOutlineAgeGroup_weights.h5\n",
            "Epoch 4/4\n",
            " - 1s - loss: 0.9474 - accuracy: 0.6169 - val_loss: 1.3373 - val_accuracy: 0.1900\n",
            "\n",
            "Epoch 00004: loss improved from 1.01912 to 0.94739, saving model to ./weights/lstmfcn_8_cells_weights/MiddlePhalanxOutlineAgeGroup_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/MiddlePhalanxOutlineAgeGroup_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/MiddlePhalanxOutlineAgeGroup_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  154 Number of test samples :  400\n",
            "Number of classes :  3\n",
            "Sequence length :  80\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/MiddlePhalanxOutlineAgeGroup_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "400/400 [==============================] - 1s 1ms/step\n",
            "\n",
            "Final Accuracy :  0.1899999976158142\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 80)        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 80, 1)        0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 80, 128)      1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 80, 128)      512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 80, 128)      0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 80, 256)      164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 80, 256)      1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 80, 256)      0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 80, 128)      98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 80, 128)      512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            2848        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 80, 128)      0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 2)            274         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 268,850\n",
            "Trainable params: 267,826\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset MiddlePhalanxOutlineCorrect ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/MiddlePhalanxOutlineCorrect_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/MiddlePhalanxOutlineCorrect_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  291 Number of test samples :  600\n",
            "Number of classes :  2\n",
            "Sequence length :  80\n",
            "Class weights :  [1.164      0.87650602]\n",
            "Train on 291 samples, validate on 600 samples\n",
            "Epoch 1/4\n",
            " - 3s - loss: 0.6671 - accuracy: 0.6117 - val_loss: 0.6558 - val_accuracy: 0.6467\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.66714, saving model to ./weights/lstmfcn_8_cells_weights/MiddlePhalanxOutlineCorrect_weights.h5\n",
            "Epoch 2/4\n",
            " - 2s - loss: 0.5982 - accuracy: 0.7045 - val_loss: 0.6681 - val_accuracy: 0.6533\n",
            "\n",
            "Epoch 00002: loss improved from 0.66714 to 0.59817, saving model to ./weights/lstmfcn_8_cells_weights/MiddlePhalanxOutlineCorrect_weights.h5\n",
            "Epoch 3/4\n",
            " - 2s - loss: 0.5720 - accuracy: 0.7354 - val_loss: 0.6798 - val_accuracy: 0.4600\n",
            "\n",
            "Epoch 00003: loss improved from 0.59817 to 0.57202, saving model to ./weights/lstmfcn_8_cells_weights/MiddlePhalanxOutlineCorrect_weights.h5\n",
            "Epoch 4/4\n",
            " - 2s - loss: 0.5525 - accuracy: 0.7251 - val_loss: 0.6487 - val_accuracy: 0.6467\n",
            "\n",
            "Epoch 00004: loss improved from 0.57202 to 0.55250, saving model to ./weights/lstmfcn_8_cells_weights/MiddlePhalanxOutlineCorrect_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/MiddlePhalanxOutlineCorrect_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/MiddlePhalanxOutlineCorrect_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  291 Number of test samples :  600\n",
            "Number of classes :  2\n",
            "Sequence length :  80\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/MiddlePhalanxOutlineCorrect_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "600/600 [==============================] - 1s 1ms/step\n",
            "\n",
            "Final Accuracy :  0.6466666460037231\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 80)        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 80, 1)        0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 80, 128)      1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 80, 128)      512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 80, 128)      0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 80, 256)      164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 80, 256)      1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 80, 256)      0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 80, 128)      98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 80, 128)      512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            2848        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 80, 128)      0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 6)            822         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 269,398\n",
            "Trainable params: 268,374\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset MiddlePhalanxTW ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/MiddlePhalanxTW_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/MiddlePhalanxTW_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  154 Number of test samples :  399\n",
            "Number of classes :  6\n",
            "Sequence length :  80\n",
            "Class weights :  [1.02666667 0.58333333 1.11594203 2.33333333 2.85185185 0.61111111]\n",
            "Train on 154 samples, validate on 399 samples\n",
            "Epoch 1/4\n",
            " - 2s - loss: 2.2273 - accuracy: 0.1494 - val_loss: 2.8345 - val_accuracy: 0.1078\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.22733, saving model to ./weights/lstmfcn_8_cells_weights/MiddlePhalanxTW_weights.h5\n",
            "Epoch 2/4\n",
            " - 1s - loss: 1.8056 - accuracy: 0.1494 - val_loss: 2.8433 - val_accuracy: 0.1078\n",
            "\n",
            "Epoch 00002: loss improved from 2.22733 to 1.80562, saving model to ./weights/lstmfcn_8_cells_weights/MiddlePhalanxTW_weights.h5\n",
            "Epoch 3/4\n",
            " - 1s - loss: 1.5563 - accuracy: 0.2922 - val_loss: 2.9850 - val_accuracy: 0.1078\n",
            "\n",
            "Epoch 00003: loss improved from 1.80562 to 1.55633, saving model to ./weights/lstmfcn_8_cells_weights/MiddlePhalanxTW_weights.h5\n",
            "Epoch 4/4\n",
            " - 1s - loss: 1.3627 - accuracy: 0.4675 - val_loss: 3.2317 - val_accuracy: 0.2105\n",
            "\n",
            "Epoch 00004: loss improved from 1.55633 to 1.36269, saving model to ./weights/lstmfcn_8_cells_weights/MiddlePhalanxTW_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/MiddlePhalanxTW_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/MiddlePhalanxTW_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  154 Number of test samples :  399\n",
            "Number of classes :  6\n",
            "Sequence length :  80\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/MiddlePhalanxTW_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "399/399 [==============================] - 0s 1ms/step\n",
            "\n",
            "Final Accuracy :  0.21052631735801697\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 80)        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 80, 1)        0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 80, 128)      1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 80, 128)      512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 80, 128)      0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 80, 256)      164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 80, 256)      1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 80, 256)      0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 80, 128)      98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 80, 128)      512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            2848        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 80, 128)      0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 3)            411         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 268,987\n",
            "Trainable params: 267,963\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset ProximalPhalanxOutlineAgeGroup ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ProximalPhalanxOutlineAgeGroup_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ProximalPhalanxOutlineAgeGroup_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  400 Number of test samples :  205\n",
            "Number of classes :  3\n",
            "Sequence length :  80\n",
            "Class weights :  [1.85185185 0.95923261 0.70546737]\n",
            "Train on 400 samples, validate on 205 samples\n",
            "Epoch 1/4\n",
            " - 3s - loss: 0.9232 - accuracy: 0.6000 - val_loss: 0.9130 - val_accuracy: 0.4293\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.92319, saving model to ./weights/lstmfcn_8_cells_weights/ProximalPhalanxOutlineAgeGroup_weights.h5\n",
            "Epoch 2/4\n",
            " - 2s - loss: 0.7120 - accuracy: 0.7325 - val_loss: 1.6704 - val_accuracy: 0.4293\n",
            "\n",
            "Epoch 00002: loss improved from 0.92319 to 0.71197, saving model to ./weights/lstmfcn_8_cells_weights/ProximalPhalanxOutlineAgeGroup_weights.h5\n",
            "Epoch 3/4\n",
            " - 2s - loss: 0.6659 - accuracy: 0.7550 - val_loss: 2.4287 - val_accuracy: 0.4293\n",
            "\n",
            "Epoch 00003: loss improved from 0.71197 to 0.66586, saving model to ./weights/lstmfcn_8_cells_weights/ProximalPhalanxOutlineAgeGroup_weights.h5\n",
            "Epoch 4/4\n",
            " - 2s - loss: 0.6137 - accuracy: 0.7550 - val_loss: 3.0774 - val_accuracy: 0.4293\n",
            "\n",
            "Epoch 00004: loss improved from 0.66586 to 0.61374, saving model to ./weights/lstmfcn_8_cells_weights/ProximalPhalanxOutlineAgeGroup_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ProximalPhalanxOutlineAgeGroup_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ProximalPhalanxOutlineAgeGroup_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  400 Number of test samples :  205\n",
            "Number of classes :  3\n",
            "Sequence length :  80\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/ProximalPhalanxOutlineAgeGroup_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "205/205 [==============================] - 0s 2ms/step\n",
            "\n",
            "Final Accuracy :  0.4292683005332947\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 80)        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 80, 1)        0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 80, 128)      1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 80, 128)      512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 80, 128)      0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 80, 256)      164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 80, 256)      1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 80, 256)      0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 80, 128)      98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 80, 128)      512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            2848        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 80, 128)      0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 2)            274         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 268,850\n",
            "Trainable params: 267,826\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset ProximalPhalanxOutlineCorrect ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ProximalPhalanxOutlineCorrect_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ProximalPhalanxOutlineCorrect_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  600 Number of test samples :  291\n",
            "Number of classes :  2\n",
            "Sequence length :  80\n",
            "Class weights :  [1.54639175 0.73891626]\n",
            "Train on 600 samples, validate on 291 samples\n",
            "Epoch 1/4\n",
            " - 3s - loss: 0.5846 - accuracy: 0.6833 - val_loss: 0.9372 - val_accuracy: 0.6838\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.58460, saving model to ./weights/lstmfcn_8_cells_weights/ProximalPhalanxOutlineCorrect_weights.h5\n",
            "Epoch 2/4\n",
            " - 2s - loss: 0.5706 - accuracy: 0.6583 - val_loss: 1.1785 - val_accuracy: 0.6838\n",
            "\n",
            "Epoch 00002: loss improved from 0.58460 to 0.57056, saving model to ./weights/lstmfcn_8_cells_weights/ProximalPhalanxOutlineCorrect_weights.h5\n",
            "Epoch 3/4\n",
            " - 2s - loss: 0.5526 - accuracy: 0.7183 - val_loss: 1.5972 - val_accuracy: 0.6838\n",
            "\n",
            "Epoch 00003: loss improved from 0.57056 to 0.55256, saving model to ./weights/lstmfcn_8_cells_weights/ProximalPhalanxOutlineCorrect_weights.h5\n",
            "Epoch 4/4\n",
            " - 2s - loss: 0.5459 - accuracy: 0.7133 - val_loss: 1.7642 - val_accuracy: 0.6838\n",
            "\n",
            "Epoch 00004: loss improved from 0.55256 to 0.54585, saving model to ./weights/lstmfcn_8_cells_weights/ProximalPhalanxOutlineCorrect_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ProximalPhalanxOutlineCorrect_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ProximalPhalanxOutlineCorrect_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  600 Number of test samples :  291\n",
            "Number of classes :  2\n",
            "Sequence length :  80\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/ProximalPhalanxOutlineCorrect_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "291/291 [==============================] - 0s 1ms/step\n",
            "\n",
            "Final Accuracy :  0.6838487982749939\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 80)        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 80, 1)        0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 80, 128)      1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 80, 128)      512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 80, 128)      0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 80, 256)      164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 80, 256)      1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 80, 256)      0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 80, 128)      98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 80, 128)      512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            2848        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 80, 128)      0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 6)            822         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 269,398\n",
            "Trainable params: 268,374\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset ProximalPhalanxTW ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ProximalPhalanxTW_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ProximalPhalanxTW_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  205 Number of test samples :  400\n",
            "Number of classes :  6\n",
            "Sequence length :  80\n",
            "Class weights :  [17.08333333  0.50995025  0.85416667  3.41666667  2.44047619  0.47453704]\n",
            "Train on 205 samples, validate on 400 samples\n",
            "Epoch 1/4\n",
            " - 2s - loss: 1.6073 - accuracy: 0.2634 - val_loss: 1.5135 - val_accuracy: 0.2725\n",
            "\n",
            "Epoch 00001: loss improved from inf to 1.60728, saving model to ./weights/lstmfcn_8_cells_weights/ProximalPhalanxTW_weights.h5\n",
            "Epoch 2/4\n",
            " - 1s - loss: 1.2757 - accuracy: 0.6927 - val_loss: 1.6773 - val_accuracy: 0.2725\n",
            "\n",
            "Epoch 00002: loss improved from 1.60728 to 1.27567, saving model to ./weights/lstmfcn_8_cells_weights/ProximalPhalanxTW_weights.h5\n",
            "Epoch 3/4\n",
            " - 1s - loss: 1.1062 - accuracy: 0.7122 - val_loss: 1.9928 - val_accuracy: 0.2725\n",
            "\n",
            "Epoch 00003: loss improved from 1.27567 to 1.10616, saving model to ./weights/lstmfcn_8_cells_weights/ProximalPhalanxTW_weights.h5\n",
            "Epoch 4/4\n",
            " - 1s - loss: 0.9687 - accuracy: 0.6976 - val_loss: 2.2864 - val_accuracy: 0.2725\n",
            "\n",
            "Epoch 00004: loss improved from 1.10616 to 0.96871, saving model to ./weights/lstmfcn_8_cells_weights/ProximalPhalanxTW_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ProximalPhalanxTW_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ProximalPhalanxTW_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  205 Number of test samples :  400\n",
            "Number of classes :  6\n",
            "Sequence length :  80\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/ProximalPhalanxTW_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "400/400 [==============================] - 1s 1ms/step\n",
            "\n",
            "Final Accuracy :  0.27250000834465027\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 84)        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 84, 1)        0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 84, 128)      1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 84, 128)      512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 84, 128)      0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 84, 256)      164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 84, 256)      1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 84, 256)      0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 84, 128)      98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 84, 128)      512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            2976        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 84, 128)      0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 2)            274         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 268,978\n",
            "Trainable params: 267,954\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset MoteStrain ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/MoteStrain_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/MoteStrain_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  20 Number of test samples :  1252\n",
            "Number of classes :  2\n",
            "Sequence length :  84\n",
            "Class weights :  [1. 1.]\n",
            "Train on 20 samples, validate on 1252 samples\n",
            "Epoch 1/4\n",
            " - 2s - loss: 0.8456 - accuracy: 0.5000 - val_loss: 1.0296 - val_accuracy: 0.4609\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.84563, saving model to ./weights/lstmfcn_8_cells_weights/MoteStrain_weights.h5\n",
            "Epoch 2/4\n",
            " - 1s - loss: 0.5172 - accuracy: 0.8000 - val_loss: 1.0316 - val_accuracy: 0.4609\n",
            "\n",
            "Epoch 00002: loss improved from 0.84563 to 0.51720, saving model to ./weights/lstmfcn_8_cells_weights/MoteStrain_weights.h5\n",
            "Epoch 3/4\n",
            " - 1s - loss: 0.4375 - accuracy: 0.9000 - val_loss: 1.1388 - val_accuracy: 0.4609\n",
            "\n",
            "Epoch 00003: loss improved from 0.51720 to 0.43748, saving model to ./weights/lstmfcn_8_cells_weights/MoteStrain_weights.h5\n",
            "Epoch 4/4\n",
            " - 1s - loss: 0.3728 - accuracy: 0.9000 - val_loss: 1.2772 - val_accuracy: 0.4609\n",
            "\n",
            "Epoch 00004: loss improved from 0.43748 to 0.37277, saving model to ./weights/lstmfcn_8_cells_weights/MoteStrain_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/MoteStrain_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/MoteStrain_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  20 Number of test samples :  1252\n",
            "Number of classes :  2\n",
            "Sequence length :  84\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/MoteStrain_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "1252/1252 [==============================] - 1s 1ms/step\n",
            "\n",
            "Final Accuracy :  0.4608626067638397\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 99)        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 99, 1)        0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 99, 128)      1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 99, 128)      512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 99, 128)      0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 99, 256)      164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 99, 256)      1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 99, 256)      0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 99, 128)      98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 99, 128)      512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            3456        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 99, 128)      0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 10)           1370        concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 270,554\n",
            "Trainable params: 269,530\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset MedicalImages ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/MedicalImages_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/MedicalImages_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  381 Number of test samples :  760\n",
            "Number of classes :  10\n",
            "Sequence length :  99\n",
            "Class weights :  [1.08857143 2.54       1.524      2.38125    3.81       5.44285714\n",
            " 2.11666667 6.35       0.82826087 0.18768473]\n",
            "Train on 381 samples, validate on 760 samples\n",
            "Epoch 1/4\n",
            " - 4s - loss: 2.0816 - accuracy: 0.2835 - val_loss: 2.0254 - val_accuracy: 0.4592\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.08163, saving model to ./weights/lstmfcn_8_cells_weights/MedicalImages_weights.h5\n",
            "Epoch 2/4\n",
            " - 2s - loss: 1.6828 - accuracy: 0.5223 - val_loss: 1.8907 - val_accuracy: 0.5039\n",
            "\n",
            "Epoch 00002: loss improved from 2.08163 to 1.68285, saving model to ./weights/lstmfcn_8_cells_weights/MedicalImages_weights.h5\n",
            "Epoch 3/4\n",
            " - 2s - loss: 1.4970 - accuracy: 0.5617 - val_loss: 2.1460 - val_accuracy: 0.5145\n",
            "\n",
            "Epoch 00003: loss improved from 1.68285 to 1.49696, saving model to ./weights/lstmfcn_8_cells_weights/MedicalImages_weights.h5\n",
            "Epoch 4/4\n",
            " - 2s - loss: 1.3814 - accuracy: 0.5591 - val_loss: 2.4837 - val_accuracy: 0.5145\n",
            "\n",
            "Epoch 00004: loss improved from 1.49696 to 1.38141, saving model to ./weights/lstmfcn_8_cells_weights/MedicalImages_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/MedicalImages_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/MedicalImages_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  381 Number of test samples :  760\n",
            "Number of classes :  10\n",
            "Sequence length :  99\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/MedicalImages_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "760/760 [==============================] - 1s 1ms/step\n",
            "\n",
            "Final Accuracy :  0.5144736766815186\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 235)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 235, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 235, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 235, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 235, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 235, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 235, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 235, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 235, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 235, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            7808        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 235, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 2)            274         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 273,810\n",
            "Trainable params: 272,786\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset Strawberry ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Strawberry_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Strawberry_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  370 Number of test samples :  613\n",
            "Number of classes :  2\n",
            "Sequence length :  235\n",
            "Class weights :  [1.40151515 0.77731092]\n",
            "Train on 370 samples, validate on 613 samples\n",
            "Epoch 1/4\n",
            " - 6s - loss: 0.6593 - accuracy: 0.6432 - val_loss: 0.6598 - val_accuracy: 0.6427\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.65931, saving model to ./weights/lstmfcn_8_cells_weights/Strawberry_weights.h5\n",
            "Epoch 2/4\n",
            " - 5s - loss: 0.5981 - accuracy: 0.6459 - val_loss: 0.6632 - val_accuracy: 0.6427\n",
            "\n",
            "Epoch 00002: loss improved from 0.65931 to 0.59810, saving model to ./weights/lstmfcn_8_cells_weights/Strawberry_weights.h5\n",
            "Epoch 3/4\n",
            " - 5s - loss: 0.5746 - accuracy: 0.6784 - val_loss: 0.7048 - val_accuracy: 0.4029\n",
            "\n",
            "Epoch 00003: loss improved from 0.59810 to 0.57460, saving model to ./weights/lstmfcn_8_cells_weights/Strawberry_weights.h5\n",
            "Epoch 4/4\n",
            " - 5s - loss: 0.5525 - accuracy: 0.6405 - val_loss: 0.7470 - val_accuracy: 0.3573\n",
            "\n",
            "Epoch 00004: loss improved from 0.57460 to 0.55250, saving model to ./weights/lstmfcn_8_cells_weights/Strawberry_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Strawberry_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Strawberry_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  370 Number of test samples :  613\n",
            "Number of classes :  2\n",
            "Sequence length :  235\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/Strawberry_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "613/613 [==============================] - 2s 3ms/step\n",
            "\n",
            "Final Accuracy :  0.3572593927383423\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 277)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 277, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 277, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 277, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 277, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 277, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 277, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 277, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 277, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 277, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            9152        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 277, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 2)            274         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 275,154\n",
            "Trainable params: 274,130\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset ToeSegmentation1 ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ToeSegmentation1_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ToeSegmentation1_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  40 Number of test samples :  228\n",
            "Number of classes :  2\n",
            "Sequence length :  277\n",
            "Class weights :  [1. 1.]\n",
            "Train on 40 samples, validate on 228 samples\n",
            "Epoch 1/4\n",
            " - 2s - loss: 0.6959 - accuracy: 0.5000 - val_loss: 1.0968 - val_accuracy: 0.4737\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.69591, saving model to ./weights/lstmfcn_8_cells_weights/ToeSegmentation1_weights.h5\n",
            "Epoch 2/4\n",
            " - 1s - loss: 0.5355 - accuracy: 0.7750 - val_loss: 1.4066 - val_accuracy: 0.4737\n",
            "\n",
            "Epoch 00002: loss improved from 0.69591 to 0.53552, saving model to ./weights/lstmfcn_8_cells_weights/ToeSegmentation1_weights.h5\n",
            "Epoch 3/4\n",
            " - 1s - loss: 0.5105 - accuracy: 0.8000 - val_loss: 1.6083 - val_accuracy: 0.4737\n",
            "\n",
            "Epoch 00003: loss improved from 0.53552 to 0.51050, saving model to ./weights/lstmfcn_8_cells_weights/ToeSegmentation1_weights.h5\n",
            "Epoch 4/4\n",
            " - 1s - loss: 0.4368 - accuracy: 0.8500 - val_loss: 1.6959 - val_accuracy: 0.4737\n",
            "\n",
            "Epoch 00004: loss improved from 0.51050 to 0.43678, saving model to ./weights/lstmfcn_8_cells_weights/ToeSegmentation1_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ToeSegmentation1_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ToeSegmentation1_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  40 Number of test samples :  228\n",
            "Number of classes :  2\n",
            "Sequence length :  277\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/ToeSegmentation1_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "228/228 [==============================] - 1s 4ms/step\n",
            "\n",
            "Final Accuracy :  0.4736842215061188\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 286)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 286, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 286, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 286, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 286, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 286, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 286, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 286, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 286, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 286, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            9440        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 286, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 2)            274         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 275,442\n",
            "Trainable params: 274,418\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset Coffee ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Coffee_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Coffee_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  28 Number of test samples :  28\n",
            "Number of classes :  2\n",
            "Sequence length :  286\n",
            "Class weights :  [1. 1.]\n",
            "Train on 28 samples, validate on 28 samples\n",
            "Epoch 1/4\n",
            " - 1s - loss: 0.8715 - accuracy: 0.5000 - val_loss: 0.6978 - val_accuracy: 0.5357\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.87149, saving model to ./weights/lstmfcn_8_cells_weights/Coffee_weights.h5\n",
            "Epoch 2/4\n",
            " - 0s - loss: 0.7351 - accuracy: 0.5000 - val_loss: 0.6820 - val_accuracy: 0.5357\n",
            "\n",
            "Epoch 00002: loss improved from 0.87149 to 0.73508, saving model to ./weights/lstmfcn_8_cells_weights/Coffee_weights.h5\n",
            "Epoch 3/4\n",
            " - 0s - loss: 0.6622 - accuracy: 0.6429 - val_loss: 0.6803 - val_accuracy: 0.7500\n",
            "\n",
            "Epoch 00003: loss improved from 0.73508 to 0.66223, saving model to ./weights/lstmfcn_8_cells_weights/Coffee_weights.h5\n",
            "Epoch 4/4\n",
            " - 0s - loss: 0.6136 - accuracy: 0.7500 - val_loss: 0.6891 - val_accuracy: 0.4643\n",
            "\n",
            "Epoch 00004: loss improved from 0.66223 to 0.61359, saving model to ./weights/lstmfcn_8_cells_weights/Coffee_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Coffee_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Coffee_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  28 Number of test samples :  28\n",
            "Number of classes :  2\n",
            "Sequence length :  286\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/Coffee_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "28/28 [==============================] - 0s 8ms/step\n",
            "\n",
            "Final Accuracy :  0.4642857015132904\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 300)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 300, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 300, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 300, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 300, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 300, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 300, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 300, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 300, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 300, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            9888        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 300, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 12)           1644        concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 277,260\n",
            "Trainable params: 276,236\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset Cricket_X ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Cricket_X_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Cricket_X_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  390 Number of test samples :  390\n",
            "Number of classes :  12\n",
            "Sequence length :  300\n",
            "Class weights :  [0.85526316 1.0483871  0.95588235 0.92857143 1.2037037  0.95588235\n",
            " 0.90277778 1.0483871  1.3        1.16071429 0.83333333 1.015625  ]\n",
            "Train on 390 samples, validate on 390 samples\n",
            "Epoch 1/4\n",
            " - 7s - loss: 2.4506 - accuracy: 0.1436 - val_loss: 2.9035 - val_accuracy: 0.0897\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.45061, saving model to ./weights/lstmfcn_8_cells_weights/Cricket_X_weights.h5\n",
            "Epoch 2/4\n",
            " - 6s - loss: 2.1605 - accuracy: 0.3359 - val_loss: 3.1613 - val_accuracy: 0.1359\n",
            "\n",
            "Epoch 00002: loss improved from 2.45061 to 2.16052, saving model to ./weights/lstmfcn_8_cells_weights/Cricket_X_weights.h5\n",
            "Epoch 3/4\n",
            " - 6s - loss: 2.0576 - accuracy: 0.4154 - val_loss: 3.2776 - val_accuracy: 0.1462\n",
            "\n",
            "Epoch 00003: loss improved from 2.16052 to 2.05758, saving model to ./weights/lstmfcn_8_cells_weights/Cricket_X_weights.h5\n",
            "Epoch 4/4\n",
            " - 6s - loss: 1.9818 - accuracy: 0.4462 - val_loss: 3.2917 - val_accuracy: 0.1333\n",
            "\n",
            "Epoch 00004: loss improved from 2.05758 to 1.98181, saving model to ./weights/lstmfcn_8_cells_weights/Cricket_X_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Cricket_X_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Cricket_X_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  390 Number of test samples :  390\n",
            "Number of classes :  12\n",
            "Sequence length :  300\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/Cricket_X_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "390/390 [==============================] - 1s 4ms/step\n",
            "\n",
            "Final Accuracy :  0.13333334028720856\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 300)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 300, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 300, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 300, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 300, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 300, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 300, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 300, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 300, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 300, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            9888        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 300, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 12)           1644        concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 277,260\n",
            "Trainable params: 276,236\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset Cricket_Y ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Cricket_Y_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Cricket_Y_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  390 Number of test samples :  390\n",
            "Number of classes :  12\n",
            "Sequence length :  300\n",
            "Class weights :  [1.16071429 0.95588235 0.90277778 1.08333333 0.90277778 1.015625\n",
            " 1.015625   0.98484848 1.0483871  1.015625   1.015625   0.95588235]\n",
            "Train on 390 samples, validate on 390 samples\n",
            "Epoch 1/4\n",
            " - 7s - loss: 2.4054 - accuracy: 0.1692 - val_loss: 2.5747 - val_accuracy: 0.1026\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.40543, saving model to ./weights/lstmfcn_8_cells_weights/Cricket_Y_weights.h5\n",
            "Epoch 2/4\n",
            " - 6s - loss: 2.1263 - accuracy: 0.3385 - val_loss: 2.5872 - val_accuracy: 0.1462\n",
            "\n",
            "Epoch 00002: loss improved from 2.40543 to 2.12631, saving model to ./weights/lstmfcn_8_cells_weights/Cricket_Y_weights.h5\n",
            "Epoch 3/4\n",
            " - 6s - loss: 2.0349 - accuracy: 0.3923 - val_loss: 2.6861 - val_accuracy: 0.2051\n",
            "\n",
            "Epoch 00003: loss improved from 2.12631 to 2.03494, saving model to ./weights/lstmfcn_8_cells_weights/Cricket_Y_weights.h5\n",
            "Epoch 4/4\n",
            " - 6s - loss: 1.9587 - accuracy: 0.4179 - val_loss: 2.9613 - val_accuracy: 0.1179\n",
            "\n",
            "Epoch 00004: loss improved from 2.03494 to 1.95868, saving model to ./weights/lstmfcn_8_cells_weights/Cricket_Y_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Cricket_Y_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Cricket_Y_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  390 Number of test samples :  390\n",
            "Number of classes :  12\n",
            "Sequence length :  300\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/Cricket_Y_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "390/390 [==============================] - 1s 4ms/step\n",
            "\n",
            "Final Accuracy :  0.11794871836900711\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 300)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 300, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 300, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 300, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 300, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 300, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 300, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 300, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 300, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 300, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            9888        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 300, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 12)           1644        concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 277,260\n",
            "Trainable params: 276,236\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset Cricket_Z ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Cricket_Z_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Cricket_Z_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  390 Number of test samples :  390\n",
            "Number of classes :  12\n",
            "Sequence length :  300\n",
            "Class weights :  [1.015625   1.2037037  1.3        1.015625   1.12068966 0.90277778\n",
            " 0.79268293 0.87837838 1.0483871  1.0483871  0.98484848 0.90277778]\n",
            "Train on 390 samples, validate on 390 samples\n",
            "Epoch 1/4\n",
            " - 7s - loss: 2.4208 - accuracy: 0.1538 - val_loss: 2.9742 - val_accuracy: 0.1333\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.42081, saving model to ./weights/lstmfcn_8_cells_weights/Cricket_Z_weights.h5\n",
            "Epoch 2/4\n",
            " - 6s - loss: 2.1489 - accuracy: 0.2846 - val_loss: 3.1955 - val_accuracy: 0.1282\n",
            "\n",
            "Epoch 00002: loss improved from 2.42081 to 2.14888, saving model to ./weights/lstmfcn_8_cells_weights/Cricket_Z_weights.h5\n",
            "Epoch 3/4\n",
            " - 6s - loss: 2.0623 - accuracy: 0.3051 - val_loss: 3.2888 - val_accuracy: 0.1359\n",
            "\n",
            "Epoch 00003: loss improved from 2.14888 to 2.06231, saving model to ./weights/lstmfcn_8_cells_weights/Cricket_Z_weights.h5\n",
            "Epoch 4/4\n",
            " - 6s - loss: 2.0040 - accuracy: 0.3077 - val_loss: 3.4872 - val_accuracy: 0.1333\n",
            "\n",
            "Epoch 00004: loss improved from 2.06231 to 2.00403, saving model to ./weights/lstmfcn_8_cells_weights/Cricket_Z_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Cricket_Z_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Cricket_Z_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  390 Number of test samples :  390\n",
            "Number of classes :  12\n",
            "Sequence length :  300\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/Cricket_Z_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "390/390 [==============================] - 1s 4ms/step\n",
            "\n",
            "Final Accuracy :  0.13333334028720856\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 315)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 315, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 315, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 315, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 315, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 315, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 315, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 315, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 315, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 315, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            10368       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 315, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 8)            1096        concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 277,192\n",
            "Trainable params: 276,168\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset uWaveGestureLibrary_X ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/uWaveGestureLibrary_X_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/uWaveGestureLibrary_X_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  896 Number of test samples :  3582\n",
            "Number of classes :  8\n",
            "Sequence length :  315\n",
            "Class weights :  [0.91803279 1.03703704 1.05660377 1.01818182 0.88188976 1.00900901\n",
            " 1.         1.12      ]\n",
            "Train on 896 samples, validate on 3582 samples\n",
            "Epoch 1/4\n",
            " - 25s - loss: 1.9176 - accuracy: 0.2824 - val_loss: 2.0967 - val_accuracy: 0.1248\n",
            "\n",
            "Epoch 00001: loss improved from inf to 1.91761, saving model to ./weights/lstmfcn_8_cells_weights/uWaveGestureLibrary_X_weights.h5\n",
            "Epoch 2/4\n",
            " - 24s - loss: 1.6871 - accuracy: 0.4821 - val_loss: 2.1783 - val_accuracy: 0.2136\n",
            "\n",
            "Epoch 00002: loss improved from 1.91761 to 1.68712, saving model to ./weights/lstmfcn_8_cells_weights/uWaveGestureLibrary_X_weights.h5\n",
            "Epoch 3/4\n",
            " - 24s - loss: 1.5752 - accuracy: 0.5312 - val_loss: 2.0957 - val_accuracy: 0.2962\n",
            "\n",
            "Epoch 00003: loss improved from 1.68712 to 1.57520, saving model to ./weights/lstmfcn_8_cells_weights/uWaveGestureLibrary_X_weights.h5\n",
            "Epoch 4/4\n",
            " - 24s - loss: 1.4664 - accuracy: 0.6083 - val_loss: 2.1251 - val_accuracy: 0.3001\n",
            "\n",
            "Epoch 00004: loss improved from 1.57520 to 1.46638, saving model to ./weights/lstmfcn_8_cells_weights/uWaveGestureLibrary_X_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/uWaveGestureLibrary_X_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/uWaveGestureLibrary_X_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  896 Number of test samples :  3582\n",
            "Number of classes :  8\n",
            "Sequence length :  315\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/uWaveGestureLibrary_X_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "3582/3582 [==============================] - 13s 4ms/step\n",
            "\n",
            "Final Accuracy :  0.30011168122291565\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 315)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 315, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 315, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 315, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 315, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 315, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 315, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 315, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 315, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 315, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            10368       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 315, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 8)            1096        concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 277,192\n",
            "Trainable params: 276,168\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset uWaveGestureLibrary_Y ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/uWaveGestureLibrary_Y_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/uWaveGestureLibrary_Y_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  896 Number of test samples :  3582\n",
            "Number of classes :  8\n",
            "Sequence length :  315\n",
            "Class weights :  [0.91803279 1.03703704 1.05660377 1.01818182 0.88188976 1.00900901\n",
            " 1.         1.12      ]\n",
            "Train on 896 samples, validate on 3582 samples\n",
            "Epoch 1/4\n",
            " - 25s - loss: 1.9005 - accuracy: 0.2879 - val_loss: 2.2223 - val_accuracy: 0.1644\n",
            "\n",
            "Epoch 00001: loss improved from inf to 1.90046, saving model to ./weights/lstmfcn_8_cells_weights/uWaveGestureLibrary_Y_weights.h5\n",
            "Epoch 2/4\n",
            " - 24s - loss: 1.6334 - accuracy: 0.4855 - val_loss: 2.1616 - val_accuracy: 0.1267\n",
            "\n",
            "Epoch 00002: loss improved from 1.90046 to 1.63340, saving model to ./weights/lstmfcn_8_cells_weights/uWaveGestureLibrary_Y_weights.h5\n",
            "Epoch 3/4\n",
            " - 24s - loss: 1.5258 - accuracy: 0.5246 - val_loss: 2.1514 - val_accuracy: 0.1267\n",
            "\n",
            "Epoch 00003: loss improved from 1.63340 to 1.52575, saving model to ./weights/lstmfcn_8_cells_weights/uWaveGestureLibrary_Y_weights.h5\n",
            "Epoch 4/4\n",
            " - 24s - loss: 1.4456 - accuracy: 0.5658 - val_loss: 2.1334 - val_accuracy: 0.1287\n",
            "\n",
            "Epoch 00004: loss improved from 1.52575 to 1.44556, saving model to ./weights/lstmfcn_8_cells_weights/uWaveGestureLibrary_Y_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/uWaveGestureLibrary_Y_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/uWaveGestureLibrary_Y_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  896 Number of test samples :  3582\n",
            "Number of classes :  8\n",
            "Sequence length :  315\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/uWaveGestureLibrary_Y_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "3582/3582 [==============================] - 13s 4ms/step\n",
            "\n",
            "Final Accuracy :  0.12869904935359955\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 315)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 315, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 315, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 315, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 315, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 315, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 315, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 315, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 315, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 315, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            10368       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 315, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 8)            1096        concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 277,192\n",
            "Trainable params: 276,168\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset uWaveGestureLibrary_Z ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/uWaveGestureLibrary_Z_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/uWaveGestureLibrary_Z_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  896 Number of test samples :  3582\n",
            "Number of classes :  8\n",
            "Sequence length :  315\n",
            "Class weights :  [0.91803279 1.03703704 1.05660377 1.01818182 0.88188976 1.00900901\n",
            " 1.         1.12      ]\n",
            "Train on 896 samples, validate on 3582 samples\n",
            "Epoch 1/4\n",
            " - 25s - loss: 1.8855 - accuracy: 0.2824 - val_loss: 2.2191 - val_accuracy: 0.1220\n",
            "\n",
            "Epoch 00001: loss improved from inf to 1.88547, saving model to ./weights/lstmfcn_8_cells_weights/uWaveGestureLibrary_Z_weights.h5\n",
            "Epoch 2/4\n",
            " - 24s - loss: 1.6437 - accuracy: 0.4565 - val_loss: 2.4156 - val_accuracy: 0.1220\n",
            "\n",
            "Epoch 00002: loss improved from 1.88547 to 1.64373, saving model to ./weights/lstmfcn_8_cells_weights/uWaveGestureLibrary_Z_weights.h5\n",
            "Epoch 3/4\n",
            " - 24s - loss: 1.5434 - accuracy: 0.4967 - val_loss: 2.4318 - val_accuracy: 0.1220\n",
            "\n",
            "Epoch 00003: loss improved from 1.64373 to 1.54337, saving model to ./weights/lstmfcn_8_cells_weights/uWaveGestureLibrary_Z_weights.h5\n",
            "Epoch 4/4\n",
            " - 24s - loss: 1.4644 - accuracy: 0.5547 - val_loss: 2.2764 - val_accuracy: 0.1340\n",
            "\n",
            "Epoch 00004: loss improved from 1.54337 to 1.46445, saving model to ./weights/lstmfcn_8_cells_weights/uWaveGestureLibrary_Z_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/uWaveGestureLibrary_Z_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/uWaveGestureLibrary_Z_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  896 Number of test samples :  3582\n",
            "Number of classes :  8\n",
            "Sequence length :  315\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/uWaveGestureLibrary_Z_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "3582/3582 [==============================] - 14s 4ms/step\n",
            "\n",
            "Final Accuracy :  0.13400335609912872\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 343)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 343, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 343, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 343, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 343, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 343, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 343, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 343, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 343, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 343, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            11264       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 343, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 2)            274         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 277,266\n",
            "Trainable params: 276,242\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset ToeSegmentation2 ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ToeSegmentation2_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ToeSegmentation2_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  36 Number of test samples :  130\n",
            "Number of classes :  2\n",
            "Sequence length :  343\n",
            "Class weights :  [1. 1.]\n",
            "Train on 36 samples, validate on 130 samples\n",
            "Epoch 1/4\n",
            " - 2s - loss: 0.6912 - accuracy: 0.5556 - val_loss: 0.7610 - val_accuracy: 0.2077\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.69122, saving model to ./weights/lstmfcn_8_cells_weights/ToeSegmentation2_weights.h5\n",
            "Epoch 2/4\n",
            " - 1s - loss: 0.4561 - accuracy: 0.8889 - val_loss: 0.9935 - val_accuracy: 0.1846\n",
            "\n",
            "Epoch 00002: loss improved from 0.69122 to 0.45608, saving model to ./weights/lstmfcn_8_cells_weights/ToeSegmentation2_weights.h5\n",
            "Epoch 3/4\n",
            " - 1s - loss: 0.3744 - accuracy: 0.9444 - val_loss: 1.0823 - val_accuracy: 0.1846\n",
            "\n",
            "Epoch 00003: loss improved from 0.45608 to 0.37443, saving model to ./weights/lstmfcn_8_cells_weights/ToeSegmentation2_weights.h5\n",
            "Epoch 4/4\n",
            " - 1s - loss: 0.3433 - accuracy: 0.8889 - val_loss: 1.1071 - val_accuracy: 0.1846\n",
            "\n",
            "Epoch 00004: loss improved from 0.37443 to 0.34325, saving model to ./weights/lstmfcn_8_cells_weights/ToeSegmentation2_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ToeSegmentation2_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ToeSegmentation2_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  36 Number of test samples :  130\n",
            "Number of classes :  2\n",
            "Sequence length :  343\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/ToeSegmentation2_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "130/130 [==============================] - 1s 5ms/step\n",
            "\n",
            "Final Accuracy :  0.1846153885126114\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 345)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 345, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 345, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 345, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 345, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 345, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 345, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 345, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 345, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 345, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            11328       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 345, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 4)            548         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 277,604\n",
            "Trainable params: 276,580\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset DiatomSizeReduction ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/DiatomSizeReduction_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/DiatomSizeReduction_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  16 Number of test samples :  306\n",
            "Number of classes :  4\n",
            "Sequence length :  345\n",
            "Class weights :  [4.         0.66666667 0.8        1.        ]\n",
            "Train on 16 samples, validate on 306 samples\n",
            "Epoch 1/4\n",
            " - 2s - loss: 1.5526 - accuracy: 0.0625 - val_loss: 1.5375 - val_accuracy: 0.2843\n",
            "\n",
            "Epoch 00001: loss improved from inf to 1.55259, saving model to ./weights/lstmfcn_8_cells_weights/DiatomSizeReduction_weights.h5\n",
            "Epoch 2/4\n",
            " - 1s - loss: 1.2676 - accuracy: 0.3125 - val_loss: 1.5851 - val_accuracy: 0.2843\n",
            "\n",
            "Epoch 00002: loss improved from 1.55259 to 1.26761, saving model to ./weights/lstmfcn_8_cells_weights/DiatomSizeReduction_weights.h5\n",
            "Epoch 3/4\n",
            " - 1s - loss: 1.1978 - accuracy: 0.6250 - val_loss: 1.5809 - val_accuracy: 0.2843\n",
            "\n",
            "Epoch 00003: loss improved from 1.26761 to 1.19777, saving model to ./weights/lstmfcn_8_cells_weights/DiatomSizeReduction_weights.h5\n",
            "Epoch 4/4\n",
            " - 1s - loss: 1.1871 - accuracy: 0.5625 - val_loss: 1.5717 - val_accuracy: 0.2843\n",
            "\n",
            "Epoch 00004: loss improved from 1.19777 to 1.18708, saving model to ./weights/lstmfcn_8_cells_weights/DiatomSizeReduction_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/DiatomSizeReduction_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/DiatomSizeReduction_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  16 Number of test samples :  306\n",
            "Number of classes :  4\n",
            "Sequence length :  345\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/DiatomSizeReduction_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "306/306 [==============================] - 1s 4ms/step\n",
            "\n",
            "Final Accuracy :  0.28431373834609985\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 577)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 577, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 577, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 577, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 577, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 577, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 577, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 577, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 577, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 577, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            18752       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 577, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 4)            548         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 285,028\n",
            "Trainable params: 284,004\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset car ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Car_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Car_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  60 Number of test samples :  60\n",
            "Number of classes :  4\n",
            "Sequence length :  577\n",
            "Class weights :  [0.9375     0.9375     1.36363636 0.88235294]\n",
            "Train on 60 samples, validate on 60 samples\n",
            "Epoch 1/4\n",
            " - 3s - loss: 1.4365 - accuracy: 0.2333 - val_loss: 1.3924 - val_accuracy: 0.3167\n",
            "\n",
            "Epoch 00001: loss improved from inf to 1.43649, saving model to ./weights/lstmfcn_8_cells_weights/car_weights.h5\n",
            "Epoch 2/4\n",
            " - 2s - loss: 1.3487 - accuracy: 0.4000 - val_loss: 1.3766 - val_accuracy: 0.3167\n",
            "\n",
            "Epoch 00002: loss improved from 1.43649 to 1.34870, saving model to ./weights/lstmfcn_8_cells_weights/car_weights.h5\n",
            "Epoch 3/4\n",
            " - 2s - loss: 1.3214 - accuracy: 0.4667 - val_loss: 1.4416 - val_accuracy: 0.2167\n",
            "\n",
            "Epoch 00003: loss improved from 1.34870 to 1.32142, saving model to ./weights/lstmfcn_8_cells_weights/car_weights.h5\n",
            "Epoch 4/4\n",
            " - 2s - loss: 1.2871 - accuracy: 0.5167 - val_loss: 1.5602 - val_accuracy: 0.3333\n",
            "\n",
            "Epoch 00004: loss improved from 1.32142 to 1.28705, saving model to ./weights/lstmfcn_8_cells_weights/car_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Car_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Car_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  60 Number of test samples :  60\n",
            "Number of classes :  4\n",
            "Sequence length :  577\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/car_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "60/60 [==============================] - 1s 9ms/step\n",
            "\n",
            "Final Accuracy :  0.3333333432674408\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 128)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 128, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 128, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 128, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 128, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 128, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 128, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 128, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 128, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 128, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            4384        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 128, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 3)            411         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 270,523\n",
            "Trainable params: 269,499\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset CBF ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/CBF_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/CBF_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  30 Number of test samples :  900\n",
            "Number of classes :  3\n",
            "Sequence length :  128\n",
            "Class weights :  [1.         0.83333333 1.25      ]\n",
            "Train on 30 samples, validate on 900 samples\n",
            "Epoch 1/4\n",
            " - 3s - loss: 1.1004 - accuracy: 0.3000 - val_loss: 1.0561 - val_accuracy: 0.4189\n",
            "\n",
            "Epoch 00001: loss improved from inf to 1.10041, saving model to ./weights/lstmfcn_8_cells_weights/CBF_weights.h5\n",
            "Epoch 2/4\n",
            " - 1s - loss: 0.8163 - accuracy: 0.7333 - val_loss: 1.0148 - val_accuracy: 0.4078\n",
            "\n",
            "Epoch 00002: loss improved from 1.10041 to 0.81628, saving model to ./weights/lstmfcn_8_cells_weights/CBF_weights.h5\n",
            "Epoch 3/4\n",
            " - 1s - loss: 0.6510 - accuracy: 0.7667 - val_loss: 1.0041 - val_accuracy: 0.3856\n",
            "\n",
            "Epoch 00003: loss improved from 0.81628 to 0.65098, saving model to ./weights/lstmfcn_8_cells_weights/CBF_weights.h5\n",
            "Epoch 4/4\n",
            " - 2s - loss: 0.5360 - accuracy: 0.9000 - val_loss: 1.0228 - val_accuracy: 0.3567\n",
            "\n",
            "Epoch 00004: loss improved from 0.65098 to 0.53604, saving model to ./weights/lstmfcn_8_cells_weights/CBF_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/CBF_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/CBF_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  30 Number of test samples :  900\n",
            "Number of classes :  3\n",
            "Sequence length :  128\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/CBF_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "900/900 [==============================] - 1s 2ms/step\n",
            "\n",
            "Final Accuracy :  0.3566666543483734\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 1639)      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 1639, 1)      0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 1639, 128)    1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 1639, 128)    512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 1639, 128)    0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 1639, 256)    164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 1639, 256)    1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 1639, 256)    0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 1639, 128)    98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 1639, 128)    512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            52736       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 1639, 128)    0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 4)            548         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 319,012\n",
            "Trainable params: 317,988\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset CinC_ECG_torso ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/CinC_ECG_torso_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/CinC_ECG_torso_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  40 Number of test samples :  1380\n",
            "Number of classes :  4\n",
            "Sequence length :  1639\n",
            "Class weights :  [2.         0.76923077 0.83333333 1.        ]\n",
            "Train on 40 samples, validate on 1380 samples\n",
            "Epoch 1/4\n",
            " - 30s - loss: 1.3709 - accuracy: 0.2750 - val_loss: 1.3807 - val_accuracy: 0.2493\n",
            "\n",
            "Epoch 00001: loss improved from inf to 1.37088, saving model to ./weights/lstmfcn_8_cells_weights/CinC_ECG_torso_weights.h5\n",
            "Epoch 2/4\n",
            " - 29s - loss: 1.1908 - accuracy: 0.5500 - val_loss: 1.3863 - val_accuracy: 0.2486\n",
            "\n",
            "Epoch 00002: loss improved from 1.37088 to 1.19081, saving model to ./weights/lstmfcn_8_cells_weights/CinC_ECG_torso_weights.h5\n",
            "Epoch 3/4\n",
            " - 29s - loss: 1.0955 - accuracy: 0.7000 - val_loss: 1.4246 - val_accuracy: 0.2529\n",
            "\n",
            "Epoch 00003: loss improved from 1.19081 to 1.09546, saving model to ./weights/lstmfcn_8_cells_weights/CinC_ECG_torso_weights.h5\n",
            "Epoch 4/4\n",
            " - 29s - loss: 1.0432 - accuracy: 0.6500 - val_loss: 1.4734 - val_accuracy: 0.3007\n",
            "\n",
            "Epoch 00004: loss improved from 1.09546 to 1.04318, saving model to ./weights/lstmfcn_8_cells_weights/CinC_ECG_torso_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/CinC_ECG_torso_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/CinC_ECG_torso_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  40 Number of test samples :  1380\n",
            "Number of classes :  4\n",
            "Sequence length :  1639\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/CinC_ECG_torso_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "1380/1380 [==============================] - 26s 19ms/step\n",
            "\n",
            "Final Accuracy :  0.3007246255874634\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 720)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 720, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 720, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 720, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 720, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 720, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 720, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 720, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 720, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 720, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            23328       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 720, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 2)            274         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 289,330\n",
            "Trainable params: 288,306\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset Computers ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Computers_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Computers_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  250 Number of test samples :  250\n",
            "Number of classes :  2\n",
            "Sequence length :  720\n",
            "Class weights :  [1. 1.]\n",
            "Train on 250 samples, validate on 250 samples\n",
            "Epoch 1/4\n",
            " - 10s - loss: 0.6405 - accuracy: 0.6040 - val_loss: 0.9707 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.64052, saving model to ./weights/lstmfcn_8_cells_weights/Computers_weights.h5\n",
            "Epoch 2/4\n",
            " - 9s - loss: 0.6020 - accuracy: 0.6400 - val_loss: 1.5598 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00002: loss improved from 0.64052 to 0.60195, saving model to ./weights/lstmfcn_8_cells_weights/Computers_weights.h5\n",
            "Epoch 3/4\n",
            " - 9s - loss: 0.5769 - accuracy: 0.6600 - val_loss: 2.1174 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00003: loss improved from 0.60195 to 0.57686, saving model to ./weights/lstmfcn_8_cells_weights/Computers_weights.h5\n",
            "Epoch 4/4\n",
            " - 9s - loss: 0.5466 - accuracy: 0.7080 - val_loss: 2.6652 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00004: loss improved from 0.57686 to 0.54655, saving model to ./weights/lstmfcn_8_cells_weights/Computers_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Computers_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Computers_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  250 Number of test samples :  250\n",
            "Number of classes :  2\n",
            "Sequence length :  720\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/Computers_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "250/250 [==============================] - 2s 9ms/step\n",
            "\n",
            "Final Accuracy :  0.5\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 512)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 512, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 512, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 512, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 512, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 512, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 512, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 512, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 512, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 512, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            16672       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 512, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 2)            274         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 282,674\n",
            "Trainable params: 281,650\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset Earthquakes ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Earthquakes_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Earthquakes_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  139 Number of test samples :  322\n",
            "Number of classes :  2\n",
            "Sequence length :  512\n",
            "Class weights :  [0.66826923 1.98571429]\n",
            "Train on 139 samples, validate on 322 samples\n",
            "Epoch 1/4\n",
            " - 6s - loss: 0.6067 - accuracy: 0.7410 - val_loss: 0.6824 - val_accuracy: 0.8199\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.60670, saving model to ./weights/lstmfcn_8_cells_weights/Earthquakes_weights.h5\n",
            "Epoch 2/4\n",
            " - 5s - loss: 0.5274 - accuracy: 0.6835 - val_loss: 0.8761 - val_accuracy: 0.8199\n",
            "\n",
            "Epoch 00002: loss improved from 0.60670 to 0.52741, saving model to ./weights/lstmfcn_8_cells_weights/Earthquakes_weights.h5\n",
            "Epoch 3/4\n",
            " - 5s - loss: 0.5151 - accuracy: 0.7482 - val_loss: 0.9846 - val_accuracy: 0.8199\n",
            "\n",
            "Epoch 00003: loss improved from 0.52741 to 0.51507, saving model to ./weights/lstmfcn_8_cells_weights/Earthquakes_weights.h5\n",
            "Epoch 4/4\n",
            " - 5s - loss: 0.5142 - accuracy: 0.7482 - val_loss: 1.1339 - val_accuracy: 0.8199\n",
            "\n",
            "Epoch 00004: loss improved from 0.51507 to 0.51425, saving model to ./weights/lstmfcn_8_cells_weights/Earthquakes_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Earthquakes_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Earthquakes_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  139 Number of test samples :  322\n",
            "Number of classes :  2\n",
            "Sequence length :  512\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/Earthquakes_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "322/322 [==============================] - 2s 6ms/step\n",
            "\n",
            "Final Accuracy :  0.8198757767677307\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 140)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 140, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 140, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 140, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 140, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 140, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 140, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 140, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 140, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 140, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            4768        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 140, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 5)            685         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 271,181\n",
            "Trainable params: 270,157\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset ECG5000 ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ECG5000_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ECG5000_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  500 Number of test samples :  4500\n",
            "Number of classes :  5\n",
            "Sequence length :  140\n",
            "Class weights :  [ 0.34246575  0.56497175 10.          5.26315789 50.        ]\n",
            "Train on 500 samples, validate on 4500 samples\n",
            "Epoch 1/4\n",
            " - 11s - loss: 1.1576 - accuracy: 0.6640 - val_loss: 0.8968 - val_accuracy: 0.5913\n",
            "\n",
            "Epoch 00001: loss improved from inf to 1.15759, saving model to ./weights/lstmfcn_8_cells_weights/ECG5000_weights.h5\n",
            "Epoch 2/4\n",
            " - 10s - loss: 0.6492 - accuracy: 0.8940 - val_loss: 0.6653 - val_accuracy: 0.9062\n",
            "\n",
            "Epoch 00002: loss improved from 1.15759 to 0.64915, saving model to ./weights/lstmfcn_8_cells_weights/ECG5000_weights.h5\n",
            "Epoch 3/4\n",
            " - 10s - loss: 0.5033 - accuracy: 0.8960 - val_loss: 0.8811 - val_accuracy: 0.4216\n",
            "\n",
            "Epoch 00003: loss improved from 0.64915 to 0.50331, saving model to ./weights/lstmfcn_8_cells_weights/ECG5000_weights.h5\n",
            "Epoch 4/4\n",
            " - 10s - loss: 0.4219 - accuracy: 0.9120 - val_loss: 1.4442 - val_accuracy: 0.3538\n",
            "\n",
            "Epoch 00004: loss improved from 0.50331 to 0.42189, saving model to ./weights/lstmfcn_8_cells_weights/ECG5000_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ECG5000_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ECG5000_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  500 Number of test samples :  4500\n",
            "Number of classes :  5\n",
            "Sequence length :  140\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/ECG5000_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "4500/4500 [==============================] - 8s 2ms/step\n",
            "\n",
            "Final Accuracy :  0.35377776622772217\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 96)        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 96, 1)        0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 96, 128)      1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 96, 128)      512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 96, 128)      0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 96, 256)      164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 96, 256)      1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 96, 256)      0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 96, 128)      98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 96, 128)      512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            3360        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 96, 128)      0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 7)            959         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 270,047\n",
            "Trainable params: 269,023\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset ElectricDevices ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ElectricDevices_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ElectricDevices_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  8926 Number of test samples :  7711\n",
            "Number of classes :  7\n",
            "Sequence length :  96\n",
            "Class weights :  [1.75397917 0.57155664 1.49840524 0.86509013 0.52998456 2.50519225\n",
            " 1.75156986]\n",
            "Train on 8926 samples, validate on 7711 samples\n",
            "Epoch 1/4\n",
            " - 46s - loss: 0.9461 - accuracy: 0.6657 - val_loss: 1.5251 - val_accuracy: 0.5710\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.94606, saving model to ./weights/lstmfcn_8_cells_weights/ElectricDevices_weights.h5\n",
            "Epoch 2/4\n",
            " - 44s - loss: 0.6829 - accuracy: 0.7527 - val_loss: 1.6537 - val_accuracy: 0.6067\n",
            "\n",
            "Epoch 00002: loss improved from 0.94606 to 0.68285, saving model to ./weights/lstmfcn_8_cells_weights/ElectricDevices_weights.h5\n",
            "Epoch 3/4\n",
            " - 44s - loss: 0.5849 - accuracy: 0.7911 - val_loss: 1.6155 - val_accuracy: 0.5881\n",
            "\n",
            "Epoch 00003: loss improved from 0.68285 to 0.58487, saving model to ./weights/lstmfcn_8_cells_weights/ElectricDevices_weights.h5\n",
            "Epoch 4/4\n",
            " - 45s - loss: 0.5404 - accuracy: 0.8091 - val_loss: 1.5410 - val_accuracy: 0.6239\n",
            "\n",
            "Epoch 00004: loss improved from 0.58487 to 0.54037, saving model to ./weights/lstmfcn_8_cells_weights/ElectricDevices_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ElectricDevices_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ElectricDevices_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  8926 Number of test samples :  7711\n",
            "Number of classes :  7\n",
            "Sequence length :  96\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/ElectricDevices_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "7711/7711 [==============================] - 9s 1ms/step\n",
            "\n",
            "Final Accuracy :  0.6239138841629028\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 131)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 131, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 131, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 131, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 131, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 131, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 131, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 131, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 131, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 131, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            4480        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 131, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 14)           1918        concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 272,126\n",
            "Trainable params: 271,102\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset FaceAll ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/FaceAll_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/FaceAll_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  560 Number of test samples :  1690\n",
            "Number of classes :  14\n",
            "Sequence length :  131\n",
            "Class weights :  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "Train on 560 samples, validate on 1690 samples\n",
            "Epoch 1/4\n",
            " - 7s - loss: 2.5786 - accuracy: 0.1571 - val_loss: 2.6519 - val_accuracy: 0.1521\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.57864, saving model to ./weights/lstmfcn_8_cells_weights/FaceAll_weights.h5\n",
            "Epoch 2/4\n",
            " - 6s - loss: 2.2310 - accuracy: 0.3875 - val_loss: 2.6748 - val_accuracy: 0.0775\n",
            "\n",
            "Epoch 00002: loss improved from 2.57864 to 2.23102, saving model to ./weights/lstmfcn_8_cells_weights/FaceAll_weights.h5\n",
            "Epoch 3/4\n",
            " - 6s - loss: 2.0664 - accuracy: 0.5357 - val_loss: 2.6214 - val_accuracy: 0.0817\n",
            "\n",
            "Epoch 00003: loss improved from 2.23102 to 2.06635, saving model to ./weights/lstmfcn_8_cells_weights/FaceAll_weights.h5\n",
            "Epoch 4/4\n",
            " - 6s - loss: 1.9220 - accuracy: 0.6179 - val_loss: 2.5828 - val_accuracy: 0.1178\n",
            "\n",
            "Epoch 00004: loss improved from 2.06635 to 1.92201, saving model to ./weights/lstmfcn_8_cells_weights/FaceAll_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/FaceAll_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/FaceAll_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  560 Number of test samples :  1690\n",
            "Number of classes :  14\n",
            "Sequence length :  131\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/FaceAll_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "1690/1690 [==============================] - 3s 2ms/step\n",
            "\n",
            "Final Accuracy :  0.11775147914886475\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 350)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 350, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 350, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 350, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 350, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 350, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 350, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 350, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 350, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 350, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            11488       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 350, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 4)            548         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 277,764\n",
            "Trainable params: 276,740\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset FaceFour ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/FaceFour_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/FaceFour_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  24 Number of test samples :  88\n",
            "Number of classes :  4\n",
            "Sequence length :  350\n",
            "Class weights :  [0.75 0.75 2.   1.2 ]\n",
            "Train on 24 samples, validate on 88 samples\n",
            "Epoch 1/4\n",
            " - 2s - loss: 1.4520 - accuracy: 0.2083 - val_loss: 1.5792 - val_accuracy: 0.2955\n",
            "\n",
            "Epoch 00001: loss improved from inf to 1.45195, saving model to ./weights/lstmfcn_8_cells_weights/FaceFour_weights.h5\n",
            "Epoch 2/4\n",
            " - 1s - loss: 1.2254 - accuracy: 0.6250 - val_loss: 1.6590 - val_accuracy: 0.3068\n",
            "\n",
            "Epoch 00002: loss improved from 1.45195 to 1.22536, saving model to ./weights/lstmfcn_8_cells_weights/FaceFour_weights.h5\n",
            "Epoch 3/4\n",
            " - 1s - loss: 1.0646 - accuracy: 0.7083 - val_loss: 1.7197 - val_accuracy: 0.3750\n",
            "\n",
            "Epoch 00003: loss improved from 1.22536 to 1.06464, saving model to ./weights/lstmfcn_8_cells_weights/FaceFour_weights.h5\n",
            "Epoch 4/4\n",
            " - 1s - loss: 0.9737 - accuracy: 0.7917 - val_loss: 1.7520 - val_accuracy: 0.4091\n",
            "\n",
            "Epoch 00004: loss improved from 1.06464 to 0.97371, saving model to ./weights/lstmfcn_8_cells_weights/FaceFour_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/FaceFour_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/FaceFour_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  24 Number of test samples :  88\n",
            "Number of classes :  4\n",
            "Sequence length :  350\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/FaceFour_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "88/88 [==============================] - 1s 6ms/step\n",
            "\n",
            "Final Accuracy :  0.40909090638160706\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 131)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 131, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 131, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 131, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 131, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 131, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 131, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 131, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 131, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 131, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            4480        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 131, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 14)           1918        concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 272,126\n",
            "Trainable params: 271,102\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset FacesUCR ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/FacesUCR_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/FacesUCR_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  200 Number of test samples :  2050\n",
            "Number of classes :  14\n",
            "Sequence length :  131\n",
            "Class weights :  [1.0989011  1.19047619 1.42857143 0.64935065 0.95238095 1.0989011\n",
            " 1.02040816 0.52910053 1.19047619 1.78571429 3.57142857 1.2987013\n",
            " 0.43290043 2.38095238]\n",
            "Train on 200 samples, validate on 2050 samples\n",
            "Epoch 1/4\n",
            " - 5s - loss: 2.7134 - accuracy: 0.0650 - val_loss: 2.9157 - val_accuracy: 0.1210\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.71339, saving model to ./weights/lstmfcn_8_cells_weights/FacesUCR_weights.h5\n",
            "Epoch 2/4\n",
            " - 4s - loss: 2.3607 - accuracy: 0.1850 - val_loss: 2.8740 - val_accuracy: 0.1200\n",
            "\n",
            "Epoch 00002: loss improved from 2.71339 to 2.36072, saving model to ./weights/lstmfcn_8_cells_weights/FacesUCR_weights.h5\n",
            "Epoch 3/4\n",
            " - 4s - loss: 2.1578 - accuracy: 0.3500 - val_loss: 2.9019 - val_accuracy: 0.1200\n",
            "\n",
            "Epoch 00003: loss improved from 2.36072 to 2.15776, saving model to ./weights/lstmfcn_8_cells_weights/FacesUCR_weights.h5\n",
            "Epoch 4/4\n",
            " - 4s - loss: 1.9949 - accuracy: 0.4250 - val_loss: 2.9367 - val_accuracy: 0.1200\n",
            "\n",
            "Epoch 00004: loss improved from 2.15776 to 1.99488, saving model to ./weights/lstmfcn_8_cells_weights/FacesUCR_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/FacesUCR_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/FacesUCR_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  200 Number of test samples :  2050\n",
            "Number of classes :  14\n",
            "Sequence length :  131\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/FacesUCR_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "2050/2050 [==============================] - 3s 2ms/step\n",
            "\n",
            "Final Accuracy :  0.11999999731779099\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 463)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 463, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 463, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 463, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 463, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 463, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 463, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 463, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 463, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 463, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            15104       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 463, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 7)            959         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 281,791\n",
            "Trainable params: 280,767\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset Fish ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/FISH_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/FISH_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  175 Number of test samples :  175\n",
            "Number of classes :  7\n",
            "Sequence length :  463\n",
            "Class weights :  [0.96153846 1.         0.89285714 1.19047619 1.13636364 1.\n",
            " 0.89285714]\n",
            "Train on 175 samples, validate on 175 samples\n",
            "Epoch 1/4\n",
            " - 5s - loss: 2.0040 - accuracy: 0.1429 - val_loss: 2.1814 - val_accuracy: 0.1257\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.00398, saving model to ./weights/lstmfcn_8_cells_weights/Fish_weights.h5\n",
            "Epoch 2/4\n",
            " - 4s - loss: 1.9242 - accuracy: 0.1886 - val_loss: 2.2044 - val_accuracy: 0.1257\n",
            "\n",
            "Epoch 00002: loss improved from 2.00398 to 1.92418, saving model to ./weights/lstmfcn_8_cells_weights/Fish_weights.h5\n",
            "Epoch 3/4\n",
            " - 4s - loss: 1.8867 - accuracy: 0.3029 - val_loss: 2.4809 - val_accuracy: 0.1429\n",
            "\n",
            "Epoch 00003: loss improved from 1.92418 to 1.88668, saving model to ./weights/lstmfcn_8_cells_weights/Fish_weights.h5\n",
            "Epoch 4/4\n",
            " - 4s - loss: 1.8695 - accuracy: 0.3429 - val_loss: 2.8906 - val_accuracy: 0.1429\n",
            "\n",
            "Epoch 00004: loss improved from 1.88668 to 1.86946, saving model to ./weights/lstmfcn_8_cells_weights/Fish_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/FISH_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/FISH_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  175 Number of test samples :  175\n",
            "Number of classes :  7\n",
            "Sequence length :  463\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/Fish_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "175/175 [==============================] - 1s 6ms/step\n",
            "\n",
            "Final Accuracy :  0.1428571492433548\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 500)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 500, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 500, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 500, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 500, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 500, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 500, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 500, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 500, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 500, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            16288       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 500, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 2)            274         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 282,290\n",
            "Trainable params: 281,266\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset FordA ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/FordA_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/FordA_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  1320 Number of test samples :  3601\n",
            "Number of classes :  2\n",
            "Sequence length :  500\n",
            "Class weights :  [0.969163   1.03286385]\n",
            "Train on 1320 samples, validate on 3601 samples\n",
            "Epoch 1/4\n",
            " - 47s - loss: 0.5375 - accuracy: 0.6985 - val_loss: 0.8048 - val_accuracy: 0.4929\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.53748, saving model to ./weights/lstmfcn_8_cells_weights/FordA_weights.h5\n",
            "Epoch 2/4\n",
            " - 50s - loss: 0.4039 - accuracy: 0.8008 - val_loss: 1.1947 - val_accuracy: 0.4896\n",
            "\n",
            "Epoch 00002: loss improved from 0.53748 to 0.40386, saving model to ./weights/lstmfcn_8_cells_weights/FordA_weights.h5\n",
            "Epoch 3/4\n",
            " - 46s - loss: 0.3605 - accuracy: 0.8280 - val_loss: 1.5398 - val_accuracy: 0.4874\n",
            "\n",
            "Epoch 00003: loss improved from 0.40386 to 0.36054, saving model to ./weights/lstmfcn_8_cells_weights/FordA_weights.h5\n",
            "Epoch 4/4\n",
            " - 46s - loss: 0.3336 - accuracy: 0.8500 - val_loss: 1.4881 - val_accuracy: 0.4879\n",
            "\n",
            "Epoch 00004: loss improved from 0.36054 to 0.33358, saving model to ./weights/lstmfcn_8_cells_weights/FordA_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/FordA_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/FordA_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  1320 Number of test samples :  3601\n",
            "Number of classes :  2\n",
            "Sequence length :  500\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/FordA_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "3601/3601 [==============================] - 22s 6ms/step\n",
            "\n",
            "Final Accuracy :  0.48792001605033875\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 500)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 500, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 500, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 500, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 500, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 500, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 500, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 500, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 500, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 500, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            16288       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 500, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 2)            274         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 282,290\n",
            "Trainable params: 281,266\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset FordB ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/FordB_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/FordB_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  810 Number of test samples :  3636\n",
            "Number of classes :  2\n",
            "Sequence length :  500\n",
            "Class weights :  [1.00997506 0.99022005]\n",
            "Train on 810 samples, validate on 3636 samples\n",
            "Epoch 1/4\n",
            " - 38s - loss: 0.8214 - accuracy: 0.5012 - val_loss: 1.0302 - val_accuracy: 0.5116\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.82136, saving model to ./weights/lstmfcn_8_cells_weights/FordB_weights.h5\n",
            "Epoch 2/4\n",
            " - 37s - loss: 0.6197 - accuracy: 0.6321 - val_loss: 0.7659 - val_accuracy: 0.5171\n",
            "\n",
            "Epoch 00002: loss improved from 0.82136 to 0.61973, saving model to ./weights/lstmfcn_8_cells_weights/FordB_weights.h5\n",
            "Epoch 3/4\n",
            " - 36s - loss: 0.6039 - accuracy: 0.6383 - val_loss: 0.6929 - val_accuracy: 0.5897\n",
            "\n",
            "Epoch 00003: loss improved from 0.61973 to 0.60390, saving model to ./weights/lstmfcn_8_cells_weights/FordB_weights.h5\n",
            "Epoch 4/4\n",
            " - 37s - loss: 0.5657 - accuracy: 0.6864 - val_loss: 0.6047 - val_accuracy: 0.5993\n",
            "\n",
            "Epoch 00004: loss improved from 0.60390 to 0.56567, saving model to ./weights/lstmfcn_8_cells_weights/FordB_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/FordB_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/FordB_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  810 Number of test samples :  3636\n",
            "Number of classes :  2\n",
            "Sequence length :  500\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/FordB_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "3636/3636 [==============================] - 21s 6ms/step\n",
            "\n",
            "Final Accuracy :  0.5992849469184875\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 150)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 150, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 150, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 150, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 150, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 150, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 150, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 150, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 150, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 150, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            5088        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 150, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 2)            274         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 271,090\n",
            "Trainable params: 270,066\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset Gun_Point ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Gun_Point_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Gun_Point_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  50 Number of test samples :  150\n",
            "Number of classes :  2\n",
            "Sequence length :  150\n",
            "Class weights :  [1.04166667 0.96153846]\n",
            "Train on 50 samples, validate on 150 samples\n",
            "Epoch 1/4\n",
            " - 2s - loss: 0.7677 - accuracy: 0.5000 - val_loss: 0.7415 - val_accuracy: 0.4933\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.76768, saving model to ./weights/lstmfcn_8_cells_weights/Gun_Point_weights.h5\n",
            "Epoch 2/4\n",
            " - 1s - loss: 0.6678 - accuracy: 0.7200 - val_loss: 0.7367 - val_accuracy: 0.4933\n",
            "\n",
            "Epoch 00002: loss improved from 0.76768 to 0.66782, saving model to ./weights/lstmfcn_8_cells_weights/Gun_Point_weights.h5\n",
            "Epoch 3/4\n",
            " - 1s - loss: 0.5554 - accuracy: 0.7400 - val_loss: 0.7782 - val_accuracy: 0.4933\n",
            "\n",
            "Epoch 00003: loss improved from 0.66782 to 0.55537, saving model to ./weights/lstmfcn_8_cells_weights/Gun_Point_weights.h5\n",
            "Epoch 4/4\n",
            " - 1s - loss: 0.5329 - accuracy: 0.7400 - val_loss: 0.8285 - val_accuracy: 0.4933\n",
            "\n",
            "Epoch 00004: loss improved from 0.55537 to 0.53291, saving model to ./weights/lstmfcn_8_cells_weights/Gun_Point_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Gun_Point_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Gun_Point_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  50 Number of test samples :  150\n",
            "Number of classes :  2\n",
            "Sequence length :  150\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/Gun_Point_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "150/150 [==============================] - 0s 3ms/step\n",
            "\n",
            "Final Accuracy :  0.4933333396911621\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 431)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 431, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 431, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 431, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 431, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 431, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 431, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 431, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 431, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 431, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            14080       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 431, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 2)            274         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 280,082\n",
            "Trainable params: 279,058\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset Ham ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Ham_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Ham_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  109 Number of test samples :  105\n",
            "Number of classes :  2\n",
            "Sequence length :  431\n",
            "Class weights :  [1.04807692 0.95614035]\n",
            "Train on 109 samples, validate on 105 samples\n",
            "Epoch 1/4\n",
            " - 3s - loss: 0.7222 - accuracy: 0.4587 - val_loss: 1.0074 - val_accuracy: 0.5143\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.72220, saving model to ./weights/lstmfcn_8_cells_weights/Ham_weights.h5\n",
            "Epoch 2/4\n",
            " - 2s - loss: 0.6705 - accuracy: 0.6147 - val_loss: 0.9281 - val_accuracy: 0.5143\n",
            "\n",
            "Epoch 00002: loss improved from 0.72220 to 0.67053, saving model to ./weights/lstmfcn_8_cells_weights/Ham_weights.h5\n",
            "Epoch 3/4\n",
            " - 2s - loss: 0.6456 - accuracy: 0.6239 - val_loss: 0.8946 - val_accuracy: 0.5143\n",
            "\n",
            "Epoch 00003: loss improved from 0.67053 to 0.64565, saving model to ./weights/lstmfcn_8_cells_weights/Ham_weights.h5\n",
            "Epoch 4/4\n",
            " - 2s - loss: 0.6082 - accuracy: 0.6789 - val_loss: 0.8881 - val_accuracy: 0.5143\n",
            "\n",
            "Epoch 00004: loss improved from 0.64565 to 0.60819, saving model to ./weights/lstmfcn_8_cells_weights/Ham_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Ham_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Ham_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  109 Number of test samples :  105\n",
            "Number of classes :  2\n",
            "Sequence length :  431\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/Ham_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "105/105 [==============================] - 1s 6ms/step\n",
            "\n",
            "Final Accuracy :  0.5142857432365417\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 2709)      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 2709, 1)      0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 2709, 128)    1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 2709, 128)    512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 2709, 128)    0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 2709, 256)    164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 2709, 256)    1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 2709, 256)    0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 2709, 128)    98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 2709, 128)    512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            86976       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 2709, 128)    0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 2)            274         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 352,978\n",
            "Trainable params: 351,954\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset HandOutlines ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/HandOutlines_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/HandOutlines_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  370 Number of test samples :  1000\n",
            "Number of classes :  2\n",
            "Sequence length :  2709\n",
            "Class weights :  [1.39097744 0.78059072]\n",
            "Train on 370 samples, validate on 1000 samples\n",
            "Epoch 1/4\n",
            " - 71s - loss: 0.7178 - accuracy: 0.5270 - val_loss: 0.6866 - val_accuracy: 0.3620\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.71783, saving model to ./weights/lstmfcn_8_cells_weights/HandOutlines_weights.h5\n",
            "Epoch 2/4\n",
            " - 69s - loss: 0.6718 - accuracy: 0.6162 - val_loss: 0.8466 - val_accuracy: 0.3620\n",
            "\n",
            "Epoch 00002: loss improved from 0.71783 to 0.67181, saving model to ./weights/lstmfcn_8_cells_weights/HandOutlines_weights.h5\n",
            "Epoch 3/4\n",
            " - 69s - loss: 0.6379 - accuracy: 0.6541 - val_loss: 0.7233 - val_accuracy: 0.3620\n",
            "\n",
            "Epoch 00003: loss improved from 0.67181 to 0.63789, saving model to ./weights/lstmfcn_8_cells_weights/HandOutlines_weights.h5\n",
            "Epoch 4/4\n",
            " - 69s - loss: 0.6058 - accuracy: 0.6838 - val_loss: 0.8207 - val_accuracy: 0.3620\n",
            "\n",
            "Epoch 00004: loss improved from 0.63789 to 0.60578, saving model to ./weights/lstmfcn_8_cells_weights/HandOutlines_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/HandOutlines_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/HandOutlines_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  370 Number of test samples :  1000\n",
            "Number of classes :  2\n",
            "Sequence length :  2709\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/HandOutlines_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "1000/1000 [==============================] - 34s 34ms/step\n",
            "\n",
            "Final Accuracy :  0.3619999885559082\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 1092)      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 1092, 1)      0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 1092, 128)    1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 1092, 128)    512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 1092, 128)    0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 1092, 256)    164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 1092, 256)    1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 1092, 256)    0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 1092, 128)    98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 1092, 128)    512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            35232       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 1092, 128)    0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 5)            685         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 301,645\n",
            "Trainable params: 300,621\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset Haptics ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Haptics_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Haptics_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  155 Number of test samples :  308\n",
            "Number of classes :  5\n",
            "Sequence length :  1092\n",
            "Class weights :  [1.72222222 0.91176471 0.91176471 0.86111111 0.93939394]\n",
            "Train on 155 samples, validate on 308 samples\n",
            "Epoch 1/4\n",
            " - 12s - loss: 1.5997 - accuracy: 0.1871 - val_loss: 2.1765 - val_accuracy: 0.1883\n",
            "\n",
            "Epoch 00001: loss improved from inf to 1.59972, saving model to ./weights/lstmfcn_8_cells_weights/Haptics_weights.h5\n",
            "Epoch 2/4\n",
            " - 10s - loss: 1.5742 - accuracy: 0.2452 - val_loss: 2.4417 - val_accuracy: 0.1883\n",
            "\n",
            "Epoch 00002: loss improved from 1.59972 to 1.57418, saving model to ./weights/lstmfcn_8_cells_weights/Haptics_weights.h5\n",
            "Epoch 3/4\n",
            " - 10s - loss: 1.5304 - accuracy: 0.3419 - val_loss: 2.6002 - val_accuracy: 0.1883\n",
            "\n",
            "Epoch 00003: loss improved from 1.57418 to 1.53044, saving model to ./weights/lstmfcn_8_cells_weights/Haptics_weights.h5\n",
            "Epoch 4/4\n",
            " - 11s - loss: 1.5285 - accuracy: 0.3677 - val_loss: 2.6887 - val_accuracy: 0.1883\n",
            "\n",
            "Epoch 00004: loss improved from 1.53044 to 1.52855, saving model to ./weights/lstmfcn_8_cells_weights/Haptics_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Haptics_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Haptics_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  155 Number of test samples :  308\n",
            "Number of classes :  5\n",
            "Sequence length :  1092\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/Haptics_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "308/308 [==============================] - 4s 13ms/step\n",
            "\n",
            "Final Accuracy :  0.18831168115139008\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 512)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 512, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 512, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 512, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 512, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 512, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 512, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 512, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 512, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 512, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            16672       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 512, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 2)            274         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 282,674\n",
            "Trainable params: 281,650\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset Herring ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Herring_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Herring_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  64 Number of test samples :  64\n",
            "Number of classes :  2\n",
            "Sequence length :  512\n",
            "Class weights :  [0.82051282 1.28      ]\n",
            "Train on 64 samples, validate on 64 samples\n",
            "Epoch 1/4\n",
            " - 3s - loss: 0.6838 - accuracy: 0.6094 - val_loss: 1.3750 - val_accuracy: 0.4062\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.68376, saving model to ./weights/lstmfcn_8_cells_weights/Herring_weights.h5\n",
            "Epoch 2/4\n",
            " - 2s - loss: 0.6773 - accuracy: 0.6250 - val_loss: 0.9797 - val_accuracy: 0.4062\n",
            "\n",
            "Epoch 00002: loss improved from 0.68376 to 0.67726, saving model to ./weights/lstmfcn_8_cells_weights/Herring_weights.h5\n",
            "Epoch 3/4\n",
            " - 2s - loss: 0.6746 - accuracy: 0.6094 - val_loss: 0.7827 - val_accuracy: 0.4062\n",
            "\n",
            "Epoch 00003: loss improved from 0.67726 to 0.67456, saving model to ./weights/lstmfcn_8_cells_weights/Herring_weights.h5\n",
            "Epoch 4/4\n",
            " - 2s - loss: 0.6688 - accuracy: 0.6094 - val_loss: 0.7246 - val_accuracy: 0.4062\n",
            "\n",
            "Epoch 00004: loss improved from 0.67456 to 0.66879, saving model to ./weights/lstmfcn_8_cells_weights/Herring_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Herring_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Herring_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  64 Number of test samples :  64\n",
            "Number of classes :  2\n",
            "Sequence length :  512\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/Herring_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "64/64 [==============================] - 1s 8ms/step\n",
            "\n",
            "Final Accuracy :  0.40625\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 1882)      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 1882, 1)      0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 1882, 128)    1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 1882, 128)    512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 1882, 128)    0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 1882, 256)    164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 1882, 256)    1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 1882, 256)    0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 1882, 128)    98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 1882, 128)    512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            60512       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 1882, 128)    0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 7)            959         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 327,199\n",
            "Trainable params: 326,175\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset InlineSkate ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/InlineSkate_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/InlineSkate_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  100 Number of test samples :  550\n",
            "Number of classes :  7\n",
            "Sequence length :  1882\n",
            "Class weights :  [1.58730159 1.02040816 0.79365079 0.79365079 0.89285714 1.02040816\n",
            " 1.2987013 ]\n",
            "Train on 100 samples, validate on 550 samples\n",
            "Epoch 1/4\n",
            " - 20s - loss: 2.0482 - accuracy: 0.1100 - val_loss: 2.0256 - val_accuracy: 0.1600\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.04821, saving model to ./weights/lstmfcn_8_cells_weights/InlineSkate_weights.h5\n",
            "Epoch 2/4\n",
            " - 19s - loss: 1.9574 - accuracy: 0.1500 - val_loss: 2.0001 - val_accuracy: 0.1545\n",
            "\n",
            "Epoch 00002: loss improved from 2.04821 to 1.95737, saving model to ./weights/lstmfcn_8_cells_weights/InlineSkate_weights.h5\n",
            "Epoch 3/4\n",
            " - 19s - loss: 1.9247 - accuracy: 0.2000 - val_loss: 1.9945 - val_accuracy: 0.1545\n",
            "\n",
            "Epoch 00003: loss improved from 1.95737 to 1.92474, saving model to ./weights/lstmfcn_8_cells_weights/InlineSkate_weights.h5\n",
            "Epoch 4/4\n",
            " - 19s - loss: 1.8551 - accuracy: 0.2900 - val_loss: 1.9990 - val_accuracy: 0.1545\n",
            "\n",
            "Epoch 00004: loss improved from 1.92474 to 1.85509, saving model to ./weights/lstmfcn_8_cells_weights/InlineSkate_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/InlineSkate_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/InlineSkate_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  100 Number of test samples :  550\n",
            "Number of classes :  7\n",
            "Sequence length :  1882\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/InlineSkate_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "550/550 [==============================] - 12s 22ms/step\n",
            "\n",
            "Final Accuracy :  0.15454545617103577\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 720)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 720, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 720, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 720, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 720, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 720, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 720, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 720, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 720, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 720, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            23328       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 720, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 3)            411         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 289,467\n",
            "Trainable params: 288,443\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset LargeKitchenAppliances ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/LargeKitchenAppliances_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/LargeKitchenAppliances_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  375 Number of test samples :  375\n",
            "Number of classes :  3\n",
            "Sequence length :  720\n",
            "Class weights :  [1. 1. 1.]\n",
            "Train on 375 samples, validate on 375 samples\n",
            "Epoch 1/4\n",
            " - 15s - loss: 1.0556 - accuracy: 0.4907 - val_loss: 1.3333 - val_accuracy: 0.3307\n",
            "\n",
            "Epoch 00001: loss improved from inf to 1.05556, saving model to ./weights/lstmfcn_8_cells_weights/LargeKitchenAppliances_weights.h5\n",
            "Epoch 2/4\n",
            " - 14s - loss: 0.8993 - accuracy: 0.6320 - val_loss: 1.4419 - val_accuracy: 0.3200\n",
            "\n",
            "Epoch 00002: loss improved from 1.05556 to 0.89935, saving model to ./weights/lstmfcn_8_cells_weights/LargeKitchenAppliances_weights.h5\n",
            "Epoch 3/4\n",
            " - 14s - loss: 0.8306 - accuracy: 0.6747 - val_loss: 1.4527 - val_accuracy: 0.3573\n",
            "\n",
            "Epoch 00003: loss improved from 0.89935 to 0.83057, saving model to ./weights/lstmfcn_8_cells_weights/LargeKitchenAppliances_weights.h5\n",
            "Epoch 4/4\n",
            " - 14s - loss: 0.7944 - accuracy: 0.6800 - val_loss: 1.3663 - val_accuracy: 0.4933\n",
            "\n",
            "Epoch 00004: loss improved from 0.83057 to 0.79445, saving model to ./weights/lstmfcn_8_cells_weights/LargeKitchenAppliances_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/LargeKitchenAppliances_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/LargeKitchenAppliances_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  375 Number of test samples :  375\n",
            "Number of classes :  3\n",
            "Sequence length :  720\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/LargeKitchenAppliances_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "375/375 [==============================] - 3s 9ms/step\n",
            "\n",
            "Final Accuracy :  0.4933333396911621\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 637)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 637, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 637, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 637, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 637, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 637, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 637, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 637, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 637, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 637, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            20672       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 637, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 2)            274         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 286,674\n",
            "Trainable params: 285,650\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset Lighting2 ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Lighting2_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Lighting2_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  60 Number of test samples :  61\n",
            "Number of classes :  2\n",
            "Sequence length :  637\n",
            "Class weights :  [1.5  0.75]\n",
            "Train on 60 samples, validate on 61 samples\n",
            "Epoch 1/4\n",
            " - 3s - loss: 0.6941 - accuracy: 0.6333 - val_loss: 0.7716 - val_accuracy: 0.5410\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.69406, saving model to ./weights/lstmfcn_8_cells_weights/Lighting2_weights.h5\n",
            "Epoch 2/4\n",
            " - 2s - loss: 0.5531 - accuracy: 0.7333 - val_loss: 0.7637 - val_accuracy: 0.5410\n",
            "\n",
            "Epoch 00002: loss improved from 0.69406 to 0.55311, saving model to ./weights/lstmfcn_8_cells_weights/Lighting2_weights.h5\n",
            "Epoch 3/4\n",
            " - 2s - loss: 0.4957 - accuracy: 0.8500 - val_loss: 0.7467 - val_accuracy: 0.5410\n",
            "\n",
            "Epoch 00003: loss improved from 0.55311 to 0.49565, saving model to ./weights/lstmfcn_8_cells_weights/Lighting2_weights.h5\n",
            "Epoch 4/4\n",
            " - 2s - loss: 0.4733 - accuracy: 0.8167 - val_loss: 0.6925 - val_accuracy: 0.5410\n",
            "\n",
            "Epoch 00004: loss improved from 0.49565 to 0.47327, saving model to ./weights/lstmfcn_8_cells_weights/Lighting2_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Lighting2_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Lighting2_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  60 Number of test samples :  61\n",
            "Number of classes :  2\n",
            "Sequence length :  637\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/Lighting2_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "61/61 [==============================] - 1s 10ms/step\n",
            "\n",
            "Final Accuracy :  0.5409836173057556\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 1024)      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 1024, 1)      0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 1024, 128)    1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 1024, 128)    512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 1024, 128)    0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 1024, 256)    164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 1024, 256)    1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 1024, 256)    0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 1024, 128)    98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 1024, 128)    512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            33056       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 1024, 128)    0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 8)            1096        concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 299,880\n",
            "Trainable params: 298,856\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset MALLAT ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Mallat_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Mallat_TEST\n",
            "Traceback (most recent call last):\n",
            "  File \"gdrive/My Drive/Colab Notebooks/LSTM-FCN/all_datasets_training.py\", line 269, in <module>\n",
            "    normalize_timeseries=normalize_dataset)\n",
            "  File \"/content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/utils/keras_utils.py\", line 62, in train_model\n",
            "    normalize_timeseries=normalize_timeseries)\n",
            "  File \"/content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/utils/generic_utils.py\", line 42, in load_dataset_at\n",
            "    raise FileNotFoundError('File %s not found!' % (TRAIN_FILES[index]))\n",
            "FileNotFoundError: File /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Mallat_TRAIN not found!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GW_Sf7cTgvLa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4051558d-3c78-4184-afaf-e49234e4a579"
      },
      "source": [
        "!python gdrive/My\\ Drive/Colab\\ Notebooks/LSTM-FCN/all_datasets_training.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "2020-07-10 01:10:56.205070: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "Num datasets :  128\n",
            "\n",
            "2020-07-10 01:11:00.262557: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2020-07-10 01:11:00.315573: E tensorflow/stream_executor/cuda/cuda_driver.cc:313] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2020-07-10 01:11:00.315655: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (0f6c0a78b83e): /proc/driver/nvidia/version does not exist\n",
            "2020-07-10 01:11:00.359844: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2300000000 Hz\n",
            "2020-07-10 01:11:00.360122: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2995100 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2020-07-10 01:11:00.360175: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 176)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 176, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 176, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 176, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 176, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 176, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 176, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 176, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 176, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 176, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            5920        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 176, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 37)           5069        concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 276,717\n",
            "Trainable params: 275,693\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset Adiac ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Adiac_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Adiac_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  390 Number of test samples :  391\n",
            "Number of classes :  37\n",
            "Sequence length :  176\n",
            "Class weights :  [0.81081081 1.05405405 2.10810811 0.75289575 2.63513514 0.87837838\n",
            " 1.05405405 0.81081081 1.17117117 0.95823096 1.05405405 0.87837838\n",
            " 1.75675676 0.95823096 0.87837838 1.05405405 0.95823096 1.17117117\n",
            " 0.75289575 1.17117117 1.31756757 1.05405405 1.17117117 0.7027027\n",
            " 0.81081081 1.17117117 0.81081081 0.81081081 1.17117117 0.81081081\n",
            " 1.17117117 0.81081081 0.81081081 1.17117117 1.05405405 0.95823096\n",
            " 1.31756757]\n",
            "Train on 390 samples, validate on 391 samples\n",
            "Epoch 1/4\n",
            " - 5s - loss: 3.6415 - accuracy: 0.0308 - val_loss: 3.8970 - val_accuracy: 0.0358\n",
            "\n",
            "Epoch 00001: loss improved from inf to 3.64155, saving model to ./weights/lstmfcn_8_cells_weights/Adiac_weights.h5\n",
            "Epoch 2/4\n",
            " - 4s - loss: 3.5615 - accuracy: 0.0769 - val_loss: 4.0129 - val_accuracy: 0.0179\n",
            "\n",
            "Epoch 00002: loss improved from 3.64155 to 3.56148, saving model to ./weights/lstmfcn_8_cells_weights/Adiac_weights.h5\n",
            "Epoch 3/4\n",
            " - 4s - loss: 3.5000 - accuracy: 0.1128 - val_loss: 4.1013 - val_accuracy: 0.0179\n",
            "\n",
            "Epoch 00003: loss improved from 3.56148 to 3.50004, saving model to ./weights/lstmfcn_8_cells_weights/Adiac_weights.h5\n",
            "Epoch 4/4\n",
            " - 4s - loss: 3.4709 - accuracy: 0.1205 - val_loss: 4.1399 - val_accuracy: 0.0281\n",
            "\n",
            "Epoch 00004: loss improved from 3.50004 to 3.47085, saving model to ./weights/lstmfcn_8_cells_weights/Adiac_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Adiac_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Adiac_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  390 Number of test samples :  391\n",
            "Number of classes :  37\n",
            "Sequence length :  176\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/Adiac_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "391/391 [==============================] - 1s 2ms/step\n",
            "\n",
            "Final Accuracy :  0.02813299186527729\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 251)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 251, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 251, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 251, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 251, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 251, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 251, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 251, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 251, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 251, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            8320        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 251, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 3)            411         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 274,459\n",
            "Trainable params: 273,435\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset ArrowHead ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ArrowHead_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ArrowHead_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  36 Number of test samples :  175\n",
            "Number of classes :  3\n",
            "Sequence length :  251\n",
            "Class weights :  [1. 1. 1.]\n",
            "Train on 36 samples, validate on 175 samples\n",
            "Epoch 1/4\n",
            " - 2s - loss: 1.1607 - accuracy: 0.3333 - val_loss: 1.0783 - val_accuracy: 0.3943\n",
            "\n",
            "Epoch 00001: loss improved from inf to 1.16069, saving model to ./weights/lstmfcn_8_cells_weights/ArrowHead_weights.h5\n",
            "Epoch 2/4\n",
            " - 1s - loss: 0.9703 - accuracy: 0.6389 - val_loss: 1.0795 - val_accuracy: 0.3943\n",
            "\n",
            "Epoch 00002: loss improved from 1.16069 to 0.97035, saving model to ./weights/lstmfcn_8_cells_weights/ArrowHead_weights.h5\n",
            "Epoch 3/4\n",
            " - 1s - loss: 0.9043 - accuracy: 0.6389 - val_loss: 1.0997 - val_accuracy: 0.3943\n",
            "\n",
            "Epoch 00003: loss improved from 0.97035 to 0.90433, saving model to ./weights/lstmfcn_8_cells_weights/ArrowHead_weights.h5\n",
            "Epoch 4/4\n",
            " - 1s - loss: 0.8421 - accuracy: 0.6944 - val_loss: 1.1471 - val_accuracy: 0.3943\n",
            "\n",
            "Epoch 00004: loss improved from 0.90433 to 0.84211, saving model to ./weights/lstmfcn_8_cells_weights/ArrowHead_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ArrowHead_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ArrowHead_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  36 Number of test samples :  175\n",
            "Number of classes :  3\n",
            "Sequence length :  251\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/ArrowHead_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "175/175 [==============================] - 1s 4ms/step\n",
            "\n",
            "Final Accuracy :  0.3942857086658478\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 166)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 166, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 166, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 166, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 166, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 166, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 166, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 166, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 166, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 166, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            5600        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 166, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 3)            411         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 271,739\n",
            "Trainable params: 270,715\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset ChlorineConcentration ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ChlorineConcentration_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ChlorineConcentration_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  467 Number of test samples :  3840\n",
            "Number of classes :  3\n",
            "Sequence length :  166\n",
            "Class weights :  [1.36549708 1.71062271 0.59414758]\n",
            "Train on 467 samples, validate on 3840 samples\n",
            "Epoch 1/4\n",
            " - 11s - loss: 1.0680 - accuracy: 0.4154 - val_loss: 2.9679 - val_accuracy: 0.2367\n",
            "\n",
            "Epoch 00001: loss improved from inf to 1.06800, saving model to ./weights/lstmfcn_8_cells_weights/ChlorineConcentration_weights.h5\n",
            "Epoch 2/4\n",
            " - 10s - loss: 0.9705 - accuracy: 0.5717 - val_loss: 3.2533 - val_accuracy: 0.2383\n",
            "\n",
            "Epoch 00002: loss improved from 1.06800 to 0.97053, saving model to ./weights/lstmfcn_8_cells_weights/ChlorineConcentration_weights.h5\n",
            "Epoch 3/4\n",
            " - 10s - loss: 0.9551 - accuracy: 0.5782 - val_loss: 3.0880 - val_accuracy: 0.2393\n",
            "\n",
            "Epoch 00003: loss improved from 0.97053 to 0.95515, saving model to ./weights/lstmfcn_8_cells_weights/ChlorineConcentration_weights.h5\n",
            "Epoch 4/4\n",
            " - 11s - loss: 0.9472 - accuracy: 0.5824 - val_loss: 2.8282 - val_accuracy: 0.2404\n",
            "\n",
            "Epoch 00004: loss improved from 0.95515 to 0.94716, saving model to ./weights/lstmfcn_8_cells_weights/ChlorineConcentration_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ChlorineConcentration_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ChlorineConcentration_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  467 Number of test samples :  3840\n",
            "Number of classes :  3\n",
            "Sequence length :  166\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/ChlorineConcentration_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "3840/3840 [==============================] - 8s 2ms/step\n",
            "\n",
            "Final Accuracy :  0.24036458134651184\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 256)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 256, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 256, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 256, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 256, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 256, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 256, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 256, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 256, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 256, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            8480        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 256, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 11)           1507        concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 275,715\n",
            "Trainable params: 274,691\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset InsectWingbeatSound ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/InsectWingbeatSound_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/InsectWingbeatSound_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  220 Number of test samples :  1980\n",
            "Number of classes :  11\n",
            "Sequence length :  256\n",
            "Class weights :  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "Train on 220 samples, validate on 1980 samples\n",
            "Epoch 1/4\n",
            " - 9s - loss: 2.4207 - accuracy: 0.1500 - val_loss: 2.6066 - val_accuracy: 0.0909\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.42070, saving model to ./weights/lstmfcn_8_cells_weights/InsectWingbeatSound_weights.h5\n",
            "Epoch 2/4\n",
            " - 8s - loss: 2.2940 - accuracy: 0.1864 - val_loss: 2.6030 - val_accuracy: 0.1222\n",
            "\n",
            "Epoch 00002: loss improved from 2.42070 to 2.29398, saving model to ./weights/lstmfcn_8_cells_weights/InsectWingbeatSound_weights.h5\n",
            "Epoch 3/4\n",
            " - 8s - loss: 2.2028 - accuracy: 0.2182 - val_loss: 2.6512 - val_accuracy: 0.0919\n",
            "\n",
            "Epoch 00003: loss improved from 2.29398 to 2.20282, saving model to ./weights/lstmfcn_8_cells_weights/InsectWingbeatSound_weights.h5\n",
            "Epoch 4/4\n",
            " - 8s - loss: 2.1629 - accuracy: 0.2500 - val_loss: 2.6982 - val_accuracy: 0.0909\n",
            "\n",
            "Epoch 00004: loss improved from 2.20282 to 2.16288, saving model to ./weights/lstmfcn_8_cells_weights/InsectWingbeatSound_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/InsectWingbeatSound_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/InsectWingbeatSound_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  220 Number of test samples :  1980\n",
            "Number of classes :  11\n",
            "Sequence length :  256\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/InsectWingbeatSound_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "1980/1980 [==============================] - 6s 3ms/step\n",
            "\n",
            "Final Accuracy :  0.09090909361839294\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 319)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 319, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 319, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 319, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 319, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 319, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 319, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 319, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 319, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 319, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            10496       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 319, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 7)            959         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 277,183\n",
            "Trainable params: 276,159\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset Lighting7 ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Lighting7_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Lighting7_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  70 Number of test samples :  73\n",
            "Number of classes :  7\n",
            "Sequence length :  319\n",
            "Class weights :  [1.25       1.25       1.25       0.83333333 2.         0.52631579\n",
            " 1.        ]\n",
            "Train on 70 samples, validate on 73 samples\n",
            "Epoch 1/4\n",
            " - 2s - loss: 1.8483 - accuracy: 0.2429 - val_loss: 1.9566 - val_accuracy: 0.1781\n",
            "\n",
            "Epoch 00001: loss improved from inf to 1.84827, saving model to ./weights/lstmfcn_8_cells_weights/Lighting7_weights.h5\n",
            "Epoch 2/4\n",
            " - 1s - loss: 1.5915 - accuracy: 0.4714 - val_loss: 1.8966 - val_accuracy: 0.3014\n",
            "\n",
            "Epoch 00002: loss improved from 1.84827 to 1.59148, saving model to ./weights/lstmfcn_8_cells_weights/Lighting7_weights.h5\n",
            "Epoch 3/4\n",
            " - 1s - loss: 1.4375 - accuracy: 0.5571 - val_loss: 1.9668 - val_accuracy: 0.2603\n",
            "\n",
            "Epoch 00003: loss improved from 1.59148 to 1.43747, saving model to ./weights/lstmfcn_8_cells_weights/Lighting7_weights.h5\n",
            "Epoch 4/4\n",
            " - 1s - loss: 1.3563 - accuracy: 0.4857 - val_loss: 2.0903 - val_accuracy: 0.2603\n",
            "\n",
            "Epoch 00004: loss improved from 1.43747 to 1.35634, saving model to ./weights/lstmfcn_8_cells_weights/Lighting7_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Lighting7_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Lighting7_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  70 Number of test samples :  73\n",
            "Number of classes :  7\n",
            "Sequence length :  319\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/Lighting7_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "73/73 [==============================] - 0s 6ms/step\n",
            "\n",
            "Final Accuracy :  0.2602739632129669\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 234)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 234, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 234, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 234, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 234, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 234, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 234, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 234, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 234, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 234, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            7776        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 234, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 2)            274         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 273,778\n",
            "Trainable params: 272,754\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset Wine ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Wine_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Wine_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  57 Number of test samples :  54\n",
            "Number of classes :  2\n",
            "Sequence length :  234\n",
            "Class weights :  [0.95       1.05555556]\n",
            "Train on 57 samples, validate on 54 samples\n",
            "Epoch 1/4\n",
            " - 2s - loss: 0.6700 - accuracy: 0.5263 - val_loss: 0.6969 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.67000, saving model to ./weights/lstmfcn_8_cells_weights/Wine_weights.h5\n",
            "Epoch 2/4\n",
            " - 1s - loss: 0.7252 - accuracy: 0.4737 - val_loss: 0.6986 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00002: loss did not improve from 0.67000\n",
            "Epoch 3/4\n",
            " - 1s - loss: 0.6980 - accuracy: 0.5965 - val_loss: 0.6962 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00003: loss did not improve from 0.67000\n",
            "Epoch 4/4\n",
            " - 1s - loss: 0.7175 - accuracy: 0.5263 - val_loss: 0.6965 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00004: loss did not improve from 0.67000\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Wine_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Wine_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  57 Number of test samples :  54\n",
            "Number of classes :  2\n",
            "Sequence length :  234\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/Wine_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "54/54 [==============================] - 0s 5ms/step\n",
            "\n",
            "Final Accuracy :  0.5\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 270)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 270, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 270, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 270, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 270, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 270, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 270, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 270, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 270, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 270, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            8928        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 270, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 25)           3425        concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 278,081\n",
            "Trainable params: 277,057\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset WordsSynonyms ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/WordsSynonyms_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/WordsSynonyms_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  267 Number of test samples :  638\n",
            "Number of classes :  25\n",
            "Sequence length :  270\n",
            "Class weights :  [1.78       0.178      2.136      0.30514286 2.136      0.62823529\n",
            " 2.67       0.445      1.78       0.89       1.78       1.335\n",
            " 3.56       1.068      2.67       0.76285714 2.136      2.136\n",
            " 2.67       1.52571429 2.67       1.335      1.78       1.52571429\n",
            " 5.34      ]\n",
            "Train on 267 samples, validate on 638 samples\n",
            "Epoch 1/4\n",
            " - 6s - loss: 3.3845 - accuracy: 0.0524 - val_loss: 3.4378 - val_accuracy: 0.0502\n",
            "\n",
            "Epoch 00001: loss improved from inf to 3.38449, saving model to ./weights/lstmfcn_8_cells_weights/WordsSynonyms_weights.h5\n",
            "Epoch 2/4\n",
            " - 5s - loss: 3.0481 - accuracy: 0.0974 - val_loss: 3.5286 - val_accuracy: 0.0549\n",
            "\n",
            "Epoch 00002: loss improved from 3.38449 to 3.04805, saving model to ./weights/lstmfcn_8_cells_weights/WordsSynonyms_weights.h5\n",
            "Epoch 3/4\n",
            " - 5s - loss: 2.8705 - accuracy: 0.1685 - val_loss: 3.5083 - val_accuracy: 0.0596\n",
            "\n",
            "Epoch 00003: loss improved from 3.04805 to 2.87045, saving model to ./weights/lstmfcn_8_cells_weights/WordsSynonyms_weights.h5\n",
            "Epoch 4/4\n",
            " - 5s - loss: 2.7496 - accuracy: 0.2397 - val_loss: 3.4769 - val_accuracy: 0.0596\n",
            "\n",
            "Epoch 00004: loss improved from 2.87045 to 2.74962, saving model to ./weights/lstmfcn_8_cells_weights/WordsSynonyms_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/WordsSynonyms_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/WordsSynonyms_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  267 Number of test samples :  638\n",
            "Number of classes :  25\n",
            "Sequence length :  270\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/WordsSynonyms_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "638/638 [==============================] - 2s 3ms/step\n",
            "\n",
            "Final Accuracy :  0.05956112965941429\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 270)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 270, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 270, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 270, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 270, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 270, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 270, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 270, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 270, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 270, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            8928        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 270, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 50)           6850        concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 281,506\n",
            "Trainable params: 280,482\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset 50words ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/50words_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/50words_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  450 Number of test samples :  455\n",
            "Number of classes :  50\n",
            "Sequence length :  270\n",
            "Class weights :  [0.17307692 0.18367347 0.27272727 0.45       0.69230769 0.6\n",
            " 0.5625     0.81818182 0.81818182 0.9        0.9        0.69230769\n",
            " 0.64285714 1.125      1.28571429 1.5        1.28571429 1.125\n",
            " 1.         1.28571429 1.28571429 1.8        1.5        2.25\n",
            " 4.5        1.5        2.25       1.5        2.25       1.5\n",
            " 3.         1.28571429 1.8        3.         1.5        1.5\n",
            " 1.8        2.25       2.25       2.25       9.         4.5\n",
            " 2.25       1.5        2.25       2.25       2.25       1.8\n",
            " 4.5        4.5       ]\n",
            "Train on 450 samples, validate on 455 samples\n",
            "Epoch 1/4\n",
            " - 7s - loss: 3.8316 - accuracy: 0.0867 - val_loss: 3.8422 - val_accuracy: 0.0198\n",
            "\n",
            "Epoch 00001: loss improved from inf to 3.83160, saving model to ./weights/lstmfcn_8_cells_weights/50words_weights.h5\n",
            "Epoch 2/4\n",
            " - 6s - loss: 3.4279 - accuracy: 0.2444 - val_loss: 3.5406 - val_accuracy: 0.2286\n",
            "\n",
            "Epoch 00002: loss improved from 3.83160 to 3.42787, saving model to ./weights/lstmfcn_8_cells_weights/50words_weights.h5\n",
            "Epoch 3/4\n",
            " - 6s - loss: 3.2387 - accuracy: 0.2511 - val_loss: 3.5104 - val_accuracy: 0.2462\n",
            "\n",
            "Epoch 00003: loss improved from 3.42787 to 3.23870, saving model to ./weights/lstmfcn_8_cells_weights/50words_weights.h5\n",
            "Epoch 4/4\n",
            " - 6s - loss: 3.1080 - accuracy: 0.2867 - val_loss: 3.5102 - val_accuracy: 0.1560\n",
            "\n",
            "Epoch 00004: loss improved from 3.23870 to 3.10796, saving model to ./weights/lstmfcn_8_cells_weights/50words_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/50words_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/50words_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  450 Number of test samples :  455\n",
            "Number of classes :  50\n",
            "Sequence length :  270\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/50words_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "455/455 [==============================] - 2s 3ms/step\n",
            "\n",
            "Final Accuracy :  0.15604396164417267\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 470)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 470, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 470, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 470, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 470, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 470, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 470, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 470, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 470, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 470, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            15328       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 470, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 5)            685         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 281,741\n",
            "Trainable params: 280,717\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset Beef ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Beef_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Beef_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  30 Number of test samples :  30\n",
            "Number of classes :  5\n",
            "Sequence length :  470\n",
            "Class weights :  [1. 1. 1. 1. 1.]\n",
            "Train on 30 samples, validate on 30 samples\n",
            "Epoch 1/4\n",
            " - 2s - loss: 1.6896 - accuracy: 0.2000 - val_loss: 1.6835 - val_accuracy: 0.2000\n",
            "\n",
            "Epoch 00001: loss improved from inf to 1.68960, saving model to ./weights/lstmfcn_8_cells_weights/Beef_weights.h5\n",
            "Epoch 2/4\n",
            " - 1s - loss: 1.5613 - accuracy: 0.3000 - val_loss: 1.6279 - val_accuracy: 0.2000\n",
            "\n",
            "Epoch 00002: loss improved from 1.68960 to 1.56133, saving model to ./weights/lstmfcn_8_cells_weights/Beef_weights.h5\n",
            "Epoch 3/4\n",
            " - 1s - loss: 1.4754 - accuracy: 0.3333 - val_loss: 1.6723 - val_accuracy: 0.2000\n",
            "\n",
            "Epoch 00003: loss improved from 1.56133 to 1.47539, saving model to ./weights/lstmfcn_8_cells_weights/Beef_weights.h5\n",
            "Epoch 4/4\n",
            " - 1s - loss: 1.4407 - accuracy: 0.4667 - val_loss: 1.7778 - val_accuracy: 0.2000\n",
            "\n",
            "Epoch 00004: loss improved from 1.47539 to 1.44075, saving model to ./weights/lstmfcn_8_cells_weights/Beef_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Beef_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Beef_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  30 Number of test samples :  30\n",
            "Number of classes :  5\n",
            "Sequence length :  470\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/Beef_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "30/30 [==============================] - 0s 10ms/step\n",
            "\n",
            "Final Accuracy :  0.20000000298023224\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 80)        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 80, 1)        0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 80, 128)      1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 80, 128)      512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 80, 128)      0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 80, 256)      164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 80, 256)      1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 80, 256)      0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 80, 128)      98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 80, 128)      512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            2848        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 80, 128)      0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 3)            411         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 268,987\n",
            "Trainable params: 267,963\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset DistalPhalanxOutlineAgeGroup ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/DistalPhalanxOutlineAgeGroup_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/DistalPhalanxOutlineAgeGroup_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  139 Number of test samples :  400\n",
            "Number of classes :  3\n",
            "Sequence length :  80\n",
            "Class weights :  [3.08888889 0.78531073 0.71282051]\n",
            "Train on 139 samples, validate on 400 samples\n",
            "Epoch 1/4\n",
            " - 2s - loss: 1.0691 - accuracy: 0.4676 - val_loss: 1.0867 - val_accuracy: 0.2825\n",
            "\n",
            "Epoch 00001: loss improved from inf to 1.06909, saving model to ./weights/lstmfcn_8_cells_weights/DistalPhalanxOutlineAgeGroup_weights.h5\n",
            "Epoch 2/4\n",
            " - 1s - loss: 0.8201 - accuracy: 0.5827 - val_loss: 1.5302 - val_accuracy: 0.2825\n",
            "\n",
            "Epoch 00002: loss improved from 1.06909 to 0.82009, saving model to ./weights/lstmfcn_8_cells_weights/DistalPhalanxOutlineAgeGroup_weights.h5\n",
            "Epoch 3/4\n",
            " - 1s - loss: 0.7892 - accuracy: 0.6331 - val_loss: 1.9987 - val_accuracy: 0.2825\n",
            "\n",
            "Epoch 00003: loss improved from 0.82009 to 0.78920, saving model to ./weights/lstmfcn_8_cells_weights/DistalPhalanxOutlineAgeGroup_weights.h5\n",
            "Epoch 4/4\n",
            " - 1s - loss: 0.7574 - accuracy: 0.6619 - val_loss: 2.3118 - val_accuracy: 0.2825\n",
            "\n",
            "Epoch 00004: loss improved from 0.78920 to 0.75740, saving model to ./weights/lstmfcn_8_cells_weights/DistalPhalanxOutlineAgeGroup_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/DistalPhalanxOutlineAgeGroup_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/DistalPhalanxOutlineAgeGroup_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  139 Number of test samples :  400\n",
            "Number of classes :  3\n",
            "Sequence length :  80\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/DistalPhalanxOutlineAgeGroup_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "400/400 [==============================] - 0s 1ms/step\n",
            "\n",
            "Final Accuracy :  0.2824999988079071\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 80)        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 80, 1)        0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 80, 128)      1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 80, 128)      512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 80, 128)      0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 80, 256)      164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 80, 256)      1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 80, 256)      0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 80, 128)      98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 80, 128)      512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            2848        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 80, 128)      0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 2)            274         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 268,850\n",
            "Trainable params: 267,826\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset DistalPhalanxOutlineCorrect ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/DistalPhalanxOutlineCorrect_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/DistalPhalanxOutlineCorrect_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  276 Number of test samples :  600\n",
            "Number of classes :  2\n",
            "Sequence length :  80\n",
            "Class weights :  [1.2        0.85714286]\n",
            "Train on 276 samples, validate on 600 samples\n",
            "Epoch 1/4\n",
            " - 3s - loss: 0.6624 - accuracy: 0.5833 - val_loss: 0.6526 - val_accuracy: 0.6300\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.66240, saving model to ./weights/lstmfcn_8_cells_weights/DistalPhalanxOutlineCorrect_weights.h5\n",
            "Epoch 2/4\n",
            " - 2s - loss: 0.5875 - accuracy: 0.7138 - val_loss: 0.7220 - val_accuracy: 0.6300\n",
            "\n",
            "Epoch 00002: loss improved from 0.66240 to 0.58750, saving model to ./weights/lstmfcn_8_cells_weights/DistalPhalanxOutlineCorrect_weights.h5\n",
            "Epoch 3/4\n",
            " - 2s - loss: 0.5647 - accuracy: 0.6848 - val_loss: 0.7776 - val_accuracy: 0.6300\n",
            "\n",
            "Epoch 00003: loss improved from 0.58750 to 0.56468, saving model to ./weights/lstmfcn_8_cells_weights/DistalPhalanxOutlineCorrect_weights.h5\n",
            "Epoch 4/4\n",
            " - 2s - loss: 0.5385 - accuracy: 0.7065 - val_loss: 0.8102 - val_accuracy: 0.6300\n",
            "\n",
            "Epoch 00004: loss improved from 0.56468 to 0.53855, saving model to ./weights/lstmfcn_8_cells_weights/DistalPhalanxOutlineCorrect_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/DistalPhalanxOutlineCorrect_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/DistalPhalanxOutlineCorrect_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  276 Number of test samples :  600\n",
            "Number of classes :  2\n",
            "Sequence length :  80\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/DistalPhalanxOutlineCorrect_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "600/600 [==============================] - 1s 1ms/step\n",
            "\n",
            "Final Accuracy :  0.6299999952316284\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 80)        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 80, 1)        0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 80, 128)      1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 80, 128)      512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 80, 128)      0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 80, 256)      164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 80, 256)      1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 80, 256)      0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 80, 128)      98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 80, 128)      512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            2848        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 80, 128)      0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 6)            822         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 269,398\n",
            "Trainable params: 268,374\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset DistalPhalanxTW ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/DistalPhalanxTW_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/DistalPhalanxTW_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  139 Number of test samples :  400\n",
            "Number of classes :  6\n",
            "Sequence length :  80\n",
            "Class weights :  [1.28703704 1.21929825 0.59401709 1.78205128 2.89583333 0.5515873 ]\n",
            "Train on 139 samples, validate on 400 samples\n",
            "Epoch 1/4\n",
            " - 2s - loss: 1.7981 - accuracy: 0.2950 - val_loss: 2.1349 - val_accuracy: 0.2050\n",
            "\n",
            "Epoch 00001: loss improved from inf to 1.79813, saving model to ./weights/lstmfcn_8_cells_weights/DistalPhalanxTW_weights.h5\n",
            "Epoch 2/4\n",
            " - 1s - loss: 1.3917 - accuracy: 0.5827 - val_loss: 2.3774 - val_accuracy: 0.2050\n",
            "\n",
            "Epoch 00002: loss improved from 1.79813 to 1.39172, saving model to ./weights/lstmfcn_8_cells_weights/DistalPhalanxTW_weights.h5\n",
            "Epoch 3/4\n",
            " - 1s - loss: 1.2145 - accuracy: 0.5899 - val_loss: 2.8349 - val_accuracy: 0.2050\n",
            "\n",
            "Epoch 00003: loss improved from 1.39172 to 1.21452, saving model to ./weights/lstmfcn_8_cells_weights/DistalPhalanxTW_weights.h5\n",
            "Epoch 4/4\n",
            " - 1s - loss: 1.1497 - accuracy: 0.5827 - val_loss: 3.3385 - val_accuracy: 0.2050\n",
            "\n",
            "Epoch 00004: loss improved from 1.21452 to 1.14975, saving model to ./weights/lstmfcn_8_cells_weights/DistalPhalanxTW_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/DistalPhalanxTW_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/DistalPhalanxTW_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  139 Number of test samples :  400\n",
            "Number of classes :  6\n",
            "Sequence length :  80\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/DistalPhalanxTW_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "400/400 [==============================] - 1s 1ms/step\n",
            "\n",
            "Final Accuracy :  0.20499999821186066\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 96)        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 96, 1)        0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 96, 128)      1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 96, 128)      512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 96, 128)      0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 96, 256)      164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 96, 256)      1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 96, 256)      0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 96, 128)      98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 96, 128)      512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            3360        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 96, 128)      0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 2)            274         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 269,362\n",
            "Trainable params: 268,338\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset ECG200 ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ECG200_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ECG200_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  100 Number of test samples :  100\n",
            "Number of classes :  2\n",
            "Sequence length :  96\n",
            "Class weights :  [1.61290323 0.72463768]\n",
            "Train on 100 samples, validate on 100 samples\n",
            "Epoch 1/4\n",
            " - 2s - loss: 0.6601 - accuracy: 0.6600 - val_loss: 0.7021 - val_accuracy: 0.3600\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.66014, saving model to ./weights/lstmfcn_8_cells_weights/ECG200_weights.h5\n",
            "Epoch 2/4\n",
            " - 1s - loss: 0.5365 - accuracy: 0.7400 - val_loss: 0.7496 - val_accuracy: 0.3500\n",
            "\n",
            "Epoch 00002: loss improved from 0.66014 to 0.53648, saving model to ./weights/lstmfcn_8_cells_weights/ECG200_weights.h5\n",
            "Epoch 3/4\n",
            " - 1s - loss: 0.4686 - accuracy: 0.7800 - val_loss: 0.8035 - val_accuracy: 0.3500\n",
            "\n",
            "Epoch 00003: loss improved from 0.53648 to 0.46865, saving model to ./weights/lstmfcn_8_cells_weights/ECG200_weights.h5\n",
            "Epoch 4/4\n",
            " - 1s - loss: 0.4506 - accuracy: 0.7800 - val_loss: 0.8309 - val_accuracy: 0.3500\n",
            "\n",
            "Epoch 00004: loss improved from 0.46865 to 0.45057, saving model to ./weights/lstmfcn_8_cells_weights/ECG200_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ECG200_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ECG200_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  100 Number of test samples :  100\n",
            "Number of classes :  2\n",
            "Sequence length :  96\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/ECG200_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "100/100 [==============================] - 0s 2ms/step\n",
            "\n",
            "Final Accuracy :  0.3499999940395355\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 136)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 136, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 136, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 136, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 136, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 136, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 136, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 136, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 136, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 136, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            4640        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 136, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 2)            274         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 270,642\n",
            "Trainable params: 269,618\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset ECGFiveDays ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ECGFiveDays_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ECGFiveDays_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  23 Number of test samples :  861\n",
            "Number of classes :  2\n",
            "Sequence length :  136\n",
            "Class weights :  [0.82142857 1.27777778]\n",
            "Train on 23 samples, validate on 861 samples\n",
            "Epoch 1/4\n",
            " - 3s - loss: 0.8195 - accuracy: 0.4348 - val_loss: 0.6998 - val_accuracy: 0.5029\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.81945, saving model to ./weights/lstmfcn_8_cells_weights/ECGFiveDays_weights.h5\n",
            "Epoch 2/4\n",
            " - 1s - loss: 0.5588 - accuracy: 0.7826 - val_loss: 0.6790 - val_accuracy: 0.5912\n",
            "\n",
            "Epoch 00002: loss improved from 0.81945 to 0.55878, saving model to ./weights/lstmfcn_8_cells_weights/ECGFiveDays_weights.h5\n",
            "Epoch 3/4\n",
            " - 2s - loss: 0.4831 - accuracy: 0.7826 - val_loss: 0.6797 - val_accuracy: 0.5017\n",
            "\n",
            "Epoch 00003: loss improved from 0.55878 to 0.48308, saving model to ./weights/lstmfcn_8_cells_weights/ECGFiveDays_weights.h5\n",
            "Epoch 4/4\n",
            " - 1s - loss: 0.4839 - accuracy: 0.7391 - val_loss: 0.6875 - val_accuracy: 0.4971\n",
            "\n",
            "Epoch 00004: loss did not improve from 0.48308\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ECGFiveDays_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ECGFiveDays_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  23 Number of test samples :  861\n",
            "Number of classes :  2\n",
            "Sequence length :  136\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/ECGFiveDays_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "861/861 [==============================] - 2s 2ms/step\n",
            "\n",
            "Final Accuracy :  0.5017421841621399\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 512)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 512, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 512, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 512, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 512, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 512, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 512, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 512, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 512, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 512, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            16672       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 512, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 2)            274         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 282,674\n",
            "Trainable params: 281,650\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset BeetleFly ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/BeetleFly_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/BeetleFly_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  20 Number of test samples :  20\n",
            "Number of classes :  2\n",
            "Sequence length :  512\n",
            "Class weights :  [1. 1.]\n",
            "Train on 20 samples, validate on 20 samples\n",
            "Epoch 1/4\n",
            " - 2s - loss: 1.0756 - accuracy: 0.5000 - val_loss: 0.6849 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00001: loss improved from inf to 1.07563, saving model to ./weights/lstmfcn_8_cells_weights/BeetleFly_weights.h5\n",
            "Epoch 2/4\n",
            " - 1s - loss: 0.7223 - accuracy: 0.5000 - val_loss: 0.6801 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00002: loss improved from 1.07563 to 0.72230, saving model to ./weights/lstmfcn_8_cells_weights/BeetleFly_weights.h5\n",
            "Epoch 3/4\n",
            " - 1s - loss: 0.5969 - accuracy: 0.8000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00003: loss improved from 0.72230 to 0.59690, saving model to ./weights/lstmfcn_8_cells_weights/BeetleFly_weights.h5\n",
            "Epoch 4/4\n",
            " - 1s - loss: 0.5019 - accuracy: 0.8000 - val_loss: 0.7433 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00004: loss improved from 0.59690 to 0.50187, saving model to ./weights/lstmfcn_8_cells_weights/BeetleFly_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/BeetleFly_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/BeetleFly_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  20 Number of test samples :  20\n",
            "Number of classes :  2\n",
            "Sequence length :  512\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/BeetleFly_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "20/20 [==============================] - 0s 13ms/step\n",
            "\n",
            "Final Accuracy :  0.5\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 512)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 512, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 512, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 512, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 512, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 512, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 512, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 512, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 512, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 512, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            16672       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 512, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 2)            274         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 282,674\n",
            "Trainable params: 281,650\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset BirdChicken ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/BirdChicken_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/BirdChicken_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  20 Number of test samples :  20\n",
            "Number of classes :  2\n",
            "Sequence length :  512\n",
            "Class weights :  [1. 1.]\n",
            "Train on 20 samples, validate on 20 samples\n",
            "Epoch 1/4\n",
            " - 2s - loss: 0.6988 - accuracy: 0.6000 - val_loss: 0.8666 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.69883, saving model to ./weights/lstmfcn_8_cells_weights/BirdChicken_weights.h5\n",
            "Epoch 2/4\n",
            " - 1s - loss: 0.5407 - accuracy: 0.8000 - val_loss: 0.8922 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00002: loss improved from 0.69883 to 0.54065, saving model to ./weights/lstmfcn_8_cells_weights/BirdChicken_weights.h5\n",
            "Epoch 3/4\n",
            " - 1s - loss: 0.4482 - accuracy: 0.9500 - val_loss: 0.9132 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00003: loss improved from 0.54065 to 0.44822, saving model to ./weights/lstmfcn_8_cells_weights/BirdChicken_weights.h5\n",
            "Epoch 4/4\n",
            " - 1s - loss: 0.3968 - accuracy: 0.9500 - val_loss: 0.9222 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00004: loss improved from 0.44822 to 0.39677, saving model to ./weights/lstmfcn_8_cells_weights/BirdChicken_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/BirdChicken_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/BirdChicken_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  20 Number of test samples :  20\n",
            "Number of classes :  2\n",
            "Sequence length :  512\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/BirdChicken_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "20/20 [==============================] - 0s 13ms/step\n",
            "\n",
            "Final Accuracy :  0.5\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 24)        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 24, 1)        0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 24, 128)      1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 24, 128)      512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 24, 128)      0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 24, 256)      164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 24, 256)      1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 24, 256)      0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 24, 128)      98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 24, 128)      512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            1056        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 24, 128)      0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 2)            274         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 267,058\n",
            "Trainable params: 266,034\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset ItalyPowerDemand ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ItalyPowerDemand_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ItalyPowerDemand_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  67 Number of test samples :  1029\n",
            "Number of classes :  2\n",
            "Sequence length :  24\n",
            "Class weights :  [0.98529412 1.01515152]\n",
            "Train on 67 samples, validate on 1029 samples\n",
            "Epoch 1/4\n",
            " - 2s - loss: 0.6463 - accuracy: 0.6119 - val_loss: 0.6458 - val_accuracy: 0.5491\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.64635, saving model to ./weights/lstmfcn_8_cells_weights/ItalyPowerDemand_weights.h5\n",
            "Epoch 2/4\n",
            " - 0s - loss: 0.3811 - accuracy: 0.9552 - val_loss: 0.7268 - val_accuracy: 0.5015\n",
            "\n",
            "Epoch 00002: loss improved from 0.64635 to 0.38113, saving model to ./weights/lstmfcn_8_cells_weights/ItalyPowerDemand_weights.h5\n",
            "Epoch 3/4\n",
            " - 0s - loss: 0.2628 - accuracy: 0.9851 - val_loss: 0.9167 - val_accuracy: 0.5015\n",
            "\n",
            "Epoch 00003: loss improved from 0.38113 to 0.26277, saving model to ./weights/lstmfcn_8_cells_weights/ItalyPowerDemand_weights.h5\n",
            "Epoch 4/4\n",
            " - 0s - loss: 0.1973 - accuracy: 0.9851 - val_loss: 1.1676 - val_accuracy: 0.5015\n",
            "\n",
            "Epoch 00004: loss improved from 0.26277 to 0.19729, saving model to ./weights/lstmfcn_8_cells_weights/ItalyPowerDemand_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ItalyPowerDemand_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ItalyPowerDemand_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  67 Number of test samples :  1029\n",
            "Number of classes :  2\n",
            "Sequence length :  24\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/ItalyPowerDemand_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "1029/1029 [==============================] - 0s 411us/step\n",
            "\n",
            "Final Accuracy :  0.5014577507972717\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 70)        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 70, 1)        0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 70, 128)      1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 70, 128)      512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 70, 128)      0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 70, 256)      164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 70, 256)      1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 70, 256)      0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 70, 128)      98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 70, 128)      512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            2528        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 70, 128)      0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 2)            274         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 268,530\n",
            "Trainable params: 267,506\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset SonyAIBORobotSurface ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/SonyAIBORobotSurface_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/SonyAIBORobotSurface_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  20 Number of test samples :  601\n",
            "Number of classes :  2\n",
            "Sequence length :  70\n",
            "Class weights :  [1.66666667 0.71428571]\n",
            "Train on 20 samples, validate on 601 samples\n",
            "Epoch 1/4\n",
            " - 2s - loss: 0.6955 - accuracy: 0.6000 - val_loss: 1.1055 - val_accuracy: 0.4293\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.69548, saving model to ./weights/lstmfcn_8_cells_weights/SonyAIBORobotSurface_weights.h5\n",
            "Epoch 2/4\n",
            " - 1s - loss: 0.3903 - accuracy: 0.9500 - val_loss: 0.8798 - val_accuracy: 0.4393\n",
            "\n",
            "Epoch 00002: loss improved from 0.69548 to 0.39026, saving model to ./weights/lstmfcn_8_cells_weights/SonyAIBORobotSurface_weights.h5\n",
            "Epoch 3/4\n",
            " - 1s - loss: 0.2604 - accuracy: 0.9500 - val_loss: 0.7265 - val_accuracy: 0.5108\n",
            "\n",
            "Epoch 00003: loss improved from 0.39026 to 0.26043, saving model to ./weights/lstmfcn_8_cells_weights/SonyAIBORobotSurface_weights.h5\n",
            "Epoch 4/4\n",
            " - 1s - loss: 0.2133 - accuracy: 0.9500 - val_loss: 0.6023 - val_accuracy: 0.6423\n",
            "\n",
            "Epoch 00004: loss improved from 0.26043 to 0.21332, saving model to ./weights/lstmfcn_8_cells_weights/SonyAIBORobotSurface_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/SonyAIBORobotSurface_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/SonyAIBORobotSurface_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  20 Number of test samples :  601\n",
            "Number of classes :  2\n",
            "Sequence length :  70\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/SonyAIBORobotSurface_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "601/601 [==============================] - 1s 1ms/step\n",
            "\n",
            "Final Accuracy :  0.642262876033783\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 65)        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 65, 1)        0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 65, 128)      1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 65, 128)      512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 65, 128)      0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 65, 256)      164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 65, 256)      1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 65, 256)      0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 65, 128)      98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 65, 128)      512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            2368        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 65, 128)      0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 2)            274         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 268,370\n",
            "Trainable params: 267,346\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset SonyAIBORobotSurfaceII ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/SonyAIBORobotSurfaceII_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/SonyAIBORobotSurfaceII_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  27 Number of test samples :  953\n",
            "Number of classes :  2\n",
            "Sequence length :  65\n",
            "Class weights :  [1.22727273 0.84375   ]\n",
            "Train on 27 samples, validate on 953 samples\n",
            "Epoch 1/4\n",
            " - 2s - loss: 0.7093 - accuracy: 0.3704 - val_loss: 0.5702 - val_accuracy: 0.6170\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.70933, saving model to ./weights/lstmfcn_8_cells_weights/SonyAIBORobotSurfaceII_weights.h5\n",
            "Epoch 2/4\n",
            " - 1s - loss: 0.4713 - accuracy: 0.9259 - val_loss: 0.5271 - val_accuracy: 0.6264\n",
            "\n",
            "Epoch 00002: loss improved from 0.70933 to 0.47135, saving model to ./weights/lstmfcn_8_cells_weights/SonyAIBORobotSurfaceII_weights.h5\n",
            "Epoch 3/4\n",
            " - 1s - loss: 0.3543 - accuracy: 0.9259 - val_loss: 0.4747 - val_accuracy: 0.6716\n",
            "\n",
            "Epoch 00003: loss improved from 0.47135 to 0.35426, saving model to ./weights/lstmfcn_8_cells_weights/SonyAIBORobotSurfaceII_weights.h5\n",
            "Epoch 4/4\n",
            " - 1s - loss: 0.2756 - accuracy: 0.9630 - val_loss: 0.4105 - val_accuracy: 0.7419\n",
            "\n",
            "Epoch 00004: loss improved from 0.35426 to 0.27561, saving model to ./weights/lstmfcn_8_cells_weights/SonyAIBORobotSurfaceII_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/SonyAIBORobotSurfaceII_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/SonyAIBORobotSurfaceII_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  27 Number of test samples :  953\n",
            "Number of classes :  2\n",
            "Sequence length :  65\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/SonyAIBORobotSurfaceII_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "953/953 [==============================] - 1s 891us/step\n",
            "\n",
            "Final Accuracy :  0.7418677806854248\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 80)        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 80, 1)        0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 80, 128)      1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 80, 128)      512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 80, 128)      0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 80, 256)      164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 80, 256)      1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 80, 256)      0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 80, 128)      98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 80, 128)      512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            2848        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 80, 128)      0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 3)            411         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 268,987\n",
            "Trainable params: 267,963\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset MiddlePhalanxOutlineAgeGroup ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/MiddlePhalanxOutlineAgeGroup_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/MiddlePhalanxOutlineAgeGroup_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  154 Number of test samples :  400\n",
            "Number of classes :  3\n",
            "Sequence length :  80\n",
            "Class weights :  [1.38738739 0.58333333 1.77011494]\n",
            "Train on 154 samples, validate on 400 samples\n",
            "Epoch 1/4\n",
            " - 2s - loss: 1.2348 - accuracy: 0.2403 - val_loss: 2.1546 - val_accuracy: 0.1375\n",
            "\n",
            "Epoch 00001: loss improved from inf to 1.23478, saving model to ./weights/lstmfcn_8_cells_weights/MiddlePhalanxOutlineAgeGroup_weights.h5\n",
            "Epoch 2/4\n",
            " - 1s - loss: 1.0125 - accuracy: 0.4935 - val_loss: 1.9720 - val_accuracy: 0.1375\n",
            "\n",
            "Epoch 00002: loss improved from 1.23478 to 1.01254, saving model to ./weights/lstmfcn_8_cells_weights/MiddlePhalanxOutlineAgeGroup_weights.h5\n",
            "Epoch 3/4\n",
            " - 1s - loss: 0.9476 - accuracy: 0.5909 - val_loss: 1.8951 - val_accuracy: 0.1375\n",
            "\n",
            "Epoch 00003: loss improved from 1.01254 to 0.94759, saving model to ./weights/lstmfcn_8_cells_weights/MiddlePhalanxOutlineAgeGroup_weights.h5\n",
            "Epoch 4/4\n",
            " - 1s - loss: 0.9083 - accuracy: 0.6299 - val_loss: 2.0174 - val_accuracy: 0.2525\n",
            "\n",
            "Epoch 00004: loss improved from 0.94759 to 0.90829, saving model to ./weights/lstmfcn_8_cells_weights/MiddlePhalanxOutlineAgeGroup_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/MiddlePhalanxOutlineAgeGroup_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/MiddlePhalanxOutlineAgeGroup_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  154 Number of test samples :  400\n",
            "Number of classes :  3\n",
            "Sequence length :  80\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/MiddlePhalanxOutlineAgeGroup_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "400/400 [==============================] - 0s 1ms/step\n",
            "\n",
            "Final Accuracy :  0.2524999976158142\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 80)        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 80, 1)        0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 80, 128)      1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 80, 128)      512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 80, 128)      0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 80, 256)      164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 80, 256)      1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 80, 256)      0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 80, 128)      98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 80, 128)      512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            2848        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 80, 128)      0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 2)            274         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 268,850\n",
            "Trainable params: 267,826\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset MiddlePhalanxOutlineCorrect ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/MiddlePhalanxOutlineCorrect_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/MiddlePhalanxOutlineCorrect_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  291 Number of test samples :  600\n",
            "Number of classes :  2\n",
            "Sequence length :  80\n",
            "Class weights :  [1.164      0.87650602]\n",
            "Train on 291 samples, validate on 600 samples\n",
            "Epoch 1/4\n",
            " - 3s - loss: 0.6984 - accuracy: 0.5189 - val_loss: 0.8166 - val_accuracy: 0.6467\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.69844, saving model to ./weights/lstmfcn_8_cells_weights/MiddlePhalanxOutlineCorrect_weights.h5\n",
            "Epoch 2/4\n",
            " - 2s - loss: 0.6199 - accuracy: 0.6735 - val_loss: 1.0947 - val_accuracy: 0.6467\n",
            "\n",
            "Epoch 00002: loss improved from 0.69844 to 0.61988, saving model to ./weights/lstmfcn_8_cells_weights/MiddlePhalanxOutlineCorrect_weights.h5\n",
            "Epoch 3/4\n",
            " - 2s - loss: 0.5888 - accuracy: 0.7113 - val_loss: 1.5185 - val_accuracy: 0.6467\n",
            "\n",
            "Epoch 00003: loss improved from 0.61988 to 0.58883, saving model to ./weights/lstmfcn_8_cells_weights/MiddlePhalanxOutlineCorrect_weights.h5\n",
            "Epoch 4/4\n",
            " - 2s - loss: 0.5599 - accuracy: 0.7423 - val_loss: 1.9307 - val_accuracy: 0.6467\n",
            "\n",
            "Epoch 00004: loss improved from 0.58883 to 0.55986, saving model to ./weights/lstmfcn_8_cells_weights/MiddlePhalanxOutlineCorrect_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/MiddlePhalanxOutlineCorrect_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/MiddlePhalanxOutlineCorrect_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  291 Number of test samples :  600\n",
            "Number of classes :  2\n",
            "Sequence length :  80\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/MiddlePhalanxOutlineCorrect_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "600/600 [==============================] - 1s 1ms/step\n",
            "\n",
            "Final Accuracy :  0.6466666460037231\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 80)        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 80, 1)        0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 80, 128)      1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 80, 128)      512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 80, 128)      0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 80, 256)      164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 80, 256)      1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 80, 256)      0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 80, 128)      98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 80, 128)      512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            2848        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 80, 128)      0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 6)            822         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 269,398\n",
            "Trainable params: 268,374\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset MiddlePhalanxTW ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/MiddlePhalanxTW_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/MiddlePhalanxTW_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  154 Number of test samples :  399\n",
            "Number of classes :  6\n",
            "Sequence length :  80\n",
            "Class weights :  [1.02666667 0.58333333 1.11594203 2.33333333 2.85185185 0.61111111]\n",
            "Train on 154 samples, validate on 399 samples\n",
            "Epoch 1/4\n",
            " - 2s - loss: 1.9452 - accuracy: 0.1234 - val_loss: 1.6166 - val_accuracy: 0.4486\n",
            "\n",
            "Epoch 00001: loss improved from inf to 1.94520, saving model to ./weights/lstmfcn_8_cells_weights/MiddlePhalanxTW_weights.h5\n",
            "Epoch 2/4\n",
            " - 1s - loss: 1.5606 - accuracy: 0.4416 - val_loss: 1.7427 - val_accuracy: 0.2105\n",
            "\n",
            "Epoch 00002: loss improved from 1.94520 to 1.56057, saving model to ./weights/lstmfcn_8_cells_weights/MiddlePhalanxTW_weights.h5\n",
            "Epoch 3/4\n",
            " - 1s - loss: 1.3798 - accuracy: 0.4416 - val_loss: 2.1086 - val_accuracy: 0.2105\n",
            "\n",
            "Epoch 00003: loss improved from 1.56057 to 1.37980, saving model to ./weights/lstmfcn_8_cells_weights/MiddlePhalanxTW_weights.h5\n",
            "Epoch 4/4\n",
            " - 1s - loss: 1.2759 - accuracy: 0.4870 - val_loss: 2.5457 - val_accuracy: 0.2105\n",
            "\n",
            "Epoch 00004: loss improved from 1.37980 to 1.27586, saving model to ./weights/lstmfcn_8_cells_weights/MiddlePhalanxTW_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/MiddlePhalanxTW_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/MiddlePhalanxTW_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  154 Number of test samples :  399\n",
            "Number of classes :  6\n",
            "Sequence length :  80\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/MiddlePhalanxTW_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "399/399 [==============================] - 0s 1ms/step\n",
            "\n",
            "Final Accuracy :  0.21052631735801697\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 80)        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 80, 1)        0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 80, 128)      1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 80, 128)      512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 80, 128)      0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 80, 256)      164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 80, 256)      1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 80, 256)      0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 80, 128)      98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 80, 128)      512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            2848        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 80, 128)      0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 3)            411         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 268,987\n",
            "Trainable params: 267,963\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset ProximalPhalanxOutlineAgeGroup ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ProximalPhalanxOutlineAgeGroup_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ProximalPhalanxOutlineAgeGroup_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  400 Number of test samples :  205\n",
            "Number of classes :  3\n",
            "Sequence length :  80\n",
            "Class weights :  [1.85185185 0.95923261 0.70546737]\n",
            "Train on 400 samples, validate on 205 samples\n",
            "Epoch 1/4\n",
            " - 3s - loss: 0.9315 - accuracy: 0.5750 - val_loss: 1.7537 - val_accuracy: 0.0829\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.93146, saving model to ./weights/lstmfcn_8_cells_weights/ProximalPhalanxOutlineAgeGroup_weights.h5\n",
            "Epoch 2/4\n",
            " - 2s - loss: 0.7003 - accuracy: 0.7425 - val_loss: 2.4762 - val_accuracy: 0.4293\n",
            "\n",
            "Epoch 00002: loss improved from 0.93146 to 0.70033, saving model to ./weights/lstmfcn_8_cells_weights/ProximalPhalanxOutlineAgeGroup_weights.h5\n",
            "Epoch 3/4\n",
            " - 2s - loss: 0.6339 - accuracy: 0.7475 - val_loss: 3.1264 - val_accuracy: 0.4244\n",
            "\n",
            "Epoch 00003: loss improved from 0.70033 to 0.63391, saving model to ./weights/lstmfcn_8_cells_weights/ProximalPhalanxOutlineAgeGroup_weights.h5\n",
            "Epoch 4/4\n",
            " - 2s - loss: 0.5914 - accuracy: 0.7650 - val_loss: 3.7770 - val_accuracy: 0.0878\n",
            "\n",
            "Epoch 00004: loss improved from 0.63391 to 0.59136, saving model to ./weights/lstmfcn_8_cells_weights/ProximalPhalanxOutlineAgeGroup_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ProximalPhalanxOutlineAgeGroup_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ProximalPhalanxOutlineAgeGroup_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  400 Number of test samples :  205\n",
            "Number of classes :  3\n",
            "Sequence length :  80\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/ProximalPhalanxOutlineAgeGroup_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "205/205 [==============================] - 0s 2ms/step\n",
            "\n",
            "Final Accuracy :  0.08780487626791\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 80)        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 80, 1)        0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 80, 128)      1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 80, 128)      512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 80, 128)      0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 80, 256)      164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 80, 256)      1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 80, 256)      0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 80, 128)      98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 80, 128)      512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            2848        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 80, 128)      0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 2)            274         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 268,850\n",
            "Trainable params: 267,826\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset ProximalPhalanxOutlineCorrect ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ProximalPhalanxOutlineCorrect_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ProximalPhalanxOutlineCorrect_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  600 Number of test samples :  291\n",
            "Number of classes :  2\n",
            "Sequence length :  80\n",
            "Class weights :  [1.54639175 0.73891626]\n",
            "Train on 600 samples, validate on 291 samples\n",
            "Epoch 1/4\n",
            " - 4s - loss: 0.5775 - accuracy: 0.7133 - val_loss: 0.7666 - val_accuracy: 0.6838\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.57748, saving model to ./weights/lstmfcn_8_cells_weights/ProximalPhalanxOutlineCorrect_weights.h5\n",
            "Epoch 2/4\n",
            " - 2s - loss: 0.5526 - accuracy: 0.6833 - val_loss: 0.9416 - val_accuracy: 0.6838\n",
            "\n",
            "Epoch 00002: loss improved from 0.57748 to 0.55256, saving model to ./weights/lstmfcn_8_cells_weights/ProximalPhalanxOutlineCorrect_weights.h5\n",
            "Epoch 3/4\n",
            " - 2s - loss: 0.5428 - accuracy: 0.7233 - val_loss: 1.1610 - val_accuracy: 0.6838\n",
            "\n",
            "Epoch 00003: loss improved from 0.55256 to 0.54285, saving model to ./weights/lstmfcn_8_cells_weights/ProximalPhalanxOutlineCorrect_weights.h5\n",
            "Epoch 4/4\n",
            " - 2s - loss: 0.5404 - accuracy: 0.7150 - val_loss: 1.3349 - val_accuracy: 0.6838\n",
            "\n",
            "Epoch 00004: loss improved from 0.54285 to 0.54040, saving model to ./weights/lstmfcn_8_cells_weights/ProximalPhalanxOutlineCorrect_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ProximalPhalanxOutlineCorrect_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ProximalPhalanxOutlineCorrect_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  600 Number of test samples :  291\n",
            "Number of classes :  2\n",
            "Sequence length :  80\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/ProximalPhalanxOutlineCorrect_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "291/291 [==============================] - 0s 1ms/step\n",
            "\n",
            "Final Accuracy :  0.6838487982749939\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 80)        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 80, 1)        0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 80, 128)      1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 80, 128)      512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 80, 128)      0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 80, 256)      164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 80, 256)      1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 80, 256)      0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 80, 128)      98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 80, 128)      512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            2848        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 80, 128)      0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 6)            822         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 269,398\n",
            "Trainable params: 268,374\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset ProximalPhalanxTW ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ProximalPhalanxTW_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ProximalPhalanxTW_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  205 Number of test samples :  400\n",
            "Number of classes :  6\n",
            "Sequence length :  80\n",
            "Class weights :  [17.08333333  0.50995025  0.85416667  3.41666667  2.44047619  0.47453704]\n",
            "Train on 205 samples, validate on 400 samples\n",
            "Epoch 1/4\n",
            " - 2s - loss: 1.7731 - accuracy: 0.1024 - val_loss: 2.2222 - val_accuracy: 0.0400\n",
            "\n",
            "Epoch 00001: loss improved from inf to 1.77311, saving model to ./weights/lstmfcn_8_cells_weights/ProximalPhalanxTW_weights.h5\n",
            "Epoch 2/4\n",
            " - 1s - loss: 1.4255 - accuracy: 0.5561 - val_loss: 2.2536 - val_accuracy: 0.0400\n",
            "\n",
            "Epoch 00002: loss improved from 1.77311 to 1.42546, saving model to ./weights/lstmfcn_8_cells_weights/ProximalPhalanxTW_weights.h5\n",
            "Epoch 3/4\n",
            " - 1s - loss: 1.1909 - accuracy: 0.7073 - val_loss: 2.3294 - val_accuracy: 0.1450\n",
            "\n",
            "Epoch 00003: loss improved from 1.42546 to 1.19092, saving model to ./weights/lstmfcn_8_cells_weights/ProximalPhalanxTW_weights.h5\n",
            "Epoch 4/4\n",
            " - 1s - loss: 1.0554 - accuracy: 0.7610 - val_loss: 2.6273 - val_accuracy: 0.1450\n",
            "\n",
            "Epoch 00004: loss improved from 1.19092 to 1.05542, saving model to ./weights/lstmfcn_8_cells_weights/ProximalPhalanxTW_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ProximalPhalanxTW_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ProximalPhalanxTW_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  205 Number of test samples :  400\n",
            "Number of classes :  6\n",
            "Sequence length :  80\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/ProximalPhalanxTW_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "400/400 [==============================] - 0s 1ms/step\n",
            "\n",
            "Final Accuracy :  0.14499999582767487\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 84)        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 84, 1)        0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 84, 128)      1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 84, 128)      512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 84, 128)      0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 84, 256)      164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 84, 256)      1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 84, 256)      0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 84, 128)      98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 84, 128)      512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            2976        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 84, 128)      0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 2)            274         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 268,978\n",
            "Trainable params: 267,954\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset MoteStrain ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/MoteStrain_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/MoteStrain_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  20 Number of test samples :  1252\n",
            "Number of classes :  2\n",
            "Sequence length :  84\n",
            "Class weights :  [1. 1.]\n",
            "Train on 20 samples, validate on 1252 samples\n",
            "Epoch 1/4\n",
            " - 2s - loss: 0.7783 - accuracy: 0.4500 - val_loss: 0.6874 - val_accuracy: 0.4688\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.77834, saving model to ./weights/lstmfcn_8_cells_weights/MoteStrain_weights.h5\n",
            "Epoch 2/4\n",
            " - 1s - loss: 0.4978 - accuracy: 0.8000 - val_loss: 0.6483 - val_accuracy: 0.5839\n",
            "\n",
            "Epoch 00002: loss improved from 0.77834 to 0.49782, saving model to ./weights/lstmfcn_8_cells_weights/MoteStrain_weights.h5\n",
            "Epoch 3/4\n",
            " - 1s - loss: 0.3710 - accuracy: 0.9000 - val_loss: 0.6213 - val_accuracy: 0.6214\n",
            "\n",
            "Epoch 00003: loss improved from 0.49782 to 0.37105, saving model to ./weights/lstmfcn_8_cells_weights/MoteStrain_weights.h5\n",
            "Epoch 4/4\n",
            " - 1s - loss: 0.3191 - accuracy: 0.9500 - val_loss: 0.5949 - val_accuracy: 0.6621\n",
            "\n",
            "Epoch 00004: loss improved from 0.37105 to 0.31912, saving model to ./weights/lstmfcn_8_cells_weights/MoteStrain_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/MoteStrain_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/MoteStrain_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  20 Number of test samples :  1252\n",
            "Number of classes :  2\n",
            "Sequence length :  84\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/MoteStrain_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "1252/1252 [==============================] - 1s 1ms/step\n",
            "\n",
            "Final Accuracy :  0.6621405482292175\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 99)        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 99, 1)        0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 99, 128)      1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 99, 128)      512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 99, 128)      0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 99, 256)      164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 99, 256)      1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 99, 256)      0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 99, 128)      98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 99, 128)      512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            3456        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 99, 128)      0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 10)           1370        concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 270,554\n",
            "Trainable params: 269,530\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset MedicalImages ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/MedicalImages_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/MedicalImages_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  381 Number of test samples :  760\n",
            "Number of classes :  10\n",
            "Sequence length :  99\n",
            "Class weights :  [1.08857143 2.54       1.524      2.38125    3.81       5.44285714\n",
            " 2.11666667 6.35       0.82826087 0.18768473]\n",
            "Train on 381 samples, validate on 760 samples\n",
            "Epoch 1/4\n",
            " - 4s - loss: 2.3349 - accuracy: 0.1076 - val_loss: 2.1122 - val_accuracy: 0.1750\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.33489, saving model to ./weights/lstmfcn_8_cells_weights/MedicalImages_weights.h5\n",
            "Epoch 2/4\n",
            " - 2s - loss: 1.8969 - accuracy: 0.4121 - val_loss: 1.8204 - val_accuracy: 0.5066\n",
            "\n",
            "Epoch 00002: loss improved from 2.33489 to 1.89694, saving model to ./weights/lstmfcn_8_cells_weights/MedicalImages_weights.h5\n",
            "Epoch 3/4\n",
            " - 3s - loss: 1.6782 - accuracy: 0.5013 - val_loss: 1.8583 - val_accuracy: 0.5132\n",
            "\n",
            "Epoch 00003: loss improved from 1.89694 to 1.67820, saving model to ./weights/lstmfcn_8_cells_weights/MedicalImages_weights.h5\n",
            "Epoch 4/4\n",
            " - 2s - loss: 1.5448 - accuracy: 0.5381 - val_loss: 1.9386 - val_accuracy: 0.5145\n",
            "\n",
            "Epoch 00004: loss improved from 1.67820 to 1.54483, saving model to ./weights/lstmfcn_8_cells_weights/MedicalImages_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/MedicalImages_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/MedicalImages_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  381 Number of test samples :  760\n",
            "Number of classes :  10\n",
            "Sequence length :  99\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/MedicalImages_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "760/760 [==============================] - 1s 1ms/step\n",
            "\n",
            "Final Accuracy :  0.5144736766815186\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 235)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 235, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 235, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 235, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 235, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 235, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 235, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 235, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 235, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 235, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            7808        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 235, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 2)            274         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 273,810\n",
            "Trainable params: 272,786\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset Strawberry ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Strawberry_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Strawberry_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  370 Number of test samples :  613\n",
            "Number of classes :  2\n",
            "Sequence length :  235\n",
            "Class weights :  [1.40151515 0.77731092]\n",
            "Train on 370 samples, validate on 613 samples\n",
            "Epoch 1/4\n",
            " - 6s - loss: 0.7270 - accuracy: 0.5270 - val_loss: 2.2980 - val_accuracy: 0.3573\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.72697, saving model to ./weights/lstmfcn_8_cells_weights/Strawberry_weights.h5\n",
            "Epoch 2/4\n",
            " - 5s - loss: 0.6141 - accuracy: 0.6541 - val_loss: 2.2579 - val_accuracy: 0.3573\n",
            "\n",
            "Epoch 00002: loss improved from 0.72697 to 0.61407, saving model to ./weights/lstmfcn_8_cells_weights/Strawberry_weights.h5\n",
            "Epoch 3/4\n",
            " - 5s - loss: 0.5926 - accuracy: 0.6486 - val_loss: 1.9869 - val_accuracy: 0.3573\n",
            "\n",
            "Epoch 00003: loss improved from 0.61407 to 0.59261, saving model to ./weights/lstmfcn_8_cells_weights/Strawberry_weights.h5\n",
            "Epoch 4/4\n",
            " - 5s - loss: 0.5780 - accuracy: 0.6811 - val_loss: 1.7623 - val_accuracy: 0.3573\n",
            "\n",
            "Epoch 00004: loss improved from 0.59261 to 0.57803, saving model to ./weights/lstmfcn_8_cells_weights/Strawberry_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Strawberry_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Strawberry_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  370 Number of test samples :  613\n",
            "Number of classes :  2\n",
            "Sequence length :  235\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/Strawberry_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "613/613 [==============================] - 2s 3ms/step\n",
            "\n",
            "Final Accuracy :  0.3572593927383423\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 277)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 277, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 277, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 277, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 277, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 277, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 277, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 277, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 277, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 277, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            9152        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 277, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 2)            274         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 275,154\n",
            "Trainable params: 274,130\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset ToeSegmentation1 ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ToeSegmentation1_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ToeSegmentation1_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  40 Number of test samples :  228\n",
            "Number of classes :  2\n",
            "Sequence length :  277\n",
            "Class weights :  [1. 1.]\n",
            "Train on 40 samples, validate on 228 samples\n",
            "Epoch 1/4\n",
            " - 2s - loss: 0.7037 - accuracy: 0.5250 - val_loss: 0.6853 - val_accuracy: 0.6009\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.70372, saving model to ./weights/lstmfcn_8_cells_weights/ToeSegmentation1_weights.h5\n",
            "Epoch 2/4\n",
            " - 1s - loss: 0.5876 - accuracy: 0.7750 - val_loss: 0.6968 - val_accuracy: 0.4737\n",
            "\n",
            "Epoch 00002: loss improved from 0.70372 to 0.58759, saving model to ./weights/lstmfcn_8_cells_weights/ToeSegmentation1_weights.h5\n",
            "Epoch 3/4\n",
            " - 1s - loss: 0.4982 - accuracy: 0.8500 - val_loss: 0.7172 - val_accuracy: 0.4737\n",
            "\n",
            "Epoch 00003: loss improved from 0.58759 to 0.49823, saving model to ./weights/lstmfcn_8_cells_weights/ToeSegmentation1_weights.h5\n",
            "Epoch 4/4\n",
            " - 1s - loss: 0.4695 - accuracy: 0.8250 - val_loss: 0.7481 - val_accuracy: 0.4737\n",
            "\n",
            "Epoch 00004: loss improved from 0.49823 to 0.46950, saving model to ./weights/lstmfcn_8_cells_weights/ToeSegmentation1_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ToeSegmentation1_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ToeSegmentation1_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  40 Number of test samples :  228\n",
            "Number of classes :  2\n",
            "Sequence length :  277\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/ToeSegmentation1_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "228/228 [==============================] - 1s 4ms/step\n",
            "\n",
            "Final Accuracy :  0.4736842215061188\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 286)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 286, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 286, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 286, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 286, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 286, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 286, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 286, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 286, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 286, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            9440        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 286, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 2)            274         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 275,442\n",
            "Trainable params: 274,418\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset Coffee ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Coffee_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Coffee_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  28 Number of test samples :  28\n",
            "Number of classes :  2\n",
            "Sequence length :  286\n",
            "Class weights :  [1. 1.]\n",
            "Train on 28 samples, validate on 28 samples\n",
            "Epoch 1/4\n",
            " - 1s - loss: 0.7061 - accuracy: 0.5000 - val_loss: 0.7343 - val_accuracy: 0.5357\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.70605, saving model to ./weights/lstmfcn_8_cells_weights/Coffee_weights.h5\n",
            "Epoch 2/4\n",
            " - 0s - loss: 0.6456 - accuracy: 0.6429 - val_loss: 0.8154 - val_accuracy: 0.5357\n",
            "\n",
            "Epoch 00002: loss improved from 0.70605 to 0.64560, saving model to ./weights/lstmfcn_8_cells_weights/Coffee_weights.h5\n",
            "Epoch 3/4\n",
            " - 0s - loss: 0.6016 - accuracy: 0.9286 - val_loss: 0.8748 - val_accuracy: 0.5357\n",
            "\n",
            "Epoch 00003: loss improved from 0.64560 to 0.60161, saving model to ./weights/lstmfcn_8_cells_weights/Coffee_weights.h5\n",
            "Epoch 4/4\n",
            " - 0s - loss: 0.5762 - accuracy: 0.9286 - val_loss: 0.8921 - val_accuracy: 0.5357\n",
            "\n",
            "Epoch 00004: loss improved from 0.60161 to 0.57617, saving model to ./weights/lstmfcn_8_cells_weights/Coffee_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Coffee_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Coffee_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  28 Number of test samples :  28\n",
            "Number of classes :  2\n",
            "Sequence length :  286\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/Coffee_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "28/28 [==============================] - 0s 8ms/step\n",
            "\n",
            "Final Accuracy :  0.5357142686843872\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 300)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 300, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 300, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 300, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 300, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 300, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 300, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 300, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 300, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 300, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            9888        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 300, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 12)           1644        concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 277,260\n",
            "Trainable params: 276,236\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset Cricket_X ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Cricket_X_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Cricket_X_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  390 Number of test samples :  390\n",
            "Number of classes :  12\n",
            "Sequence length :  300\n",
            "Class weights :  [0.85526316 1.0483871  0.95588235 0.92857143 1.2037037  0.95588235\n",
            " 0.90277778 1.0483871  1.3        1.16071429 0.83333333 1.015625  ]\n",
            "Train on 390 samples, validate on 390 samples\n",
            "Epoch 1/4\n",
            " - 7s - loss: 2.4804 - accuracy: 0.1333 - val_loss: 2.7551 - val_accuracy: 0.1282\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.48039, saving model to ./weights/lstmfcn_8_cells_weights/Cricket_X_weights.h5\n",
            "Epoch 2/4\n",
            " - 6s - loss: 2.1593 - accuracy: 0.2744 - val_loss: 2.7777 - val_accuracy: 0.1205\n",
            "\n",
            "Epoch 00002: loss improved from 2.48039 to 2.15928, saving model to ./weights/lstmfcn_8_cells_weights/Cricket_X_weights.h5\n",
            "Epoch 3/4\n",
            " - 6s - loss: 2.0830 - accuracy: 0.2615 - val_loss: 2.8253 - val_accuracy: 0.1231\n",
            "\n",
            "Epoch 00003: loss improved from 2.15928 to 2.08299, saving model to ./weights/lstmfcn_8_cells_weights/Cricket_X_weights.h5\n",
            "Epoch 4/4\n",
            " - 6s - loss: 2.0130 - accuracy: 0.3205 - val_loss: 2.8318 - val_accuracy: 0.1308\n",
            "\n",
            "Epoch 00004: loss improved from 2.08299 to 2.01298, saving model to ./weights/lstmfcn_8_cells_weights/Cricket_X_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Cricket_X_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Cricket_X_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  390 Number of test samples :  390\n",
            "Number of classes :  12\n",
            "Sequence length :  300\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/Cricket_X_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "390/390 [==============================] - 1s 4ms/step\n",
            "\n",
            "Final Accuracy :  0.13076923787593842\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 300)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 300, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 300, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 300, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 300, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 300, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 300, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 300, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 300, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 300, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            9888        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 300, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 12)           1644        concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 277,260\n",
            "Trainable params: 276,236\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset Cricket_Y ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Cricket_Y_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Cricket_Y_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  390 Number of test samples :  390\n",
            "Number of classes :  12\n",
            "Sequence length :  300\n",
            "Class weights :  [1.16071429 0.95588235 0.90277778 1.08333333 0.90277778 1.015625\n",
            " 1.015625   0.98484848 1.0483871  1.015625   1.015625   0.95588235]\n",
            "Train on 390 samples, validate on 390 samples\n",
            "Epoch 1/4\n",
            " - 7s - loss: 2.4872 - accuracy: 0.1154 - val_loss: 3.0284 - val_accuracy: 0.0897\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.48718, saving model to ./weights/lstmfcn_8_cells_weights/Cricket_Y_weights.h5\n",
            "Epoch 2/4\n",
            " - 6s - loss: 2.1838 - accuracy: 0.2538 - val_loss: 3.2473 - val_accuracy: 0.0897\n",
            "\n",
            "Epoch 00002: loss improved from 2.48718 to 2.18384, saving model to ./weights/lstmfcn_8_cells_weights/Cricket_Y_weights.h5\n",
            "Epoch 3/4\n",
            " - 6s - loss: 2.0728 - accuracy: 0.3359 - val_loss: 3.1067 - val_accuracy: 0.0897\n",
            "\n",
            "Epoch 00003: loss improved from 2.18384 to 2.07284, saving model to ./weights/lstmfcn_8_cells_weights/Cricket_Y_weights.h5\n",
            "Epoch 4/4\n",
            " - 6s - loss: 1.9965 - accuracy: 0.4077 - val_loss: 2.9540 - val_accuracy: 0.0897\n",
            "\n",
            "Epoch 00004: loss improved from 2.07284 to 1.99650, saving model to ./weights/lstmfcn_8_cells_weights/Cricket_Y_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Cricket_Y_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Cricket_Y_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  390 Number of test samples :  390\n",
            "Number of classes :  12\n",
            "Sequence length :  300\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/Cricket_Y_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "390/390 [==============================] - 1s 4ms/step\n",
            "\n",
            "Final Accuracy :  0.08974359184503555\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 300)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 300, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 300, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 300, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 300, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 300, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 300, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 300, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 300, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 300, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            9888        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 300, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 12)           1644        concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 277,260\n",
            "Trainable params: 276,236\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset Cricket_Z ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Cricket_Z_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Cricket_Z_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  390 Number of test samples :  390\n",
            "Number of classes :  12\n",
            "Sequence length :  300\n",
            "Class weights :  [1.015625   1.2037037  1.3        1.015625   1.12068966 0.90277778\n",
            " 0.79268293 0.87837838 1.0483871  1.0483871  0.98484848 0.90277778]\n",
            "Train on 390 samples, validate on 390 samples\n",
            "Epoch 1/4\n",
            " - 7s - loss: 2.4238 - accuracy: 0.1513 - val_loss: 2.5970 - val_accuracy: 0.0949\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.42378, saving model to ./weights/lstmfcn_8_cells_weights/Cricket_Z_weights.h5\n",
            "Epoch 2/4\n",
            " - 6s - loss: 2.1739 - accuracy: 0.3256 - val_loss: 2.7139 - val_accuracy: 0.0897\n",
            "\n",
            "Epoch 00002: loss improved from 2.42378 to 2.17394, saving model to ./weights/lstmfcn_8_cells_weights/Cricket_Z_weights.h5\n",
            "Epoch 3/4\n",
            " - 6s - loss: 2.0792 - accuracy: 0.3256 - val_loss: 2.7856 - val_accuracy: 0.0795\n",
            "\n",
            "Epoch 00003: loss improved from 2.17394 to 2.07917, saving model to ./weights/lstmfcn_8_cells_weights/Cricket_Z_weights.h5\n",
            "Epoch 4/4\n",
            " - 6s - loss: 2.0248 - accuracy: 0.3231 - val_loss: 2.9003 - val_accuracy: 0.0872\n",
            "\n",
            "Epoch 00004: loss improved from 2.07917 to 2.02482, saving model to ./weights/lstmfcn_8_cells_weights/Cricket_Z_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Cricket_Z_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Cricket_Z_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  390 Number of test samples :  390\n",
            "Number of classes :  12\n",
            "Sequence length :  300\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/Cricket_Z_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "390/390 [==============================] - 1s 4ms/step\n",
            "\n",
            "Final Accuracy :  0.08717948943376541\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 315)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 315, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 315, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 315, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 315, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 315, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 315, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 315, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 315, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 315, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            10368       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 315, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 8)            1096        concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 277,192\n",
            "Trainable params: 276,168\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset uWaveGestureLibrary_X ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/uWaveGestureLibrary_X_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/uWaveGestureLibrary_X_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  896 Number of test samples :  3582\n",
            "Number of classes :  8\n",
            "Sequence length :  315\n",
            "Class weights :  [0.91803279 1.03703704 1.05660377 1.01818182 0.88188976 1.00900901\n",
            " 1.         1.12      ]\n",
            "Train on 896 samples, validate on 3582 samples\n",
            "Epoch 1/4\n",
            " - 26s - loss: 1.9915 - accuracy: 0.2455 - val_loss: 2.3969 - val_accuracy: 0.1371\n",
            "\n",
            "Epoch 00001: loss improved from inf to 1.99149, saving model to ./weights/lstmfcn_8_cells_weights/uWaveGestureLibrary_X_weights.h5\n",
            "Epoch 2/4\n",
            " - 24s - loss: 1.7096 - accuracy: 0.4342 - val_loss: 2.2803 - val_accuracy: 0.1267\n",
            "\n",
            "Epoch 00002: loss improved from 1.99149 to 1.70956, saving model to ./weights/lstmfcn_8_cells_weights/uWaveGestureLibrary_X_weights.h5\n",
            "Epoch 3/4\n",
            " - 24s - loss: 1.6052 - accuracy: 0.5279 - val_loss: 2.2689 - val_accuracy: 0.1435\n",
            "\n",
            "Epoch 00003: loss improved from 1.70956 to 1.60521, saving model to ./weights/lstmfcn_8_cells_weights/uWaveGestureLibrary_X_weights.h5\n",
            "Epoch 4/4\n",
            " - 24s - loss: 1.5094 - accuracy: 0.5960 - val_loss: 2.0761 - val_accuracy: 0.1778\n",
            "\n",
            "Epoch 00004: loss improved from 1.60521 to 1.50941, saving model to ./weights/lstmfcn_8_cells_weights/uWaveGestureLibrary_X_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/uWaveGestureLibrary_X_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/uWaveGestureLibrary_X_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  896 Number of test samples :  3582\n",
            "Number of classes :  8\n",
            "Sequence length :  315\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/uWaveGestureLibrary_X_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "3582/3582 [==============================] - 13s 4ms/step\n",
            "\n",
            "Final Accuracy :  0.17783361673355103\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 315)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 315, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 315, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 315, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 315, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 315, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 315, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 315, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 315, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 315, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            10368       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 315, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 8)            1096        concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 277,192\n",
            "Trainable params: 276,168\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset uWaveGestureLibrary_Y ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/uWaveGestureLibrary_Y_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/uWaveGestureLibrary_Y_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  896 Number of test samples :  3582\n",
            "Number of classes :  8\n",
            "Sequence length :  315\n",
            "Class weights :  [0.91803279 1.03703704 1.05660377 1.01818182 0.88188976 1.00900901\n",
            " 1.         1.12      ]\n",
            "Train on 896 samples, validate on 3582 samples\n",
            "Epoch 1/4\n",
            " - 25s - loss: 1.8995 - accuracy: 0.2913 - val_loss: 2.0490 - val_accuracy: 0.1463\n",
            "\n",
            "Epoch 00001: loss improved from inf to 1.89948, saving model to ./weights/lstmfcn_8_cells_weights/uWaveGestureLibrary_Y_weights.h5\n",
            "Epoch 2/4\n",
            " - 24s - loss: 1.6534 - accuracy: 0.4487 - val_loss: 2.0121 - val_accuracy: 0.1245\n",
            "\n",
            "Epoch 00002: loss improved from 1.89948 to 1.65341, saving model to ./weights/lstmfcn_8_cells_weights/uWaveGestureLibrary_Y_weights.h5\n",
            "Epoch 3/4\n",
            " - 24s - loss: 1.5463 - accuracy: 0.5078 - val_loss: 2.0056 - val_accuracy: 0.1477\n",
            "\n",
            "Epoch 00003: loss improved from 1.65341 to 1.54633, saving model to ./weights/lstmfcn_8_cells_weights/uWaveGestureLibrary_Y_weights.h5\n",
            "Epoch 4/4\n",
            " - 24s - loss: 1.4642 - accuracy: 0.5469 - val_loss: 2.0658 - val_accuracy: 0.1248\n",
            "\n",
            "Epoch 00004: loss improved from 1.54633 to 1.46417, saving model to ./weights/lstmfcn_8_cells_weights/uWaveGestureLibrary_Y_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/uWaveGestureLibrary_Y_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/uWaveGestureLibrary_Y_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  896 Number of test samples :  3582\n",
            "Number of classes :  8\n",
            "Sequence length :  315\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/uWaveGestureLibrary_Y_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "3582/3582 [==============================] - 13s 4ms/step\n",
            "\n",
            "Final Accuracy :  0.12479061633348465\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 315)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 315, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 315, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 315, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 315, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 315, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 315, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 315, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 315, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 315, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            10368       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 315, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 8)            1096        concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 277,192\n",
            "Trainable params: 276,168\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset uWaveGestureLibrary_Z ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/uWaveGestureLibrary_Z_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/uWaveGestureLibrary_Z_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  896 Number of test samples :  3582\n",
            "Number of classes :  8\n",
            "Sequence length :  315\n",
            "Class weights :  [0.91803279 1.03703704 1.05660377 1.01818182 0.88188976 1.00900901\n",
            " 1.         1.12      ]\n",
            "Train on 896 samples, validate on 3582 samples\n",
            "Epoch 1/4\n",
            " - 25s - loss: 1.9514 - accuracy: 0.2645 - val_loss: 2.3944 - val_accuracy: 0.1323\n",
            "\n",
            "Epoch 00001: loss improved from inf to 1.95136, saving model to ./weights/lstmfcn_8_cells_weights/uWaveGestureLibrary_Z_weights.h5\n",
            "Epoch 2/4\n",
            " - 24s - loss: 1.6598 - accuracy: 0.4342 - val_loss: 2.3123 - val_accuracy: 0.1859\n",
            "\n",
            "Epoch 00002: loss improved from 1.95136 to 1.65977, saving model to ./weights/lstmfcn_8_cells_weights/uWaveGestureLibrary_Z_weights.h5\n",
            "Epoch 3/4\n",
            " - 24s - loss: 1.5552 - accuracy: 0.4888 - val_loss: 2.2105 - val_accuracy: 0.2272\n",
            "\n",
            "Epoch 00003: loss improved from 1.65977 to 1.55519, saving model to ./weights/lstmfcn_8_cells_weights/uWaveGestureLibrary_Z_weights.h5\n",
            "Epoch 4/4\n",
            " - 24s - loss: 1.4563 - accuracy: 0.5558 - val_loss: 2.1937 - val_accuracy: 0.1605\n",
            "\n",
            "Epoch 00004: loss improved from 1.55519 to 1.45629, saving model to ./weights/lstmfcn_8_cells_weights/uWaveGestureLibrary_Z_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/uWaveGestureLibrary_Z_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/uWaveGestureLibrary_Z_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  896 Number of test samples :  3582\n",
            "Number of classes :  8\n",
            "Sequence length :  315\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/uWaveGestureLibrary_Z_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "3582/3582 [==============================] - 13s 4ms/step\n",
            "\n",
            "Final Accuracy :  0.16052484512329102\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 343)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 343, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 343, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 343, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 343, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 343, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 343, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 343, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 343, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 343, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            11264       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 343, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 2)            274         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 277,266\n",
            "Trainable params: 276,242\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset ToeSegmentation2 ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ToeSegmentation2_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ToeSegmentation2_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  36 Number of test samples :  130\n",
            "Number of classes :  2\n",
            "Sequence length :  343\n",
            "Class weights :  [1. 1.]\n",
            "Train on 36 samples, validate on 130 samples\n",
            "Epoch 1/4\n",
            " - 2s - loss: 0.7180 - accuracy: 0.4722 - val_loss: 0.7648 - val_accuracy: 0.2000\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.71798, saving model to ./weights/lstmfcn_8_cells_weights/ToeSegmentation2_weights.h5\n",
            "Epoch 2/4\n",
            " - 1s - loss: 0.4723 - accuracy: 0.8889 - val_loss: 1.0659 - val_accuracy: 0.1846\n",
            "\n",
            "Epoch 00002: loss improved from 0.71798 to 0.47234, saving model to ./weights/lstmfcn_8_cells_weights/ToeSegmentation2_weights.h5\n",
            "Epoch 3/4\n",
            " - 1s - loss: 0.3832 - accuracy: 0.9167 - val_loss: 1.4487 - val_accuracy: 0.1846\n",
            "\n",
            "Epoch 00003: loss improved from 0.47234 to 0.38317, saving model to ./weights/lstmfcn_8_cells_weights/ToeSegmentation2_weights.h5\n",
            "Epoch 4/4\n",
            " - 1s - loss: 0.3161 - accuracy: 0.9167 - val_loss: 1.8164 - val_accuracy: 0.1846\n",
            "\n",
            "Epoch 00004: loss improved from 0.38317 to 0.31608, saving model to ./weights/lstmfcn_8_cells_weights/ToeSegmentation2_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ToeSegmentation2_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ToeSegmentation2_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  36 Number of test samples :  130\n",
            "Number of classes :  2\n",
            "Sequence length :  343\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/ToeSegmentation2_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "130/130 [==============================] - 1s 5ms/step\n",
            "\n",
            "Final Accuracy :  0.1846153885126114\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 345)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 345, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 345, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 345, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 345, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 345, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 345, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 345, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 345, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 345, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            11328       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 345, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 4)            548         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 277,604\n",
            "Trainable params: 276,580\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset DiatomSizeReduction ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/DiatomSizeReduction_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/DiatomSizeReduction_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  16 Number of test samples :  306\n",
            "Number of classes :  4\n",
            "Sequence length :  345\n",
            "Class weights :  [4.         0.66666667 0.8        1.        ]\n",
            "Train on 16 samples, validate on 306 samples\n",
            "Epoch 1/4\n",
            " - 3s - loss: 1.6498 - accuracy: 0.2500 - val_loss: 1.9713 - val_accuracy: 0.2843\n",
            "\n",
            "Epoch 00001: loss improved from inf to 1.64978, saving model to ./weights/lstmfcn_8_cells_weights/DiatomSizeReduction_weights.h5\n",
            "Epoch 2/4\n",
            " - 1s - loss: 1.4201 - accuracy: 0.2500 - val_loss: 1.6935 - val_accuracy: 0.2843\n",
            "\n",
            "Epoch 00002: loss improved from 1.64978 to 1.42015, saving model to ./weights/lstmfcn_8_cells_weights/DiatomSizeReduction_weights.h5\n",
            "Epoch 3/4\n",
            " - 1s - loss: 1.3322 - accuracy: 0.2500 - val_loss: 1.5374 - val_accuracy: 0.1078\n",
            "\n",
            "Epoch 00003: loss improved from 1.42015 to 1.33219, saving model to ./weights/lstmfcn_8_cells_weights/DiatomSizeReduction_weights.h5\n",
            "Epoch 4/4\n",
            " - 1s - loss: 1.2072 - accuracy: 0.5000 - val_loss: 1.4976 - val_accuracy: 0.1078\n",
            "\n",
            "Epoch 00004: loss improved from 1.33219 to 1.20721, saving model to ./weights/lstmfcn_8_cells_weights/DiatomSizeReduction_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/DiatomSizeReduction_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/DiatomSizeReduction_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  16 Number of test samples :  306\n",
            "Number of classes :  4\n",
            "Sequence length :  345\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/DiatomSizeReduction_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "306/306 [==============================] - 1s 5ms/step\n",
            "\n",
            "Final Accuracy :  0.10784313827753067\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 577)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 577, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 577, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 577, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 577, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 577, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 577, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 577, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 577, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 577, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            18752       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 577, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 4)            548         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 285,028\n",
            "Trainable params: 284,004\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset car ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Car_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Car_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  60 Number of test samples :  60\n",
            "Number of classes :  4\n",
            "Sequence length :  577\n",
            "Class weights :  [0.9375     0.9375     1.36363636 0.88235294]\n",
            "Train on 60 samples, validate on 60 samples\n",
            "Epoch 1/4\n",
            " - 3s - loss: 1.5683 - accuracy: 0.2833 - val_loss: 1.5766 - val_accuracy: 0.2167\n",
            "\n",
            "Epoch 00001: loss improved from inf to 1.56835, saving model to ./weights/lstmfcn_8_cells_weights/car_weights.h5\n",
            "Epoch 2/4\n",
            " - 2s - loss: 1.4296 - accuracy: 0.4000 - val_loss: 1.5575 - val_accuracy: 0.2167\n",
            "\n",
            "Epoch 00002: loss improved from 1.56835 to 1.42959, saving model to ./weights/lstmfcn_8_cells_weights/car_weights.h5\n",
            "Epoch 3/4\n",
            " - 2s - loss: 1.3237 - accuracy: 0.4000 - val_loss: 1.5704 - val_accuracy: 0.2333\n",
            "\n",
            "Epoch 00003: loss improved from 1.42959 to 1.32369, saving model to ./weights/lstmfcn_8_cells_weights/car_weights.h5\n",
            "Epoch 4/4\n",
            " - 2s - loss: 1.3026 - accuracy: 0.4667 - val_loss: 1.6348 - val_accuracy: 0.2333\n",
            "\n",
            "Epoch 00004: loss improved from 1.32369 to 1.30258, saving model to ./weights/lstmfcn_8_cells_weights/car_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Car_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Car_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  60 Number of test samples :  60\n",
            "Number of classes :  4\n",
            "Sequence length :  577\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/car_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "60/60 [==============================] - 1s 9ms/step\n",
            "\n",
            "Final Accuracy :  0.23333333432674408\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 128)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 128, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 128, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 128, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 128, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 128, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 128, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 128, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 128, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 128, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            4384        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 128, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 3)            411         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 270,523\n",
            "Trainable params: 269,499\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset CBF ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/CBF_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/CBF_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  30 Number of test samples :  900\n",
            "Number of classes :  3\n",
            "Sequence length :  128\n",
            "Class weights :  [1.         0.83333333 1.25      ]\n",
            "Train on 30 samples, validate on 900 samples\n",
            "Epoch 1/4\n",
            " - 3s - loss: 1.2070 - accuracy: 0.2667 - val_loss: 1.0853 - val_accuracy: 0.3333\n",
            "\n",
            "Epoch 00001: loss improved from inf to 1.20698, saving model to ./weights/lstmfcn_8_cells_weights/CBF_weights.h5\n",
            "Epoch 2/4\n",
            " - 2s - loss: 0.8598 - accuracy: 0.5667 - val_loss: 1.1036 - val_accuracy: 0.3767\n",
            "\n",
            "Epoch 00002: loss improved from 1.20698 to 0.85976, saving model to ./weights/lstmfcn_8_cells_weights/CBF_weights.h5\n",
            "Epoch 3/4\n",
            " - 2s - loss: 0.6632 - accuracy: 0.6667 - val_loss: 1.1624 - val_accuracy: 0.5200\n",
            "\n",
            "Epoch 00003: loss improved from 0.85976 to 0.66319, saving model to ./weights/lstmfcn_8_cells_weights/CBF_weights.h5\n",
            "Epoch 4/4\n",
            " - 2s - loss: 0.5545 - accuracy: 0.8667 - val_loss: 1.2136 - val_accuracy: 0.5978\n",
            "\n",
            "Epoch 00004: loss improved from 0.66319 to 0.55448, saving model to ./weights/lstmfcn_8_cells_weights/CBF_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/CBF_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/CBF_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  30 Number of test samples :  900\n",
            "Number of classes :  3\n",
            "Sequence length :  128\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/CBF_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "900/900 [==============================] - 1s 2ms/step\n",
            "\n",
            "Final Accuracy :  0.597777783870697\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 1639)      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 1639, 1)      0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 1639, 128)    1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 1639, 128)    512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 1639, 128)    0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 1639, 256)    164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 1639, 256)    1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 1639, 256)    0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 1639, 128)    98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 1639, 128)    512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            52736       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 1639, 128)    0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 4)            548         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 319,012\n",
            "Trainable params: 317,988\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset CinC_ECG_torso ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/CinC_ECG_torso_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/CinC_ECG_torso_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  40 Number of test samples :  1380\n",
            "Number of classes :  4\n",
            "Sequence length :  1639\n",
            "Class weights :  [2.         0.76923077 0.83333333 1.        ]\n",
            "Train on 40 samples, validate on 1380 samples\n",
            "Epoch 1/4\n",
            " - 31s - loss: 1.4170 - accuracy: 0.3000 - val_loss: 1.4203 - val_accuracy: 0.2493\n",
            "\n",
            "Epoch 00001: loss improved from inf to 1.41703, saving model to ./weights/lstmfcn_8_cells_weights/CinC_ECG_torso_weights.h5\n",
            "Epoch 2/4\n",
            " - 29s - loss: 1.2754 - accuracy: 0.3500 - val_loss: 1.3999 - val_accuracy: 0.2486\n",
            "\n",
            "Epoch 00002: loss improved from 1.41703 to 1.27535, saving model to ./weights/lstmfcn_8_cells_weights/CinC_ECG_torso_weights.h5\n",
            "Epoch 3/4\n",
            " - 29s - loss: 1.1434 - accuracy: 0.5000 - val_loss: 1.3894 - val_accuracy: 0.2486\n",
            "\n",
            "Epoch 00003: loss improved from 1.27535 to 1.14336, saving model to ./weights/lstmfcn_8_cells_weights/CinC_ECG_torso_weights.h5\n",
            "Epoch 4/4\n",
            " - 29s - loss: 1.0859 - accuracy: 0.6250 - val_loss: 1.4267 - val_accuracy: 0.3522\n",
            "\n",
            "Epoch 00004: loss improved from 1.14336 to 1.08587, saving model to ./weights/lstmfcn_8_cells_weights/CinC_ECG_torso_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/CinC_ECG_torso_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/CinC_ECG_torso_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  40 Number of test samples :  1380\n",
            "Number of classes :  4\n",
            "Sequence length :  1639\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/CinC_ECG_torso_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "1380/1380 [==============================] - 27s 20ms/step\n",
            "\n",
            "Final Accuracy :  0.35217392444610596\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 720)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 720, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 720, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 720, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 720, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 720, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 720, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 720, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 720, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 720, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            23328       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 720, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 2)            274         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 289,330\n",
            "Trainable params: 288,306\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset Computers ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Computers_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Computers_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  250 Number of test samples :  250\n",
            "Number of classes :  2\n",
            "Sequence length :  720\n",
            "Class weights :  [1. 1.]\n",
            "Train on 250 samples, validate on 250 samples\n",
            "Epoch 1/4\n",
            " - 10s - loss: 0.6382 - accuracy: 0.6120 - val_loss: 1.2536 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.63815, saving model to ./weights/lstmfcn_8_cells_weights/Computers_weights.h5\n",
            "Epoch 2/4\n",
            " - 9s - loss: 0.6120 - accuracy: 0.6640 - val_loss: 1.4946 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00002: loss improved from 0.63815 to 0.61197, saving model to ./weights/lstmfcn_8_cells_weights/Computers_weights.h5\n",
            "Epoch 3/4\n",
            " - 9s - loss: 0.5624 - accuracy: 0.7040 - val_loss: 2.0348 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00003: loss improved from 0.61197 to 0.56244, saving model to ./weights/lstmfcn_8_cells_weights/Computers_weights.h5\n",
            "Epoch 4/4\n",
            " - 9s - loss: 0.5582 - accuracy: 0.7000 - val_loss: 2.3659 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00004: loss improved from 0.56244 to 0.55815, saving model to ./weights/lstmfcn_8_cells_weights/Computers_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Computers_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Computers_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  250 Number of test samples :  250\n",
            "Number of classes :  2\n",
            "Sequence length :  720\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/Computers_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "250/250 [==============================] - 2s 9ms/step\n",
            "\n",
            "Final Accuracy :  0.5\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 512)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 512, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 512, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 512, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 512, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 512, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 512, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 512, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 512, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 512, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            16672       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 512, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 2)            274         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 282,674\n",
            "Trainable params: 281,650\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset Earthquakes ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Earthquakes_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Earthquakes_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  139 Number of test samples :  322\n",
            "Number of classes :  2\n",
            "Sequence length :  512\n",
            "Class weights :  [0.66826923 1.98571429]\n",
            "Train on 139 samples, validate on 322 samples\n",
            "Epoch 1/4\n",
            " - 6s - loss: 0.5692 - accuracy: 0.7482 - val_loss: 0.5748 - val_accuracy: 0.8199\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.56922, saving model to ./weights/lstmfcn_8_cells_weights/Earthquakes_weights.h5\n",
            "Epoch 2/4\n",
            " - 5s - loss: 0.5141 - accuracy: 0.7482 - val_loss: 0.6317 - val_accuracy: 0.8199\n",
            "\n",
            "Epoch 00002: loss improved from 0.56922 to 0.51415, saving model to ./weights/lstmfcn_8_cells_weights/Earthquakes_weights.h5\n",
            "Epoch 3/4\n",
            " - 5s - loss: 0.5125 - accuracy: 0.7482 - val_loss: 0.6597 - val_accuracy: 0.8199\n",
            "\n",
            "Epoch 00003: loss improved from 0.51415 to 0.51254, saving model to ./weights/lstmfcn_8_cells_weights/Earthquakes_weights.h5\n",
            "Epoch 4/4\n",
            " - 5s - loss: 0.5105 - accuracy: 0.7482 - val_loss: 0.7387 - val_accuracy: 0.8199\n",
            "\n",
            "Epoch 00004: loss improved from 0.51254 to 0.51046, saving model to ./weights/lstmfcn_8_cells_weights/Earthquakes_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Earthquakes_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Earthquakes_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  139 Number of test samples :  322\n",
            "Number of classes :  2\n",
            "Sequence length :  512\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/Earthquakes_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "322/322 [==============================] - 2s 6ms/step\n",
            "\n",
            "Final Accuracy :  0.8198757767677307\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 140)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 140, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 140, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 140, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 140, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 140, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 140, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 140, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 140, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 140, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            4768        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 140, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 5)            685         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 271,181\n",
            "Trainable params: 270,157\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset ECG5000 ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ECG5000_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ECG5000_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  500 Number of test samples :  4500\n",
            "Number of classes :  5\n",
            "Sequence length :  140\n",
            "Class weights :  [ 0.34246575  0.56497175 10.          5.26315789 50.        ]\n",
            "Train on 500 samples, validate on 4500 samples\n",
            "Epoch 1/4\n",
            " - 11s - loss: 1.4523 - accuracy: 0.4200 - val_loss: 1.3313 - val_accuracy: 0.5838\n",
            "\n",
            "Epoch 00001: loss improved from inf to 1.45227, saving model to ./weights/lstmfcn_8_cells_weights/ECG5000_weights.h5\n",
            "Epoch 2/4\n",
            " - 10s - loss: 0.7549 - accuracy: 0.8940 - val_loss: 0.8598 - val_accuracy: 0.7027\n",
            "\n",
            "Epoch 00002: loss improved from 1.45227 to 0.75491, saving model to ./weights/lstmfcn_8_cells_weights/ECG5000_weights.h5\n",
            "Epoch 3/4\n",
            " - 10s - loss: 0.5525 - accuracy: 0.9080 - val_loss: 0.7220 - val_accuracy: 0.8820\n",
            "\n",
            "Epoch 00003: loss improved from 0.75491 to 0.55254, saving model to ./weights/lstmfcn_8_cells_weights/ECG5000_weights.h5\n",
            "Epoch 4/4\n",
            " - 10s - loss: 0.4628 - accuracy: 0.9100 - val_loss: 1.0052 - val_accuracy: 0.3909\n",
            "\n",
            "Epoch 00004: loss improved from 0.55254 to 0.46284, saving model to ./weights/lstmfcn_8_cells_weights/ECG5000_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ECG5000_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ECG5000_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  500 Number of test samples :  4500\n",
            "Number of classes :  5\n",
            "Sequence length :  140\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/ECG5000_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "4500/4500 [==============================] - 7s 2ms/step\n",
            "\n",
            "Final Accuracy :  0.39088889956474304\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 96)        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 96, 1)        0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 96, 128)      1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 96, 128)      512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 96, 128)      0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 96, 256)      164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 96, 256)      1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 96, 256)      0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 96, 128)      98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 96, 128)      512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            3360        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 96, 128)      0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 7)            959         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 270,047\n",
            "Trainable params: 269,023\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset ElectricDevices ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ElectricDevices_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ElectricDevices_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  8926 Number of test samples :  7711\n",
            "Number of classes :  7\n",
            "Sequence length :  96\n",
            "Class weights :  [1.75397917 0.57155664 1.49840524 0.86509013 0.52998456 2.50519225\n",
            " 1.75156986]\n",
            "Train on 8926 samples, validate on 7711 samples\n",
            "Epoch 1/4\n",
            " - 46s - loss: 0.9420 - accuracy: 0.6652 - val_loss: 1.4968 - val_accuracy: 0.5523\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.94198, saving model to ./weights/lstmfcn_8_cells_weights/ElectricDevices_weights.h5\n",
            "Epoch 2/4\n",
            " - 45s - loss: 0.6809 - accuracy: 0.7579 - val_loss: 1.4613 - val_accuracy: 0.5726\n",
            "\n",
            "Epoch 00002: loss improved from 0.94198 to 0.68086, saving model to ./weights/lstmfcn_8_cells_weights/ElectricDevices_weights.h5\n",
            "Epoch 3/4\n",
            " - 45s - loss: 0.5878 - accuracy: 0.7883 - val_loss: 1.4422 - val_accuracy: 0.6504\n",
            "\n",
            "Epoch 00003: loss improved from 0.68086 to 0.58783, saving model to ./weights/lstmfcn_8_cells_weights/ElectricDevices_weights.h5\n",
            "Epoch 4/4\n",
            " - 45s - loss: 0.5404 - accuracy: 0.8058 - val_loss: 1.5701 - val_accuracy: 0.6358\n",
            "\n",
            "Epoch 00004: loss improved from 0.58783 to 0.54041, saving model to ./weights/lstmfcn_8_cells_weights/ElectricDevices_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ElectricDevices_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/ElectricDevices_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  8926 Number of test samples :  7711\n",
            "Number of classes :  7\n",
            "Sequence length :  96\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/ElectricDevices_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "7711/7711 [==============================] - 9s 1ms/step\n",
            "\n",
            "Final Accuracy :  0.635844886302948\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 131)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 131, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 131, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 131, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 131, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 131, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 131, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 131, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 131, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 131, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            4480        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 131, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 14)           1918        concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 272,126\n",
            "Trainable params: 271,102\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset FaceAll ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/FaceAll_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/FaceAll_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  560 Number of test samples :  1690\n",
            "Number of classes :  14\n",
            "Sequence length :  131\n",
            "Class weights :  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "Train on 560 samples, validate on 1690 samples\n",
            "Epoch 1/4\n",
            " - 7s - loss: 2.6274 - accuracy: 0.1268 - val_loss: 2.7157 - val_accuracy: 0.1604\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.62743, saving model to ./weights/lstmfcn_8_cells_weights/FaceAll_weights.h5\n",
            "Epoch 2/4\n",
            " - 6s - loss: 2.2483 - accuracy: 0.3786 - val_loss: 2.8289 - val_accuracy: 0.0509\n",
            "\n",
            "Epoch 00002: loss improved from 2.62743 to 2.24826, saving model to ./weights/lstmfcn_8_cells_weights/FaceAll_weights.h5\n",
            "Epoch 3/4\n",
            " - 6s - loss: 2.0433 - accuracy: 0.5589 - val_loss: 3.0580 - val_accuracy: 0.1148\n",
            "\n",
            "Epoch 00003: loss improved from 2.24826 to 2.04328, saving model to ./weights/lstmfcn_8_cells_weights/FaceAll_weights.h5\n",
            "Epoch 4/4\n",
            " - 6s - loss: 1.8932 - accuracy: 0.6696 - val_loss: 3.3077 - val_accuracy: 0.1533\n",
            "\n",
            "Epoch 00004: loss improved from 2.04328 to 1.89319, saving model to ./weights/lstmfcn_8_cells_weights/FaceAll_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/FaceAll_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/FaceAll_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  560 Number of test samples :  1690\n",
            "Number of classes :  14\n",
            "Sequence length :  131\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/FaceAll_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "1690/1690 [==============================] - 3s 2ms/step\n",
            "\n",
            "Final Accuracy :  0.153254434466362\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 350)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 350, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 350, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 350, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 350, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 350, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 350, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 350, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 350, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 350, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            11488       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 350, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 4)            548         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 277,764\n",
            "Trainable params: 276,740\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset FaceFour ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/FaceFour_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/FaceFour_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  24 Number of test samples :  88\n",
            "Number of classes :  4\n",
            "Sequence length :  350\n",
            "Class weights :  [0.75 0.75 2.   1.2 ]\n",
            "Train on 24 samples, validate on 88 samples\n",
            "Epoch 1/4\n",
            " - 2s - loss: 1.3156 - accuracy: 0.3750 - val_loss: 1.5499 - val_accuracy: 0.2955\n",
            "\n",
            "Epoch 00001: loss improved from inf to 1.31563, saving model to ./weights/lstmfcn_8_cells_weights/FaceFour_weights.h5\n",
            "Epoch 2/4\n",
            " - 1s - loss: 1.1435 - accuracy: 0.4583 - val_loss: 1.4477 - val_accuracy: 0.2955\n",
            "\n",
            "Epoch 00002: loss improved from 1.31563 to 1.14345, saving model to ./weights/lstmfcn_8_cells_weights/FaceFour_weights.h5\n",
            "Epoch 3/4\n",
            " - 1s - loss: 1.0316 - accuracy: 0.6667 - val_loss: 1.3885 - val_accuracy: 0.4432\n",
            "\n",
            "Epoch 00003: loss improved from 1.14345 to 1.03165, saving model to ./weights/lstmfcn_8_cells_weights/FaceFour_weights.h5\n",
            "Epoch 4/4\n",
            " - 1s - loss: 0.9811 - accuracy: 0.7500 - val_loss: 1.3627 - val_accuracy: 0.4318\n",
            "\n",
            "Epoch 00004: loss improved from 1.03165 to 0.98109, saving model to ./weights/lstmfcn_8_cells_weights/FaceFour_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/FaceFour_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/FaceFour_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  24 Number of test samples :  88\n",
            "Number of classes :  4\n",
            "Sequence length :  350\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/FaceFour_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "88/88 [==============================] - 0s 6ms/step\n",
            "\n",
            "Final Accuracy :  0.4318181872367859\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 131)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 131, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 131, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 131, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 131, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 131, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 131, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 131, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 131, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 131, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            4480        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 131, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 14)           1918        concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 272,126\n",
            "Trainable params: 271,102\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset FacesUCR ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/FacesUCR_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/FacesUCR_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  200 Number of test samples :  2050\n",
            "Number of classes :  14\n",
            "Sequence length :  131\n",
            "Class weights :  [1.0989011  1.19047619 1.42857143 0.64935065 0.95238095 1.0989011\n",
            " 1.02040816 0.52910053 1.19047619 1.78571429 3.57142857 1.2987013\n",
            " 0.43290043 2.38095238]\n",
            "Train on 200 samples, validate on 2050 samples\n",
            "Epoch 1/4\n",
            " - 5s - loss: 2.6341 - accuracy: 0.0850 - val_loss: 3.1170 - val_accuracy: 0.0727\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.63410, saving model to ./weights/lstmfcn_8_cells_weights/FacesUCR_weights.h5\n",
            "Epoch 2/4\n",
            " - 4s - loss: 2.2711 - accuracy: 0.2400 - val_loss: 3.2259 - val_accuracy: 0.0620\n",
            "\n",
            "Epoch 00002: loss improved from 2.63410 to 2.27113, saving model to ./weights/lstmfcn_8_cells_weights/FacesUCR_weights.h5\n",
            "Epoch 3/4\n",
            " - 4s - loss: 2.0861 - accuracy: 0.4450 - val_loss: 3.3716 - val_accuracy: 0.0673\n",
            "\n",
            "Epoch 00003: loss improved from 2.27113 to 2.08613, saving model to ./weights/lstmfcn_8_cells_weights/FacesUCR_weights.h5\n",
            "Epoch 4/4\n",
            " - 4s - loss: 1.9499 - accuracy: 0.4700 - val_loss: 3.4917 - val_accuracy: 0.1010\n",
            "\n",
            "Epoch 00004: loss improved from 2.08613 to 1.94986, saving model to ./weights/lstmfcn_8_cells_weights/FacesUCR_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/FacesUCR_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/FacesUCR_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  200 Number of test samples :  2050\n",
            "Number of classes :  14\n",
            "Sequence length :  131\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/FacesUCR_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "2050/2050 [==============================] - 3s 2ms/step\n",
            "\n",
            "Final Accuracy :  0.10097561031579971\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 463)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 463, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 463, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 463, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 463, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 463, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 463, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 463, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 463, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 463, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            15104       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 463, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 7)            959         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 281,791\n",
            "Trainable params: 280,767\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset Fish ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/FISH_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/FISH_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  175 Number of test samples :  175\n",
            "Number of classes :  7\n",
            "Sequence length :  463\n",
            "Class weights :  [0.96153846 1.         0.89285714 1.19047619 1.13636364 1.\n",
            " 0.89285714]\n",
            "Train on 175 samples, validate on 175 samples\n",
            "Epoch 1/4\n",
            " - 5s - loss: 2.0495 - accuracy: 0.1314 - val_loss: 2.1442 - val_accuracy: 0.1657\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.04951, saving model to ./weights/lstmfcn_8_cells_weights/Fish_weights.h5\n",
            "Epoch 2/4\n",
            " - 4s - loss: 1.9337 - accuracy: 0.1943 - val_loss: 2.0936 - val_accuracy: 0.1657\n",
            "\n",
            "Epoch 00002: loss improved from 2.04951 to 1.93372, saving model to ./weights/lstmfcn_8_cells_weights/Fish_weights.h5\n",
            "Epoch 3/4\n",
            " - 4s - loss: 1.9034 - accuracy: 0.2343 - val_loss: 2.1189 - val_accuracy: 0.1657\n",
            "\n",
            "Epoch 00003: loss improved from 1.93372 to 1.90342, saving model to ./weights/lstmfcn_8_cells_weights/Fish_weights.h5\n",
            "Epoch 4/4\n",
            " - 4s - loss: 1.8817 - accuracy: 0.3486 - val_loss: 2.1939 - val_accuracy: 0.1371\n",
            "\n",
            "Epoch 00004: loss improved from 1.90342 to 1.88171, saving model to ./weights/lstmfcn_8_cells_weights/Fish_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/FISH_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/FISH_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  175 Number of test samples :  175\n",
            "Number of classes :  7\n",
            "Sequence length :  463\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/Fish_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "175/175 [==============================] - 1s 6ms/step\n",
            "\n",
            "Final Accuracy :  0.1371428519487381\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 500)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 500, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 500, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 500, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 500, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 500, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 500, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 500, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 500, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 500, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            16288       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 500, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 2)            274         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 282,290\n",
            "Trainable params: 281,266\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset FordA ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/FordA_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/FordA_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  1320 Number of test samples :  3601\n",
            "Number of classes :  2\n",
            "Sequence length :  500\n",
            "Class weights :  [0.969163   1.03286385]\n",
            "Train on 1320 samples, validate on 3601 samples\n",
            "Epoch 1/4\n",
            " - 48s - loss: 0.5136 - accuracy: 0.7227 - val_loss: 0.8421 - val_accuracy: 0.5435\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.51364, saving model to ./weights/lstmfcn_8_cells_weights/FordA_weights.h5\n",
            "Epoch 2/4\n",
            " - 47s - loss: 0.4004 - accuracy: 0.8068 - val_loss: 1.0354 - val_accuracy: 0.5446\n",
            "\n",
            "Epoch 00002: loss improved from 0.51364 to 0.40041, saving model to ./weights/lstmfcn_8_cells_weights/FordA_weights.h5\n",
            "Epoch 3/4\n",
            " - 47s - loss: 0.3702 - accuracy: 0.8227 - val_loss: 0.5891 - val_accuracy: 0.6993\n",
            "\n",
            "Epoch 00003: loss improved from 0.40041 to 0.37023, saving model to ./weights/lstmfcn_8_cells_weights/FordA_weights.h5\n",
            "Epoch 4/4\n",
            " - 47s - loss: 0.3406 - accuracy: 0.8409 - val_loss: 0.4195 - val_accuracy: 0.7748\n",
            "\n",
            "Epoch 00004: loss improved from 0.37023 to 0.34063, saving model to ./weights/lstmfcn_8_cells_weights/FordA_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/FordA_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/FordA_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  1320 Number of test samples :  3601\n",
            "Number of classes :  2\n",
            "Sequence length :  500\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/FordA_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "3601/3601 [==============================] - 21s 6ms/step\n",
            "\n",
            "Final Accuracy :  0.7747848033905029\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 500)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 500, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 500, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 500, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 500, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 500, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 500, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 500, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 500, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 500, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            16288       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 500, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 2)            274         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 282,290\n",
            "Trainable params: 281,266\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset FordB ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/FordB_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/FordB_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  810 Number of test samples :  3636\n",
            "Number of classes :  2\n",
            "Sequence length :  500\n",
            "Class weights :  [1.00997506 0.99022005]\n",
            "Train on 810 samples, validate on 3636 samples\n",
            "Epoch 1/4\n",
            " - 39s - loss: 0.6690 - accuracy: 0.5802 - val_loss: 0.6452 - val_accuracy: 0.5231\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.66901, saving model to ./weights/lstmfcn_8_cells_weights/FordB_weights.h5\n",
            "Epoch 2/4\n",
            " - 38s - loss: 0.6092 - accuracy: 0.6407 - val_loss: 0.6283 - val_accuracy: 0.5476\n",
            "\n",
            "Epoch 00002: loss improved from 0.66901 to 0.60916, saving model to ./weights/lstmfcn_8_cells_weights/FordB_weights.h5\n",
            "Epoch 3/4\n",
            " - 37s - loss: 0.5694 - accuracy: 0.6840 - val_loss: 0.7051 - val_accuracy: 0.5025\n",
            "\n",
            "Epoch 00003: loss improved from 0.60916 to 0.56939, saving model to ./weights/lstmfcn_8_cells_weights/FordB_weights.h5\n",
            "Epoch 4/4\n",
            " - 37s - loss: 0.5343 - accuracy: 0.7160 - val_loss: 1.1926 - val_accuracy: 0.4884\n",
            "\n",
            "Epoch 00004: loss improved from 0.56939 to 0.53430, saving model to ./weights/lstmfcn_8_cells_weights/FordB_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/FordB_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/FordB_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  810 Number of test samples :  3636\n",
            "Number of classes :  2\n",
            "Sequence length :  500\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/FordB_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "3636/3636 [==============================] - 22s 6ms/step\n",
            "\n",
            "Final Accuracy :  0.4884488582611084\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 150)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 150, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 150, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 150, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 150, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 150, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 150, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 150, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 150, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 150, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            5088        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 150, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 2)            274         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 271,090\n",
            "Trainable params: 270,066\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset Gun_Point ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Gun_Point_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Gun_Point_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  50 Number of test samples :  150\n",
            "Number of classes :  2\n",
            "Sequence length :  150\n",
            "Class weights :  [1.04166667 0.96153846]\n",
            "Train on 50 samples, validate on 150 samples\n",
            "Epoch 1/4\n",
            " - 2s - loss: 0.6387 - accuracy: 0.6200 - val_loss: 0.8874 - val_accuracy: 0.4933\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.63872, saving model to ./weights/lstmfcn_8_cells_weights/Gun_Point_weights.h5\n",
            "Epoch 2/4\n",
            " - 1s - loss: 0.6102 - accuracy: 0.6600 - val_loss: 0.8869 - val_accuracy: 0.4933\n",
            "\n",
            "Epoch 00002: loss improved from 0.63872 to 0.61020, saving model to ./weights/lstmfcn_8_cells_weights/Gun_Point_weights.h5\n",
            "Epoch 3/4\n",
            " - 1s - loss: 0.5625 - accuracy: 0.7200 - val_loss: 0.8316 - val_accuracy: 0.4933\n",
            "\n",
            "Epoch 00003: loss improved from 0.61020 to 0.56246, saving model to ./weights/lstmfcn_8_cells_weights/Gun_Point_weights.h5\n",
            "Epoch 4/4\n",
            " - 1s - loss: 0.4998 - accuracy: 0.8600 - val_loss: 0.7906 - val_accuracy: 0.4933\n",
            "\n",
            "Epoch 00004: loss improved from 0.56246 to 0.49981, saving model to ./weights/lstmfcn_8_cells_weights/Gun_Point_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Gun_Point_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Gun_Point_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  50 Number of test samples :  150\n",
            "Number of classes :  2\n",
            "Sequence length :  150\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/Gun_Point_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "150/150 [==============================] - 0s 3ms/step\n",
            "\n",
            "Final Accuracy :  0.4933333396911621\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 431)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 431, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 431, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 431, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 431, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 431, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 431, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 431, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 431, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 431, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            14080       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 431, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 2)            274         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 280,082\n",
            "Trainable params: 279,058\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset Ham ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Ham_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Ham_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  109 Number of test samples :  105\n",
            "Number of classes :  2\n",
            "Sequence length :  431\n",
            "Class weights :  [1.04807692 0.95614035]\n",
            "Train on 109 samples, validate on 105 samples\n",
            "Epoch 1/4\n",
            " - 3s - loss: 0.7283 - accuracy: 0.4037 - val_loss: 0.7140 - val_accuracy: 0.4476\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.72831, saving model to ./weights/lstmfcn_8_cells_weights/Ham_weights.h5\n",
            "Epoch 2/4\n",
            " - 2s - loss: 0.6480 - accuracy: 0.5413 - val_loss: 0.7966 - val_accuracy: 0.4857\n",
            "\n",
            "Epoch 00002: loss improved from 0.72831 to 0.64795, saving model to ./weights/lstmfcn_8_cells_weights/Ham_weights.h5\n",
            "Epoch 3/4\n",
            " - 2s - loss: 0.6225 - accuracy: 0.6514 - val_loss: 0.8986 - val_accuracy: 0.4857\n",
            "\n",
            "Epoch 00003: loss improved from 0.64795 to 0.62246, saving model to ./weights/lstmfcn_8_cells_weights/Ham_weights.h5\n",
            "Epoch 4/4\n",
            " - 2s - loss: 0.6154 - accuracy: 0.7064 - val_loss: 0.9552 - val_accuracy: 0.4857\n",
            "\n",
            "Epoch 00004: loss improved from 0.62246 to 0.61543, saving model to ./weights/lstmfcn_8_cells_weights/Ham_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Ham_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Ham_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  109 Number of test samples :  105\n",
            "Number of classes :  2\n",
            "Sequence length :  431\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/Ham_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "105/105 [==============================] - 1s 6ms/step\n",
            "\n",
            "Final Accuracy :  0.48571428656578064\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 2709)      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 2709, 1)      0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 2709, 128)    1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 2709, 128)    512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 2709, 128)    0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 2709, 256)    164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 2709, 256)    1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 2709, 256)    0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 2709, 128)    98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 2709, 128)    512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            86976       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 2709, 128)    0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 2)            274         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 352,978\n",
            "Trainable params: 351,954\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset HandOutlines ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/HandOutlines_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/HandOutlines_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  370 Number of test samples :  1000\n",
            "Number of classes :  2\n",
            "Sequence length :  2709\n",
            "Class weights :  [1.39097744 0.78059072]\n",
            "Train on 370 samples, validate on 1000 samples\n",
            "Epoch 1/4\n",
            " - 75s - loss: 0.6344 - accuracy: 0.6351 - val_loss: 0.7186 - val_accuracy: 0.6380\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.63441, saving model to ./weights/lstmfcn_8_cells_weights/HandOutlines_weights.h5\n",
            "Epoch 2/4\n",
            " - 70s - loss: 0.6534 - accuracy: 0.6459 - val_loss: 0.7295 - val_accuracy: 0.6380\n",
            "\n",
            "Epoch 00002: loss did not improve from 0.63441\n",
            "Epoch 3/4\n",
            " - 70s - loss: 0.6273 - accuracy: 0.6486 - val_loss: 0.6483 - val_accuracy: 0.6380\n",
            "\n",
            "Epoch 00003: loss improved from 0.63441 to 0.62731, saving model to ./weights/lstmfcn_8_cells_weights/HandOutlines_weights.h5\n",
            "Epoch 4/4\n",
            " - 69s - loss: 0.6246 - accuracy: 0.6486 - val_loss: 0.6412 - val_accuracy: 0.7890\n",
            "\n",
            "Epoch 00004: loss improved from 0.62731 to 0.62460, saving model to ./weights/lstmfcn_8_cells_weights/HandOutlines_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/HandOutlines_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/HandOutlines_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  370 Number of test samples :  1000\n",
            "Number of classes :  2\n",
            "Sequence length :  2709\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/HandOutlines_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "1000/1000 [==============================] - 32s 32ms/step\n",
            "\n",
            "Final Accuracy :  0.7889999747276306\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 1092)      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 1092, 1)      0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 1092, 128)    1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 1092, 128)    512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 1092, 128)    0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 1092, 256)    164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 1092, 256)    1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 1092, 256)    0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 1092, 128)    98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 1092, 128)    512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            35232       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 1092, 128)    0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 5)            685         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 301,645\n",
            "Trainable params: 300,621\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset Haptics ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Haptics_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Haptics_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  155 Number of test samples :  308\n",
            "Number of classes :  5\n",
            "Sequence length :  1092\n",
            "Class weights :  [1.72222222 0.91176471 0.91176471 0.86111111 0.93939394]\n",
            "Train on 155 samples, validate on 308 samples\n",
            "Epoch 1/4\n",
            " - 12s - loss: 1.7215 - accuracy: 0.2129 - val_loss: 2.0020 - val_accuracy: 0.1916\n",
            "\n",
            "Epoch 00001: loss improved from inf to 1.72153, saving model to ./weights/lstmfcn_8_cells_weights/Haptics_weights.h5\n",
            "Epoch 2/4\n",
            " - 11s - loss: 1.5944 - accuracy: 0.1935 - val_loss: 1.9697 - val_accuracy: 0.1916\n",
            "\n",
            "Epoch 00002: loss improved from 1.72153 to 1.59442, saving model to ./weights/lstmfcn_8_cells_weights/Haptics_weights.h5\n",
            "Epoch 3/4\n",
            " - 11s - loss: 1.5589 - accuracy: 0.2774 - val_loss: 1.9140 - val_accuracy: 0.1883\n",
            "\n",
            "Epoch 00003: loss improved from 1.59442 to 1.55889, saving model to ./weights/lstmfcn_8_cells_weights/Haptics_weights.h5\n",
            "Epoch 4/4\n",
            " - 11s - loss: 1.5546 - accuracy: 0.2903 - val_loss: 1.9477 - val_accuracy: 0.1883\n",
            "\n",
            "Epoch 00004: loss improved from 1.55889 to 1.55457, saving model to ./weights/lstmfcn_8_cells_weights/Haptics_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Haptics_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Haptics_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  155 Number of test samples :  308\n",
            "Number of classes :  5\n",
            "Sequence length :  1092\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/Haptics_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "308/308 [==============================] - 4s 13ms/step\n",
            "\n",
            "Final Accuracy :  0.18831168115139008\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 512)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 512, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 512, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 512, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 512, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 512, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 512, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 512, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 512, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 512, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            16672       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 512, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 2)            274         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 282,674\n",
            "Trainable params: 281,650\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset Herring ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Herring_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Herring_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  64 Number of test samples :  64\n",
            "Number of classes :  2\n",
            "Sequence length :  512\n",
            "Class weights :  [0.82051282 1.28      ]\n",
            "Train on 64 samples, validate on 64 samples\n",
            "Epoch 1/4\n",
            " - 3s - loss: 0.6691 - accuracy: 0.6094 - val_loss: 0.8699 - val_accuracy: 0.5938\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.66909, saving model to ./weights/lstmfcn_8_cells_weights/Herring_weights.h5\n",
            "Epoch 2/4\n",
            " - 2s - loss: 0.7086 - accuracy: 0.4531 - val_loss: 1.2422 - val_accuracy: 0.5938\n",
            "\n",
            "Epoch 00002: loss did not improve from 0.66909\n",
            "Epoch 3/4\n",
            " - 2s - loss: 0.6620 - accuracy: 0.6094 - val_loss: 1.6056 - val_accuracy: 0.5938\n",
            "\n",
            "Epoch 00003: loss improved from 0.66909 to 0.66204, saving model to ./weights/lstmfcn_8_cells_weights/Herring_weights.h5\n",
            "Epoch 4/4\n",
            " - 2s - loss: 0.6667 - accuracy: 0.6094 - val_loss: 1.9072 - val_accuracy: 0.5938\n",
            "\n",
            "Epoch 00004: loss did not improve from 0.66204\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Herring_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Herring_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  64 Number of test samples :  64\n",
            "Number of classes :  2\n",
            "Sequence length :  512\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/Herring_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "64/64 [==============================] - 1s 8ms/step\n",
            "\n",
            "Final Accuracy :  0.59375\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 1882)      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 1882, 1)      0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 1882, 128)    1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 1882, 128)    512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 1882, 128)    0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 1882, 256)    164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 1882, 256)    1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 1882, 256)    0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 1882, 128)    98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 1882, 128)    512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            60512       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 1882, 128)    0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 7)            959         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 327,199\n",
            "Trainable params: 326,175\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset InlineSkate ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/InlineSkate_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/InlineSkate_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  100 Number of test samples :  550\n",
            "Number of classes :  7\n",
            "Sequence length :  1882\n",
            "Class weights :  [1.58730159 1.02040816 0.79365079 0.79365079 0.89285714 1.02040816\n",
            " 1.2987013 ]\n",
            "Train on 100 samples, validate on 550 samples\n",
            "Epoch 1/4\n",
            " - 20s - loss: 2.0682 - accuracy: 0.0800 - val_loss: 2.0145 - val_accuracy: 0.1836\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.06817, saving model to ./weights/lstmfcn_8_cells_weights/InlineSkate_weights.h5\n",
            "Epoch 2/4\n",
            " - 19s - loss: 1.9654 - accuracy: 0.1400 - val_loss: 2.0204 - val_accuracy: 0.1836\n",
            "\n",
            "Epoch 00002: loss improved from 2.06817 to 1.96538, saving model to ./weights/lstmfcn_8_cells_weights/InlineSkate_weights.h5\n",
            "Epoch 3/4\n",
            " - 19s - loss: 1.8910 - accuracy: 0.2300 - val_loss: 2.0880 - val_accuracy: 0.1836\n",
            "\n",
            "Epoch 00003: loss improved from 1.96538 to 1.89104, saving model to ./weights/lstmfcn_8_cells_weights/InlineSkate_weights.h5\n",
            "Epoch 4/4\n",
            " - 19s - loss: 1.8562 - accuracy: 0.2900 - val_loss: 2.2074 - val_accuracy: 0.1836\n",
            "\n",
            "Epoch 00004: loss improved from 1.89104 to 1.85624, saving model to ./weights/lstmfcn_8_cells_weights/InlineSkate_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/InlineSkate_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/InlineSkate_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  100 Number of test samples :  550\n",
            "Number of classes :  7\n",
            "Sequence length :  1882\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/InlineSkate_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "550/550 [==============================] - 12s 22ms/step\n",
            "\n",
            "Final Accuracy :  0.1836363673210144\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 720)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 720, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 720, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 720, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 720, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 720, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 720, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 720, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 720, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 720, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            23328       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 720, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 3)            411         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 289,467\n",
            "Trainable params: 288,443\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset LargeKitchenAppliances ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/LargeKitchenAppliances_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/LargeKitchenAppliances_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  375 Number of test samples :  375\n",
            "Number of classes :  3\n",
            "Sequence length :  720\n",
            "Class weights :  [1. 1. 1.]\n",
            "Train on 375 samples, validate on 375 samples\n",
            "Epoch 1/4\n",
            " - 15s - loss: 1.0372 - accuracy: 0.4773 - val_loss: 1.1696 - val_accuracy: 0.3227\n",
            "\n",
            "Epoch 00001: loss improved from inf to 1.03719, saving model to ./weights/lstmfcn_8_cells_weights/LargeKitchenAppliances_weights.h5\n",
            "Epoch 2/4\n",
            " - 14s - loss: 0.9026 - accuracy: 0.5947 - val_loss: 1.3502 - val_accuracy: 0.3787\n",
            "\n",
            "Epoch 00002: loss improved from 1.03719 to 0.90259, saving model to ./weights/lstmfcn_8_cells_weights/LargeKitchenAppliances_weights.h5\n",
            "Epoch 3/4\n",
            " - 14s - loss: 0.8330 - accuracy: 0.6907 - val_loss: 1.4192 - val_accuracy: 0.4667\n",
            "\n",
            "Epoch 00003: loss improved from 0.90259 to 0.83304, saving model to ./weights/lstmfcn_8_cells_weights/LargeKitchenAppliances_weights.h5\n",
            "Epoch 4/4\n",
            " - 14s - loss: 0.8020 - accuracy: 0.6960 - val_loss: 1.4402 - val_accuracy: 0.5653\n",
            "\n",
            "Epoch 00004: loss improved from 0.83304 to 0.80197, saving model to ./weights/lstmfcn_8_cells_weights/LargeKitchenAppliances_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/LargeKitchenAppliances_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/LargeKitchenAppliances_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  375 Number of test samples :  375\n",
            "Number of classes :  3\n",
            "Sequence length :  720\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/LargeKitchenAppliances_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "375/375 [==============================] - 3s 9ms/step\n",
            "\n",
            "Final Accuracy :  0.5653333067893982\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 637)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 637, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 637, 128)     1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 637, 128)     512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 637, 128)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 637, 256)     164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 637, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 637, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 637, 128)     98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 637, 128)     512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            20672       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 637, 128)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 2)            274         concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 286,674\n",
            "Trainable params: 285,650\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset Lighting2 ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Lighting2_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Lighting2_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  60 Number of test samples :  61\n",
            "Number of classes :  2\n",
            "Sequence length :  637\n",
            "Class weights :  [1.5  0.75]\n",
            "Train on 60 samples, validate on 61 samples\n",
            "Epoch 1/4\n",
            " - 3s - loss: 0.6464 - accuracy: 0.6667 - val_loss: 0.7935 - val_accuracy: 0.4590\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.64635, saving model to ./weights/lstmfcn_8_cells_weights/Lighting2_weights.h5\n",
            "Epoch 2/4\n",
            " - 2s - loss: 0.5357 - accuracy: 0.7000 - val_loss: 0.9000 - val_accuracy: 0.4590\n",
            "\n",
            "Epoch 00002: loss improved from 0.64635 to 0.53565, saving model to ./weights/lstmfcn_8_cells_weights/Lighting2_weights.h5\n",
            "Epoch 3/4\n",
            " - 2s - loss: 0.4853 - accuracy: 0.8000 - val_loss: 0.9861 - val_accuracy: 0.4590\n",
            "\n",
            "Epoch 00003: loss improved from 0.53565 to 0.48531, saving model to ./weights/lstmfcn_8_cells_weights/Lighting2_weights.h5\n",
            "Epoch 4/4\n",
            " - 2s - loss: 0.4781 - accuracy: 0.7833 - val_loss: 1.0997 - val_accuracy: 0.4590\n",
            "\n",
            "Epoch 00004: loss improved from 0.48531 to 0.47808, saving model to ./weights/lstmfcn_8_cells_weights/Lighting2_weights.h5\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Lighting2_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/Lighting2_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  60 Number of test samples :  61\n",
            "Number of classes :  2\n",
            "Sequence length :  637\n",
            "Weights loaded from  ./weights/lstmfcn_8_cells_weights/Lighting2_weights.h5\n",
            "\n",
            "Evaluating : \n",
            "61/61 [==============================] - 1s 10ms/step\n",
            "\n",
            "Final Accuracy :  0.4590163826942444\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1, 1024)      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 1024, 1)      0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 1024, 128)    1152        permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 1024, 128)    512         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 1024, 128)    0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 1024, 256)    164096      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 1024, 256)    1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 1024, 256)    0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 1024, 128)    98432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 1024, 128)    512         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 8)            33056       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 1024, 128)    0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 8)            1096        concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 299,880\n",
            "Trainable params: 298,856\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "******************** Training model for dataset MALLAT ********************\n",
            "Loading train / test dataset :  /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/MALLAT_TRAIN /content/gdrive/My Drive/Colab Notebooks/LSTM-FCN/data/MALLAT_TEST\n",
            "Finished loading train dataset..\n",
            "Finished loading test dataset..\n",
            "\n",
            "Number of train samples :  55 Number of test samples :  2345\n",
            "Number of classes :  8\n",
            "Sequence length :  1024\n",
            "Class weights :  [1.14583333 0.859375   1.14583333 0.625      3.4375     1.14583333\n",
            " 0.76388889 0.98214286]\n",
            "Train on 55 samples, validate on 2345 samples\n",
            "Epoch 1/4\n",
            " - 34s - loss: 2.1943 - accuracy: 0.1455 - val_loss: 2.2245 - val_accuracy: 0.1254\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.19430, saving model to ./weights/lstmfcn_8_cells_weights/MALLAT_weights.h5\n",
            "Epoch 2/4\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}